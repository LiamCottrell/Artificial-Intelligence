{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "#for the sigmoid function we need expit() from scipy\n",
    "import scipy.special\n",
    "#library for plotting arrays\n",
    "import matplotlib.pyplot as plt\n",
    "# A particularly interesting backend, provided by IPython, is the inline backend. \n",
    "# This is available only for the Jupyter Notebook and the Jupyter QtConsole. \n",
    "# It can be invoked as follows: %matplotlib inline\n",
    "# With this backend, the output of plotting commands is displayed inline \n",
    "# within frontends like the Jupyter notebook, directly below the code cell that produced it. \n",
    "# The resulting plots are inside this notebook, not an external window.\n",
    "\n",
    "import pandas as pd # to manage data frames and reading csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set our Global Variables\n",
    "later you will need to modify these to present your solution to the Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of input, hidden and output nodes\n",
    "input_nodes = 784 #we have 28 * 28 matrix to describe each digit\n",
    "hidden_nodes = 200\n",
    "output_nodes = 10\n",
    "\n",
    "learning_rate = 0.3\n",
    "batch_size = 10\n",
    "\n",
    "# epochs is the number of training iterations \n",
    "epochs = 30\n",
    "\n",
    "# datasets to read\n",
    "# you can change these when trying out other datasets\n",
    "train_file = \"data/mnist_train.csv\"\n",
    "test_file = \"data/mnist_test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify our Dataset for Classification\n",
    "Note we have indicated a train set for model training and test set for testing the learned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset\n",
    "The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
    "The database is also widely used for training and testing in the field of machine learning.\n",
    "It was created by \"re-mixing\" the samples from NIST's original datasets. \n",
    "The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments.\n",
    "Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.\n",
    "<img src=\"mnist.png\">\n",
    " \n",
    "The MNIST database contains 60,000 training images and 10,000 testing images.\n",
    "Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset.\n",
    "There have been a number of scientific papers on attempts to achieve the lowest error rate; one paper, using a hierarchical system of convolutional neural networks, manages to get an error rate on the MNIST database of 0.23 percent.\n",
    "The original creators of the database keep a list of some of the methods tested on it.\n",
    "In their original paper, they use a support vector machine to get an error rate of 0.8 percent.\n",
    "\n",
    "### MINIST for the lab\n",
    "Our training and test set contains 60K and 10K instances.\n",
    "To reduce the time taken for training we can use a smaller random sample during our lab. \n",
    "\n",
    "Lets use the pandas library to have a look at an instance in the MNIST dataset below.\n",
    "\n",
    "You can change the idx value to access different instances (example below accesses the 21st indexed instance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the file into a pandas frame\n",
    "df = pd.read_csv(train_file) \n",
    "\n",
    "#get a specific instance and read values other than position 1 e.g. slice using 1:\n",
    "idx = 21\n",
    "instance = df.iloc[idx:(idx+1), 1:].values\n",
    "\n",
    "# now reshape the 784 features into a 28x28 grid\n",
    "# here asfarray helps to convert values into real numbers\n",
    "image_array = np.asfarray(instance).flatten().reshape((28,28))\n",
    "\n",
    "# print the grid in grey scale\n",
    "plt.imshow(image_array, cmap='Greys', interpolation='None') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the mnist training data CSV file into a list\n",
    "train_data_file = open(train_file, 'r')\n",
    "train_data_list = train_data_file.readlines() # read all lines into memory \n",
    "train_data_file.close() \n",
    "print(\"train set size: \", len(train_data_list))\n",
    "\n",
    "#testing the network\n",
    "#load the mnist test data CSV file into a list\n",
    "#test_data_file = open(\"mnist/mnist_test_10.csv\", 'r') # read the file with 10 instances first\n",
    "test_data_file = open(test_file, 'r') # read the file with 10 instances first\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()\n",
    "print(\"test set size: \", len(test_data_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our Artificial Neural Network class\n",
    "Some of the key methods include:\n",
    "\n",
    "- __init__ : initialisation method to set all the class variables\n",
    "- train : this is the method that implments the forward and back ward pass as well as updating the wights. You should study the weight update lines to understand ; which values actually contribute to the modification of weights. Note there are two sets of weights - wih are wwights from input to hidden nodes; and who weights from hidden to output nodes.\n",
    "- test: this takes a set in test instances and queries the network to obtain a prediction which can then be compared against the expected target class to compute accuracy\n",
    "\n",
    "Some of the key hyper-parameters include:\n",
    "- ep (epoch) which is the number of training iterations on the full dataset); \n",
    "- lr (learning rate) the amount of moderation used to manage by how much we adjust the weights\n",
    "- batch_size the number of examples considered within a single epoch before updating network parameters; \n",
    "- hnodes (hidden nodes) the number of hidden layer nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    \"\"\"Artificial Neural Network classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    lr : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    ep : int\n",
    "      Number of epochs for training the network towards achieving convergence\n",
    "    batch_size : int\n",
    "      Size of the training batch to be used when calculating the gradient descent. \n",
    "      batch_size = 0 standard gradient descent\n",
    "      batch_size > 0 stochastic gradient descent \n",
    "\n",
    "    inodes : int\n",
    "      Number of input nodes which is normally the number of features in an instance.\n",
    "    hnodes : int\n",
    "      Number of hidden nodes in the net.\n",
    "    onodes : int\n",
    "      Number of output nodes in the net.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    wih : 2d-array\n",
    "      Input2Hidden node weights after fitting \n",
    "    who : 2d-array\n",
    "      Hidden2Output node weights after fitting \n",
    "    E : list\n",
    "      Sum-of-squares error value in each epoch.\n",
    "      \n",
    "    Results : list\n",
    "      Target and predicted class labels for the test data.\n",
    "      \n",
    "    Functions\n",
    "    ---------\n",
    "    activation_function : float (between 1 and -1)\n",
    "        implments the sigmoid function which squashes the node input\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputnodes=784, hiddennodes=200, outputnodes=10, learningrate=0.3, batch_size=10, epochs=30):\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        #link weight matrices, wih (input to hidden) and who (hidden to output)\n",
    "        #a weight on link from node i to node j is w_ij\n",
    "        \n",
    "        \n",
    "        #Draw random samples from a normal (Gaussian) distribution centered around 0.\n",
    "        #numpy.random.normal(loc to centre gaussian=0.0, scale=1, size=dimensions of the array we want) \n",
    "        #scale is usually set to the standard deviation which is related to the number of incoming links i.e. \n",
    "        #1/sqrt(num of incoming inputs). we use pow to raise it to the power of -0.5.\n",
    "        #We have set 0 as the centre of the guassian dist.\n",
    "        # size is set to the dimensions of the number of hnodes, inodes and onodes\n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        #set the learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        #set the batch size\n",
    "        self.bs = batch_size\n",
    "        \n",
    "        #set the number of epochs\n",
    "        self.ep = epochs\n",
    "        \n",
    "        #store errors at each epoch\n",
    "        self.E= []\n",
    "        \n",
    "        #store results from testing the model\n",
    "        #keep track of the network performance on each test instance\n",
    "        self.results= []\n",
    "        \n",
    "        #define the activation function here\n",
    "        #specify the sigmoid squashing function. Here expit() provides the sigmoid function.\n",
    "        #lambda is a short cut function which is executed there and then with no def (i.e. like an anonymous function)\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "   \n",
    "    def batch_input(self, input_list):\n",
    "        \"\"\"Yield consecutive batches of the specified size from the input list.\"\"\"\n",
    "        for i in range(0, len(input_list), self.bs):\n",
    "            yield input_list[i:i + self.bs]\n",
    "    \n",
    "    #train the neural net\n",
    "    #note the first part is very similar to the query function because they both require the forward pass\n",
    "    def train(self, train_inputs):\n",
    "        \"\"\"Training the neural net. \n",
    "           This includes the forward pass ; error computation; \n",
    "           backprop of the error ; calculation of gradients and updating the weights.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            train_inputs : {array-like}, shape = [n_instances, n_features]\n",
    "            Training vectors, where n_instances is the number of training instances and\n",
    "            n_features is the number of features.\n",
    "            Note this contains all features including the class feature which is in first position\n",
    "        \n",
    "            Returns\n",
    "            -------\n",
    "            self : object\n",
    "        \"\"\"\n",
    "      \n",
    "        for e in range(self.ep):\n",
    "            print(\"Training epoch#: \", e)\n",
    "            sum_error = 0.0   \n",
    "            for batch in self.batch_input(train_inputs):\n",
    "                #creating variables to store the gradients   \n",
    "                delta_who = 0\n",
    "                delta_wih = 0\n",
    "                \n",
    "                # iterate through the inputs sent in\n",
    "                for instance in batch:\n",
    "                    # split it by the commas\n",
    "                    all_values = instance.split(',')\n",
    "                    # scale and shift the inputs to address the problem of diminishing weights due to multiplying by zero\n",
    "                    # divide the raw inputs which are in the range 0-255 by 255 will bring them into the range 0-1\n",
    "                    # multiply by 0.99 to bring them into the range 0.0 - 0.99.\n",
    "                    # add 0.01 to shift them up to the desired range 0.01 - 1. \n",
    "                    inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "                    #create the target output values for each instance so that we can use it with the neural net\n",
    "                    #note we need 10 nodes where each represents one of the digits\n",
    "                    targets = np.zeros(output_nodes) + 0.01 #all initialised to 0.01\n",
    "                    #all_value[0] has the target class label for this instance\n",
    "                    targets[int(all_values[0])] = 0.99\n",
    "        \n",
    "                    #convert  inputs list to 2d array\n",
    "                    inputs = np.array(inputs,  ndmin=2).T\n",
    "                    targets = np.array(targets, ndmin=2).T\n",
    "\n",
    "                    #calculate signals into hidden layer\n",
    "                    hidden_inputs = np.dot(self.wih, inputs)\n",
    "                    #calculate the signals emerging from the hidden layer\n",
    "                    hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "                    #calculate signals into final output layer\n",
    "                    final_inputs=np.dot(self.who, hidden_outputs)\n",
    "                    #calculate the signals emerging from final output layer\n",
    "                    final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "                    #to calculate the error we need to compute the element wise diff between target and actual\n",
    "                    output_errors = targets - final_outputs\n",
    "                    #Next distribute the error to the hidden layer such that hidden layer error\n",
    "                    #is the output_errors, split by weights, recombined at hidden nodes\n",
    "                    hidden_errors = np.dot(self.who.T, output_errors)\n",
    "            \n",
    "                       \n",
    "                    ## for each instance accumilate the gradients from each instance\n",
    "                    ## delta_who are the gradients between hidden and output weights\n",
    "                    ## delta_wih are the gradients between input and hidden weights\n",
    "                    delta_who += np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "                    delta_wih += np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "                    \n",
    "                    sum_error += np.dot(output_errors.T, output_errors)#this is the sum of squared error accumilated over each batced instance\n",
    "                   \n",
    "                pass #instance\n",
    "            \n",
    "                # update the weights by multiplying the gradient with the learning rate\n",
    "                # note that the deltas are divided by batch size to obtain the average gradient according to the given batch\n",
    "                # obviously if batch size = 1 then we dont need to bother with an average\n",
    "                self.who += self.lr * (delta_who / self.bs)\n",
    "                self.wih += self.lr * (delta_wih / self.bs)\n",
    "            pass # batch\n",
    "            self.E.append(np.asfarray(sum_error).flatten())\n",
    "            print(\"errors (SSE): \", self.E[-1])\n",
    "        pass # epoch\n",
    "    \n",
    "    #query the neural net\n",
    "    def query(self, inputs_list):\n",
    "        #convert inputs_list to a 2d array\n",
    "        #print(numpy.matrix(inputs_list))\n",
    "        #inputs_list [[ 1.   0.5 -1.5]]\n",
    "        inputs = np.array(inputs_list, ndmin=2).T \n",
    "        #once converted it appears as follows\n",
    "        #[[ 1. ]\n",
    "        # [ 0.5]\n",
    "        # [-1.5]]\n",
    "        #print(numpy.matrix(inputs))\n",
    "        \n",
    "        #propogate input into hidden layer. This is the start of the forward pass\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        \n",
    "        \n",
    "        #squash the content in the hidden node using the sigmoid function (value between 1, -1)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "                \n",
    "        #propagate into output layer and the apply the squashing sigmoid function\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        \n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        return final_outputs\n",
    "    \n",
    "     \n",
    "    #iterate through all the test data to calculate model accuracy\n",
    "    def test(self, test_inputs):\n",
    "        self.results = []\n",
    "        \n",
    "        #go through each test instances\n",
    "        for instance in test_inputs:\n",
    "            all_values = instance.split(',') # extract the input feature values for the instance\n",
    "    \n",
    "            target_label = int(all_values[0]) # get the target class for the instance\n",
    "    \n",
    "            #scale and shift the inputs this is to make sure values dont lead to zero when multiplied with weights\n",
    "            inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "    \n",
    "            #query the network with test inputs\n",
    "            #note this returns 10 output values ; of which the index of the highest value\n",
    "            # is the networks predicted class label\n",
    "            outputs = self.query(inputs)\n",
    "    \n",
    "            #get the index of the highest output node as this corresponds to the predicted class\n",
    "            predict_label = np.argmax(outputs) #this is the class predicted by the ANN\n",
    "    \n",
    "            self.results.append([predict_label, target_label])\n",
    "            #compute network error\n",
    "            #if (predict_label == target_label):\n",
    "            #    self.results.append(1)\n",
    "            #else: \n",
    "            #    self.results.append(0)\n",
    "            pass\n",
    "        pass\n",
    "        self.results = np.asfarray(self.results) # flatten results to avoid nested arrays\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Artificial Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instance of neuralnet\n",
    "n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate, batch_size, epochs)\n",
    "\n",
    "# numpy.random.choice generates a random sample from a given 1-D array\n",
    "# we can use this to select a sample from our training data in case we want to work with a small sample\n",
    "# for instance we use a small sample here such as 1500\n",
    "mini_training_data = np.random.choice(train_data_list, 1500, replace = False)\n",
    "print(\"Percentage of training data used:\", (len(mini_training_data)/len(train_data_list)) * 100)\n",
    "n.train(mini_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the model error with increasing epochs\n",
    "\n",
    "Error at the end of each epoch has been stored in self.E\n",
    "We can now use mathplotlib to plot the error at the end of each epoch. \n",
    "Our expectation is that as we continue to descend (hill-walking) we should move closer to the minima\n",
    "as such error should decrease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the error over epochs\n",
    "\n",
    "plt.figure(figsize=(20,5)) #width, height settings for figures\n",
    "plt.plot(range(0, n.ep), np.asfarray(n.E), marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of errors')\n",
    "\n",
    "\n",
    "\n",
    "# plt.savefig('images/02_07.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the ANN and compute the Accuracy\n",
    "We will keep track of the predicted and actual outputs in order to \n",
    "calculate the accuracy of the model on the unseen test data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.test(test_data_list)\n",
    "#print network performance as an accuracy metric\n",
    "correct = 0 # number of predictions that were correct\n",
    "\n",
    "#iteratre through each tested instance and accumilate number of correct predictions\n",
    "for result in n.results:\n",
    "    if (result[0] == result[1]):\n",
    "            correct += 1\n",
    "    pass\n",
    "pass\n",
    "\n",
    "# print the accuracy on test set\n",
    "print (\"Test set accuracy% = \", (100 * correct / len(n.results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring different values of a hyperparameter - a study on the batch_size\n",
    "Often we need to explore a variety of values for an algorithms hyper parameters before we can suggest the best model. For instance with the ANN we can study the accuracy by changing the number of epochs, or the batch_size or learning rate , for instance. \n",
    "\n",
    "Lets look at what happens when the batch size \n",
    "- batch_size = size of training data; we adopt the Gradient Decent (GD) approach to optimisaing the ANN's weights. This means that the weights are updated once at each epoch (= pass over the training dataset). The amount by which we update is an aggregation over all the training instances. \n",
    "\n",
    "- batch_size = 1, we adopt the Stochastic Gradient Descent (SGD) optimisation approach to updating weights. SGD is sometimes also referred to as iterative or on-line Gradient Decent (GD). In a given epoch we update the weights many times. That is to say that we check the error with each training instance; compute the gradient and update the weights. As such we don't accumulate the weight updates as we do with GD.\n",
    "Here, the term \"stochastic\" comes from the fact that the gradient based on a single training sample is a \"stochastic approximation\" of the \"true\" cost gradient. Due to its stochastic nature, the path towards the global cost minimum is not \"direct\" as in GD, but may go \"zig-zag\" if we are visualizing the cost surface in a 2D space. However, it has been shown that SGD although is time consuming is more likely to converge to the global cost minimum.\n",
    "\n",
    "- batch_size >1 and < len(training data); this will be an inbetween mini-batch approach to updating the weights. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy.random.choice generates a random sample from a given 1-D array\n",
    "# we can use this to select a sample from our training data in case we want to work with a small sample\n",
    "# for instance we use a small sample here such as 1500\n",
    "mini_training_data = np.random.choice(train_data_list, 1500, replace = False)\n",
    "print(\"Percentage of training data used:\", (len(mini_training_data)/len(train_data_list)) * 100)\n",
    "\n",
    "print(\"This will take a few moments ...\")\n",
    "n_list = []\n",
    "batch_sizes = [1, 20, 100, 200, 500, len(mini_training_data)]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    n = neuralNetwork(batch_size=batch_size)\n",
    "    n.train(mini_training_data)\n",
    "    n_list.append(n)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather the test results for each ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iteratre through each model and accumilate number of correct predictions\n",
    "model_results = []\n",
    "for model in n_list: \n",
    "    correct = 0\n",
    "    model.test(test_data_list)\n",
    "    for result in model.results:\n",
    "        if (result[0] == result[1]):\n",
    "                correct += 1\n",
    "        pass\n",
    "    correct = 100 * (correct/len(model.results))\n",
    "    model_results.append(correct)\n",
    "    print(correct)\n",
    "    pass\n",
    "pass\n",
    "\n",
    "\n",
    "# print the accuracy on test set\n",
    "#print (\"Test set accuracy% = \", (100 * correct / len(n.results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = batch_sizes\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = model_results\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('batch_size')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the impact of varying two hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_training_data = np.random.choice(train_data_list, 1500, replace = False)\n",
    "print(\"Percentage of training data used:\", (len(mini_training_data)/len(train_data_list)) * 100)\n",
    "\n",
    "print(\"This will take a few moments ...\")\n",
    "n_list = []\n",
    "batch_sizes = [1, 20, 100, 200, 500, len(mini_training_data)]\n",
    "hidden_nodes = [200, 5] # lets try two different configurations of hidden node layer\n",
    "\n",
    "# lets create the set of modesl\n",
    "for hidden in hidden_nodes:\n",
    "    n_list_element = [] # first set of nets with a given hidden node size\n",
    "    \n",
    "    for batch in batch_sizes:\n",
    "        n = neuralNetwork(batch_size=batch, hiddennodes=hidden)\n",
    "        n.train(mini_training_data)\n",
    "        n_list_element.append(n)\n",
    "        pass\n",
    "    \n",
    "    n_list.append(n_list_element)# now append the set of models \n",
    "    pass\n",
    "\n",
    "#lets test the models \n",
    "#iterate through each model and accumilate number of correct predictions\n",
    "model_results = []\n",
    "model_result_element = []\n",
    "\n",
    "for model1 in n_list:\n",
    "    model_result_element = []\n",
    "    \n",
    "    for model2 in model1:\n",
    "        correct = 0\n",
    "        model2.test(test_data_list)\n",
    "        \n",
    "        for result in model2.results:\n",
    "           \n",
    "            if (result[0] == result[1]): \n",
    "                correct +=1\n",
    "            pass\n",
    "        correct = 100 * (correct / len(model2.results))\n",
    "        model_result_element.append(correct)\n",
    "        print(correct)\n",
    "        pass\n",
    "    pass\n",
    "    model_results.append(model_result_element)\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results of hidden nodes versus batch size\n",
    "\n",
    "In the graph below we have plotted the accuracy for 6 different batch sizes comparing against 2 architectures (one with 200 hidden nodes and the other with 4 hidden nodes). \n",
    "As expected when the ANN architecture has fewer hidden nodes it has less oppertunity to learn a sensible mapping between the input features and expected outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = batch_sizes\n",
    "\n",
    "y_pos = np.arange(len(objects))\n",
    "performance1 = model_results[0]\n",
    "performance2 = model_results[1]\n",
    " \n",
    "plt.bar(y_pos, performance1, align='center', alpha=0.5 )\n",
    "plt.bar(y_pos, performance2, align='center', alpha=0.5)\n",
    "\n",
    "plt.legend(hidden_nodes)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('batch_size')\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
