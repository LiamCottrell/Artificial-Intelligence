{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} ggplot\n",
    "# !sed -i \"s,from pandas.lib import Timestamp,from pandas import Timestamp,g\" /opt/conda/lib/python3.6/site-packages/ggplot/stats/smoothers.py\n",
    "# !sed -i \"s,smoothed_data.sort('x'),smoothed_data.sort_values('x'),g\" /opt/conda/lib/python3.6/site-packages/ggplot/stats/stat_smooth.py\n",
    "# !sed -i \"s,sort(fillcol_raw),sort_values(fillcol_raw),g\" /opt/conda/lib/python3.6/site-packages/ggplot/ggplot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division # backward compatibility for python2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special\n",
    "import operator\n",
    "import random\n",
    "#library for plotting arrays\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ggplot import *\n",
    "\n",
    "# Import timer for Benchmarking time\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "# Import SKlearn packages that allow us to scale `Normalise` our data.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# A particularly interesting backend, provided by IPython, is the inline backend. \n",
    "# This is available only for the Jupyter Notebook and the Jupyter QtConsole. \n",
    "# It can be invoked as follows: %matplotlib inline\n",
    "# With this backend, the output of plotting commands is displayed inline \n",
    "# within frontends like the Jupyter notebook, directly below the code cell that produced it. \n",
    "# The resulting plots are inside this notebook, not an external window.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 256712\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Our Global Variables\n",
    "Later you will need to modify these to present your solution to the Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets to read\n",
    "# you can change these when trying out other datasets\n",
    "# data_file = \"Iris.csv\"\n",
    "data_file = \"data/cw_mnist.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Dataset using a Pandas Data Frame Object\n",
    "\n",
    "Lets create a function to read the data file and split it into test and train \n",
    "based on the ratio specified by split. For instance when split = 0.5 then train and test will \n",
    "contain a similar number of instances. \n",
    "Typically we would use a 70:30 or 80:20 split (i.e. 0.7 or 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename, split=0.8, fraction = 1.0):\n",
    "    dataframe = pd.read_csv(filename)\n",
    "    \n",
    "    dataframe = dataframe.sample(frac=fraction, replace=True)\n",
    "    \n",
    "    \n",
    "    #Rearange to make first column the last\n",
    "    \n",
    "#     print(\"Before\")\n",
    "#     print(dataframe.head())\n",
    "    \n",
    "    cols = dataframe.columns.tolist()\n",
    "    cols = cols[-(len(cols)-1):] + cols[:-(len(cols)-1)]\n",
    "    \n",
    "    dataframe = dataframe[cols]\n",
    "    \n",
    "    \n",
    "    print(len(dataframe.columns.tolist()))\n",
    "    \n",
    "#     print(\"After\")\n",
    "#     print(dataframe.head())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Overide to Class_index to always be the Last Column\n",
    "    class_idx = len(dataframe.columns)-1\n",
    "    \n",
    "    \n",
    "    #dataframe = dataframe.sample(len(dataframe))\n",
    "    instances = dataframe.values\n",
    "\n",
    "\n",
    "    print (\"Class Index: \"+str(class_idx))\n",
    "    # dpockemivide data into label and feature sets.\n",
    "    X = instances[:,0:class_idx] # you may need to change these depending on which dataset you are use\n",
    "    Y = instances[:,class_idx] \n",
    "    \n",
    "#     print(\"X\",X,\":\",\"Y\",Y)\n",
    "    \n",
    "   \n",
    "    X_train = [] # features for the train set\n",
    "    Y_train = [] # class labels for the train set\n",
    "    X_test = [] # features for the test set\n",
    "    Y_test = [] # class labels for the test set\n",
    "    \n",
    "    # the zip iterator is a neat construct in Python\n",
    "    # it lets you iterate over 2 arrays / lists structures \n",
    "    # importantly it iterates upto the length of the smallest structure of the two \n",
    "    # in our case X and Y will be of same length\n",
    "    for  x, y in zip(X, Y): \n",
    "        if random.random() < split: # Return the next random floating point number in the range [0.0, 1.0) and compare\n",
    "            X_train.append(x)\n",
    "            Y_train.append(y)\n",
    "        else:\n",
    "            X_test.append(x)\n",
    "            Y_test.append(y)       \n",
    "    print(\"train set size: \", len(X_train))       \n",
    "    print(\"test set size: \", len(X_test))\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Similarity Metrics\n",
    "In this section we define alternative similarity metrics.\n",
    "We have provided the Manhatten and Euclidean that was discussed in the class.\n",
    "You can also see that we can use the basic dot product as a sim metric. However results tend to be poor because a dot product is only concerned about the overlap between two lists and less conernced about the differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Within our class we now need code for each of the components of k-NN.\n",
    "#First, lets create a method that will measure the distance between two vectors.\n",
    "def euclidean(instance1, instance2):\n",
    "        '''\n",
    "        Calculates euclidean distance between two instances of data\n",
    "        instance1 will be a List of Float values\n",
    "        instance2 will be a List of Float values\n",
    "        length will be an Integer denoting the length of the Lists\n",
    "        '''\n",
    "        distance = 0\n",
    "        for val1, val2 in zip(instance1, instance2):            \n",
    "            distance += pow((val1 - val2), 2)\n",
    "        \n",
    "        distance = pow(distance, 1/2)\n",
    "             \n",
    "              \n",
    "        return 1 / (1+ distance)\n",
    "    \n",
    "\n",
    "def manhattan(instance1, instance2):\n",
    "        '''\n",
    "        Calculates manhattan distance between two instances of data\n",
    "        instance1 will be a List of Float values\n",
    "        instance2 will be a List of Float values\n",
    "        length will be an Integer denoting the length of the Lists\n",
    "        '''\n",
    "        distance = 0\n",
    "        for val1, val2 in zip(instance1, instance2):\n",
    "            distance += abs(val1 - val2)      \n",
    "              \n",
    "        return 1 / (1+ distance)\n",
    "    \n",
    "def dot_product(instance1, instance2):\n",
    "        '''\n",
    "        Calculates dot product between two instances \n",
    "        instance1 will be a List of Float values\n",
    "        instance2 will be a List of Float values\n",
    "        length will be an Integer denoting the length of the Lists\n",
    "        '''\n",
    "        return np.dot(instance1, instance2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "How might you implement a new similarity metric such as cosine?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Metrics\n",
    "We will use the accuracy on test set as a measure of kNN's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Finally, we can test to see how many of the test instances we got correct\n",
    "    def accuracy(results):\n",
    "        correct = 0\n",
    "        for predict, target in results:\n",
    "            \n",
    "            if predict == target:\n",
    "                correct += 1\n",
    "        return (correct/float(len(results))) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our k-NN Class\n",
    "Some of the key methods include:\n",
    "\n",
    "init : initialisation method to set all the class variables\n",
    "\n",
    "get_neighbours : get the nearest neighbours using the specified similarity function\n",
    "\n",
    "predict: get the prediction either using weighted or unweighted voting\n",
    "\n",
    "test: return results of applying kNN to each instances in a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNN:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    X_train, Y_train : list\n",
    "    these consists of the training set feature values and associated class labels\n",
    "    k : int\n",
    "    specify the number of neighbours\n",
    "    sim : literal\n",
    "    specify the name of the similarity metric (e.g. manhattan, eucliedean)\n",
    "    weighted : Boolean\n",
    "    specify the voting strategy as weighted or not weighted by similarity values\n",
    "  \n",
    "    Attributes\n",
    "    -----------  \n",
    "    Results : list\n",
    "      Target and predicted class labels for the test data.    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, X_train, Y_train, k=3, sim=manhattan, weighted=False):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        \n",
    "        if k <= len(self.X_train):\n",
    "            self.k = k # set the k value for neighbourhood size\n",
    "        else:\n",
    "            self.k = len(self.X_train) # to ensure the get_neighbours dont crash\n",
    "    \n",
    "        self.similarity = sim # specify a sim metric that has been pre-defined e.g. manhattan or euclidean\n",
    "        \n",
    "        self.weighted = weighted # boolean to choose between weighted / unweighted majority voting\n",
    "        \n",
    "        #store results from testing \n",
    "        self.results= []\n",
    "        \n",
    "    #With k-NN, we are interested in finding the k number of points with the greatest similarity \n",
    "    # to the the query or test instance.\n",
    "    def get_neighbours(self, test_instance):\n",
    "        '''\n",
    "        Locate most similar neighbours \n",
    "        X_train will be a containing features (Float) values (i.e. your training data)\n",
    "        Y_train will be the corresponding class labels for each instance in X_train\n",
    "        test_instance will be a List of Float values (i.e. a query instance)\n",
    "        '''\n",
    "        similarities = [] # collection to store the similarities to be computed\n",
    "\n",
    "        for train_instance, y in zip(self.X_train, self.Y_train): #for each member of the training set\n",
    "            sim = self.similarity(test_instance, train_instance) #calculate the similarity to the test instance\n",
    "            \n",
    "            similarities.append((y, sim)) #add the actual label of the example and the computed similarity to a collection \n",
    "        #print(distances)\n",
    "        similarities.sort(key = operator.itemgetter(1), reverse = True) #sort the collection by decreasing similarity\n",
    "        neighbours = [] # holds the k most similar neighbours\n",
    "        for x in range(self.k): #extract the k top indices of the collection for return\n",
    "            neighbours.append(similarities[x])\n",
    "\n",
    "        return neighbours\n",
    "\n",
    "    # given the neighbours make a prediction\n",
    "    # the boolean parameter when set to False will use unweighted majority voting; otherwise weighted majority voting\n",
    "    # weighting can be helpful to break any ties in voting\n",
    "    def predict(self, neighbours):\n",
    "        '''\n",
    "        Summarise a prediction based upon weighted neighbours calculation\n",
    "        '''\n",
    "        class_votes = {}\n",
    "        for x in range(len(neighbours)):\n",
    "            response = neighbours[x][0]\n",
    "            if response in class_votes:\n",
    "                class_votes[response] += (1-self.weighted) + (self.weighted * neighbours[x][1]) #if not weighted simply add 1\n",
    "                #class_votes[response] += [1, neighbours[x][1]][weighted == True] \n",
    "              \n",
    "            else:\n",
    "                class_votes[response] = (1-self.weighted) + (self.weighted * neighbours[x][1])\n",
    "                #class_votes[response] = [1, neighbours[x][1]][weighted == True] \n",
    "                \n",
    "        #print(class_votes)\n",
    "        sorted_votes = sorted(class_votes, key = lambda k: (class_votes[k], k), reverse = True)\n",
    "        #print(sorted_votes)\n",
    "        return sorted_votes[0]\n",
    "    \n",
    "    #iterate through all the test data to calculate accuracy\n",
    "    def test(self, X_test, Y_test):\n",
    "        self.results = [] # store the predictions returned by kNN\n",
    "\n",
    "        for test_instance, target_label in zip(X_test, Y_test):\n",
    "            neighbours = self.get_neighbours(test_instance)\n",
    "            print(neighbours)\n",
    "            predict_label = self.predict(neighbours)\n",
    "            self.results.append([predict_label, target_label])\n",
    "            #print('> predicted = ', result,', actual = ', test_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset and maintain the features (X) and class labels (Y) separately  \n",
    "# make sure you understand what the 4 and 0.8 default values are in the call\n",
    "# you may have to modify these depending on the dataset you work with.\n",
    "X_train, Y_train, X_test, Y_test = load_dataset(data_file, split=0.8, fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply kNN to Test Data\n",
    "Here we can try out both versions of the kNN i.e. weighted and unweighted\n",
    "and compare their results. This differences becomes obvious mostly in \n",
    "situations when ties are frequently encountered in voting. \n",
    "\n",
    "Notice how we use the accuracy function to compute performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance of kNN \n",
    "# pass the training instances with their class labels (i.e. X_train and Y_train)\n",
    "# we will use the default kNN class settings for parameters i.e. k=3, sim=manhattan, weighted=False\n",
    "\n",
    "print(\"Deduce a Good K-Value: \", math.sqrt(len(Y_test)))\n",
    "\n",
    "knn = kNN(X_train, Y_train, k=int(math.sqrt(len(Y_test))))\n",
    "knn.test(X_test, Y_test) # now get the predictions on the test set\n",
    "\n",
    "print(\"kNN Accuracy on test set is: \", accuracy(knn.results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.similarity(X_test[1],X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "Apply kNN to test data and to explore:\n",
    "- impact of different train-test splits in \"Load the data set\"\n",
    "- impact of different k values\n",
    "- impact of different similarity metrics\n",
    "\n",
    "Is the accuracy better, worse or similar?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Setup an Experiment with Multiple Values of k?\n",
    "Often we want to explore the impact of increasing values of k on kNN performace.\n",
    "Given the class descriptions above we can set this up as shown below.\n",
    "Lets load the dataset using a new split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset and maintain the features (X) and class labels (Y) separately  \n",
    "# make sure you understand what the 4 and 0.8 default values are in the call\n",
    "X_train, Y_train, X_test, Y_test = load_dataset(data_file, split=0.8, fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setup the kNN instances ...\")\n",
    "euc_knn_list = []\n",
    "\n",
    "euc_time_list = []\n",
    "\n",
    "ks = []\n",
    "\n",
    "\n",
    "for k_val in range(1, 100, 5):\n",
    "    ks.append(k_val)\n",
    "\n",
    "# ks = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 75, 100] # try a few different values for k\n",
    "is_weighted = [False, True] # try two different forms of voting\n",
    "\n",
    "# iterate over different voting strategies\n",
    "for weighted in is_weighted:\n",
    "    euc_knn_list_element = [] # first set of knns with a specified voting scheme\n",
    "    #iterate over different k values\n",
    "    for k in ks:\n",
    "        #create the different instances of the kNN class\n",
    "        \n",
    "        knn = kNN(X_train, Y_train, k, euclidean, weighted)\n",
    "        \n",
    "        euc_knn_list_element.append(knn)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    euc_compiled_row = {}\n",
    "    \n",
    "    euc_knn_list.append(euc_knn_list_element)# now append the set of models \n",
    "    pass\n",
    "\n",
    "\n",
    "#lets test the kNNs \n",
    "#iterate through each model and accumilate number of correct predictions\n",
    "euc_knn_results = []\n",
    "euc_knn_result_element = []\n",
    "\n",
    "for knn1 in euc_knn_list:\n",
    "    euc_knn_result_element = []\n",
    "\n",
    "    for knn2 in knn1:\n",
    "        knn2.test(X_test, Y_test)\n",
    "             \n",
    "        euc_knn_result_element.append(accuracy(knn2.results))\n",
    "        \n",
    "        pass\n",
    "    pass\n",
    "    euc_knn_results.append(euc_knn_result_element)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results from trials...\", euc_knn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euc_compiled_results = []\n",
    "for weight_index in range(0, len(euc_knn_results)):\n",
    "    for accuracy_index in range(0, len(euc_knn_results[weight_index])):\n",
    "        euc_compiled_row = {}\n",
    "        euc_compiled_row['K'] = ks[accuracy_index]\n",
    "        if weight_index:\n",
    "            euc_compiled_row['Weighted'] = True\n",
    "        else:\n",
    "            euc_compiled_row['Weighted'] = False\n",
    "        euc_compiled_row['Accuracy'] = euc_knn_results[weight_index][accuracy_index]\n",
    "        euc_compiled_results.append(euc_compiled_row)\n",
    "\n",
    "euc_K_on_accuracy_df = pd.DataFrame(euc_compiled_results)\n",
    "euc_K_on_accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Plot Our Results on a Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(ks) # this is the number of results we want to plot pn the x-axis\n",
    "ind = np.arange(N) \n",
    "\n",
    "performance1 = euc_knn_results[0]\n",
    "performance2 = euc_knn_results[1]\n",
    "\n",
    "width = 0.35 # width of the bar      \n",
    "plt.bar(ind, performance1, width, label='Unweighted')\n",
    "plt.bar(ind + width, performance2, width, label='Weighted')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('k')\n",
    "plt.title('kNN performance with increasing')\n",
    "\n",
    "plt.xticks(ind + width / 2, ks)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(aes(x='K', y='Accuracy', color='factor(Weighted)'), data=euc_K_on_accuracy_df) +\\\n",
    "    geom_line() + \\\n",
    "    stat_smooth(method = 'lm') +\\\n",
    "    xlab(\"K - Value\") + \\\n",
    "    ylab(\"Accuracy\") + \\\n",
    "    ggtitle(\"K Value's Effect on Model Accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ggplot(aes(x='factor(K)', weight='Accuracy', fill='factor(Weighted)'), data=K_on_accuracy_df) + \\\n",
    "#     geom_bar() + \\\n",
    "#     xlab(\"K - Value\") + \\\n",
    "#     ylab(\"Accuracy\") + \\\n",
    "#     ggtitle(\"K Value's \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "- Modify the code to explore further values of increasing k and plot the results in a bar chart. Explore how this changes from small values of k to larger values of k. \n",
    "- Compare the role of different similarity metrics on kNN performance. Again you may want to plot these on the bar chart using increasing k values. Can you explain your findings? \n",
    "- Setup a similar experiment using the mnist dataset from week 3. You will need to change the class index accordingly to load the dataset. Remember to use a small sample from the mnist (e.g. 500) as the original train set has 10,000 instances and may take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setup the kNN instances ...\")\n",
    "man_knn_list = []\n",
    "\n",
    "man_time_list = []\n",
    "# iterate over different voting strategies\n",
    "for weighted in is_weighted:\n",
    "    man_knn_list_element = [] # first set of knns with a specified voting scheme\n",
    "    #iterate over different k values\n",
    "    for k in ks:\n",
    "        #create the different instances of the kNN class\n",
    "        \n",
    "        knn = kNN(X_train, Y_train, k, manhattan, weighted)\n",
    "        \n",
    "        man_knn_list_element.append(knn)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    man_compiled_row = {}\n",
    "    \n",
    "    man_knn_list.append(man_knn_list_element)# now append the set of models \n",
    "    pass\n",
    "\n",
    "\n",
    "#lets test the kNNs \n",
    "#iterate through each model and accumilate number of correct predictions\n",
    "man_knn_results = []\n",
    "man_knn_result_element = []\n",
    "\n",
    "for knn1 in man_knn_list:\n",
    "    man_knn_result_element = []\n",
    "\n",
    "    for knn2 in knn1:\n",
    "        knn2.test(X_test, Y_test)\n",
    "             \n",
    "        man_knn_result_element.append(accuracy(knn2.results))\n",
    "        \n",
    "        pass\n",
    "    pass\n",
    "    man_knn_results.append(man_knn_result_element)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_compiled_results = []\n",
    "for weight_index in range(0, len(man_knn_results)):\n",
    "    for accuracy_index in range(0, len(man_knn_results[weight_index])):\n",
    "        man_compiled_row = {}\n",
    "        man_compiled_row['K'] = ks[accuracy_index]\n",
    "        if weight_index:\n",
    "            man_compiled_row['Weighted'] = True\n",
    "        else:\n",
    "            man_compiled_row['Weighted'] = False\n",
    "        man_compiled_row['Accuracy'] = man_knn_results[weight_index][accuracy_index]\n",
    "        man_compiled_results.append(man_compiled_row)\n",
    "\n",
    "man_K_on_accuracy_df = pd.DataFrame(man_compiled_results)\n",
    "man_K_on_accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(ks) # this is the number of results we want to plot pn the x-axis\n",
    "ind = np.arange(N) \n",
    "\n",
    "performance1 = man_knn_results[0]\n",
    "performance2 = man_knn_results[1]\n",
    "\n",
    "width = 0.35 # width of the bar      \n",
    "plt.bar(ind, performance1, width, label='Unweighted')\n",
    "plt.bar(ind + width, performance2, width, label='Weighted')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('k')\n",
    "plt.title('kNN performance with increasing')\n",
    "\n",
    "plt.xticks(ind + width / 2, ks)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(aes(x='K', y='Accuracy', color='factor(Weighted)'), data=man_K_on_accuracy_df) +\\\n",
    "    geom_line() + \\\n",
    "    stat_smooth(method = 'lm') +\\\n",
    "    xlab(\"K - Value\") + \\\n",
    "    ylab(\"Accuracy\") + \\\n",
    "    ggtitle(\"K Value's Effect on Model Accuracy.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
