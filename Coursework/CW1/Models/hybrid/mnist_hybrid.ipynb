{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "#for the sigmoid function we need expit() from scipy\n",
    "import scipy.special\n",
    "#library for plotting arrays\n",
    "import matplotlib.pyplot as plt\n",
    "# A particularly interesting backend, provided by IPython, is the inline backend. \n",
    "# This is available only for the Jupyter Notebook and the Jupyter QtConsole. \n",
    "# It can be invoked as follows: %matplotlib inline\n",
    "# With this backend, the output of plotting commands is displayed inline \n",
    "# within frontends like the Jupyter notebook, directly below the code cell that produced it. \n",
    "# The resulting plots are inside this notebook, not an external window.\n",
    "\n",
    "import pandas as pd # to manage data frames and reading csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set our Global Variables\n",
    "later you will need to modify these to present your solution to the Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of input, hidden and output nodes\n",
    "input_nodes = 784 #we have 28 * 28 matrix to describe each digit\n",
    "hidden_nodes = 200\n",
    "output_nodes = 10\n",
    "\n",
    "learning_rate = 0.3\n",
    "batch_size = 10\n",
    "\n",
    "# epochs is the number of training iterations \n",
    "epochs = 30\n",
    "\n",
    "# datasets to read\n",
    "# you can change these when trying out other datasets\n",
    "train_file = \"data/mnist_train.csv\"\n",
    "test_file = \"data/mnist_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:  60000\n",
      "test set size:  10000\n"
     ]
    }
   ],
   "source": [
    "#load the mnist training data CSV file into a list\n",
    "train_data_file = open(train_file, 'r')\n",
    "train_data_list = train_data_file.readlines() # read all lines into memory \n",
    "train_data_file.close() \n",
    "print(\"train set size: \", len(train_data_list))\n",
    "\n",
    "#testing the network\n",
    "#load the mnist test data CSV file into a list\n",
    "#test_data_file = open(\"mnist/mnist_test_10.csv\", 'r') # read the file with 10 instances first\n",
    "test_data_file = open(test_file, 'r') # read the file with 10 instances first\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()\n",
    "print(\"test set size: \", len(test_data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    \"\"\"Artificial Neural Network classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    lr : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    ep : int\n",
    "      Number of epochs for training the network towards achieving convergence\n",
    "    batch_size : int\n",
    "      Size of the training batch to be used when calculating the gradient descent. \n",
    "      batch_size = 0 standard gradient descent\n",
    "      batch_size > 0 stochastic gradient descent \n",
    "\n",
    "    inodes : int\n",
    "      Number of input nodes which is normally the number of features in an instance.\n",
    "    hnodes : int\n",
    "      Number of hidden nodes in the net.\n",
    "    onodes : int\n",
    "      Number of output nodes in the net.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    wih : 2d-array\n",
    "      Input2Hidden node weights after fitting \n",
    "    who : 2d-array\n",
    "      Hidden2Output node weights after fitting \n",
    "    E : list\n",
    "      Sum-of-squares error value in each epoch.\n",
    "      \n",
    "    Results : list\n",
    "      Target and predicted class labels for the test data.\n",
    "      \n",
    "    Functions\n",
    "    ---------\n",
    "    activation_function : float (between 1 and -1)\n",
    "        implments the sigmoid function which squashes the node input\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputnodes=784, hiddennodes=200, outputnodes=10, learningrate=0.3, batch_size=10, epochs=30):\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        #link weight matrices, wih (input to hidden) and who (hidden to output)\n",
    "        #a weight on link from node i to node j is w_ij\n",
    "        \n",
    "        \n",
    "        #Draw random samples from a normal (Gaussian) distribution centered around 0.\n",
    "        #numpy.random.normal(loc to centre gaussian=0.0, scale=1, size=dimensions of the array we want) \n",
    "        #scale is usually set to the standard deviation which is related to the number of incoming links i.e. \n",
    "        #1/sqrt(num of incoming inputs). we use pow to raise it to the power of -0.5.\n",
    "        #We have set 0 as the centre of the guassian dist.\n",
    "        # size is set to the dimensions of the number of hnodes, inodes and onodes\n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        #set the learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        #set the batch size\n",
    "        self.bs = batch_size\n",
    "        \n",
    "        #set the number of epochs\n",
    "        self.ep = epochs\n",
    "        \n",
    "        #store errors at each epoch\n",
    "        self.E= []\n",
    "        \n",
    "        #store results from testing the model\n",
    "        #keep track of the network performance on each test instance\n",
    "        self.results= []\n",
    "        \n",
    "        #define the activation function here\n",
    "        #specify the sigmoid squashing function. Here expit() provides the sigmoid function.\n",
    "        #lambda is a short cut function which is executed there and then with no def (i.e. like an anonymous function)\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "   \n",
    "    def batch_input(self, input_list):\n",
    "        \"\"\"Yield consecutive batches of the specified size from the input list.\"\"\"\n",
    "        for i in range(0, len(input_list), self.bs):\n",
    "            yield input_list[i:i + self.bs]\n",
    "    \n",
    "    #train the neural net\n",
    "    #note the first part is very similar to the query function because they both require the forward pass\n",
    "    def train(self, train_inputs):\n",
    "        \"\"\"Training the neural net. \n",
    "           This includes the forward pass ; error computation; \n",
    "           backprop of the error ; calculation of gradients and updating the weights.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            train_inputs : {array-like}, shape = [n_instances, n_features]\n",
    "            Training vectors, where n_instances is the number of training instances and\n",
    "            n_features is the number of features.\n",
    "            Note this contains all features including the class feature which is in first position\n",
    "        \n",
    "            Returns\n",
    "            -------\n",
    "            self : object\n",
    "        \"\"\"\n",
    "        self.hybrid_values = []\n",
    "        self.hybrid_class = []\n",
    "        for e in range(self.ep):\n",
    "            print(\"Training epoch#: \", e)\n",
    "            sum_error = 0.0   \n",
    "            for batch in self.batch_input(train_inputs):\n",
    "                #creating variables to store the gradients   \n",
    "                delta_who = 0\n",
    "                delta_wih = 0\n",
    "                \n",
    "                # iterate through the inputs sent in\n",
    "                for instance in batch:\n",
    "                    # split it by the commas\n",
    "                    all_values = instance.split(',')\n",
    "                    self.hybrid_class.append(all_values[0])\n",
    "                    # scale and shift the inputs to address the problem of diminishing weights due to multiplying by zero\n",
    "                    # divide the raw inputs which are in the range 0-255 by 255 will bring them into the range 0-1\n",
    "                    # multiply by 0.99 to bring them into the range 0.0 - 0.99.\n",
    "                    # add 0.01 to shift them up to the desired range 0.01 - 1. \n",
    "                    inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "                    #create the target output values for each instance so that we can use it with the neural net\n",
    "                    #note we need 10 nodes where each represents one of the digits\n",
    "                    targets = np.zeros(output_nodes) + 0.01 #all initialised to 0.01\n",
    "                    #all_value[0] has the target class label for this instance\n",
    "                    targets[int(all_values[0])] = 0.99\n",
    "        \n",
    "                    #convert  inputs list to 2d array\n",
    "                    inputs = np.array(inputs,  ndmin=2).T\n",
    "                    targets = np.array(targets, ndmin=2).T\n",
    "\n",
    "                    #calculate signals into hidden layer\n",
    "                    hidden_inputs = np.dot(self.wih, inputs)\n",
    "                    #calculate the signals emerging from the hidden layer\n",
    "                    hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "                    #calculate signals into final output layer\n",
    "                    final_inputs=np.dot(self.who, hidden_outputs)\n",
    "                    \n",
    "                    self.hybrid_values.append(hidden_outputs)\n",
    "                    #calculate the signals emerging from final output layer\n",
    "                    final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "                    #to calculate the error we need to compute the element wise diff between target and actual\n",
    "                    output_errors = targets - final_outputs\n",
    "                    #Next distribute the error to the hidden layer such that hidden layer error\n",
    "                    #is the output_errors, split by weights, recombined at hidden nodes\n",
    "                    hidden_errors = np.dot(self.who.T, output_errors)\n",
    "            \n",
    "                       \n",
    "                    ## for each instance accumilate the gradients from each instance\n",
    "                    ## delta_who are the gradients between hidden and output weights\n",
    "                    ## delta_wih are the gradients between input and hidden weights\n",
    "                    delta_who += np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "                    delta_wih += np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "                    \n",
    "                    sum_error += np.dot(output_errors.T, output_errors)#this is the sum of squared error accumilated over each batced instance\n",
    "                   \n",
    "                pass #instance\n",
    "            \n",
    "                # update the weights by multiplying the gradient with the learning rate\n",
    "                # note that the deltas are divided by batch size to obtain the average gradient according to the given batch\n",
    "                # obviously if batch size = 1 then we dont need to bother with an average\n",
    "                self.who += self.lr * (delta_who / self.bs)\n",
    "                self.wih += self.lr * (delta_wih / self.bs)\n",
    "            pass # batch\n",
    "            self.E.append(np.asfarray(sum_error).flatten())\n",
    "            print(\"errors (SSE): \", self.E[-1])\n",
    "        pass # epoch\n",
    "    \n",
    "    #query the neural net\n",
    "    def query(self, inputs_list):\n",
    "        #convert inputs_list to a 2d array\n",
    "        #print(numpy.matrix(inputs_list))\n",
    "        #inputs_list [[ 1.   0.5 -1.5]]\n",
    "        inputs = np.array(inputs_list, ndmin=2).T \n",
    "        #once converted it appears as follows\n",
    "        #[[ 1. ]\n",
    "        # [ 0.5]\n",
    "        # [-1.5]]\n",
    "        #print(numpy.matrix(inputs))\n",
    "        \n",
    "        #propogate input into hidden layer. This is the start of the forward pass\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        \n",
    "        \n",
    "        #squash the content in the hidden node using the sigmoid function (value between 1, -1)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "                \n",
    "        #propagate into output layer and the apply the squashing sigmoid function\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        \n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        return final_outputs\n",
    "    \n",
    "     \n",
    "    #iterate through all the test data to calculate model accuracy\n",
    "    def test(self, test_inputs):\n",
    "        self.results = []\n",
    "        \n",
    "        #go through each test instances\n",
    "        for instance in test_inputs:\n",
    "            all_values = instance.split(',') # extract the input feature values for the instance\n",
    "    \n",
    "            target_label = int(all_values[0]) # get the target class for the instance\n",
    "    \n",
    "            #scale and shift the inputs this is to make sure values dont lead to zero when multiplied with weights\n",
    "            inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "    \n",
    "            #query the network with test inputs\n",
    "            #note this returns 10 output values ; of which the index of the highest value\n",
    "            # is the networks predicted class label\n",
    "            outputs = self.query(inputs)\n",
    "    \n",
    "            #get the index of the highest output node as this corresponds to the predicted class\n",
    "            predict_label = np.argmax(outputs) #this is the class predicted by the ANN\n",
    "    \n",
    "            self.results.append([predict_label, target_label])\n",
    "            #compute network error\n",
    "            #if (predict_label == target_label):\n",
    "            #    self.results.append(1)\n",
    "            #else: \n",
    "            #    self.results.append(0)\n",
    "            pass\n",
    "        pass\n",
    "        self.results = np.asfarray(self.results) # flatten results to avoid nested arrays\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Artificial Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of training data used: 2.5\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [601.59229778]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [322.50894568]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [266.29124565]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [230.16940996]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [202.85824199]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [181.03821225]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [163.0610146]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [147.93148176]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [135.09980422]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [124.12942223]\n",
      "Training epoch#:  10\n",
      "errors (SSE):  [114.56135179]\n",
      "Training epoch#:  11\n",
      "errors (SSE):  [106.0691998]\n",
      "Training epoch#:  12\n",
      "errors (SSE):  [98.43124873]\n",
      "Training epoch#:  13\n",
      "errors (SSE):  [91.46580164]\n",
      "Training epoch#:  14\n",
      "errors (SSE):  [85.06148525]\n",
      "Training epoch#:  15\n",
      "errors (SSE):  [79.15790905]\n",
      "Training epoch#:  16\n",
      "errors (SSE):  [73.72932086]\n",
      "Training epoch#:  17\n",
      "errors (SSE):  [68.77283096]\n",
      "Training epoch#:  18\n",
      "errors (SSE):  [64.24479121]\n",
      "Training epoch#:  19\n",
      "errors (SSE):  [60.07006401]\n",
      "Training epoch#:  20\n",
      "errors (SSE):  [56.1827311]\n",
      "Training epoch#:  21\n",
      "errors (SSE):  [52.55245468]\n",
      "Training epoch#:  22\n",
      "errors (SSE):  [49.16934772]\n",
      "Training epoch#:  23\n",
      "errors (SSE):  [46.02803851]\n",
      "Training epoch#:  24\n",
      "errors (SSE):  [43.11472866]\n",
      "Training epoch#:  25\n",
      "errors (SSE):  [40.40584214]\n",
      "Training epoch#:  26\n",
      "errors (SSE):  [37.8906106]\n",
      "Training epoch#:  27\n",
      "errors (SSE):  [35.56843513]\n",
      "Training epoch#:  28\n",
      "errors (SSE):  [33.4259061]\n",
      "Training epoch#:  29\n",
      "errors (SSE):  [31.44436203]\n"
     ]
    }
   ],
   "source": [
    "#create instance of neuralnet\n",
    "n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate, batch_size, epochs)\n",
    "\n",
    "# numpy.random.choice generates a random sample from a given 1-D array\n",
    "# we can use this to select a sample from our training data in case we want to work with a small sample\n",
    "# for instance we use a small sample here such as 1500\n",
    "# mini_training_data = np.random.choice(train_data_list, 1500, replace = False)\n",
    "mini_training_data = train_data_list[:1500]\n",
    "print(\"Percentage of training data used:\", (len(mini_training_data)/len(train_data_list)) * 100)\n",
    "n.train(mini_training_data)\n",
    "\n",
    "# n.train(train_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the model error with increasing epochs\n",
    "\n",
    "Error at the end of each epoch has been stored in self.E\n",
    "We can now use mathplotlib to plot the error at the end of each epoch. \n",
    "Our expectation is that as we continue to descend (hill-walking) we should move closer to the minima\n",
    "as such error should decrease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJUAAAFACAYAAAAF/03lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZIkaWz/AQFGigZO0a6QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print the error over epochs\n",
    "\n",
    "plt.figure(figsize=(20,5)) #width, height settings for figures\n",
    "plt.plot(range(0, n.ep), np.asfarray(n.E), marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of errors')\n",
    "\n",
    "\n",
    "\n",
    "# plt.savefig('images/02_07.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the ANN and compute the Accuracy\n",
    "We will keep track of the predicted and actual outputs in order to \n",
    "calculate the accuracy of the model on the unseen test data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy% =  90.21\n"
     ]
    }
   ],
   "source": [
    "n.test(test_data_list)\n",
    "#print network performance as an accuracy metric\n",
    "correct = 0 # number of predictions that were correct\n",
    "\n",
    "#iteratre through each tested instance and accumilate number of correct predictions\n",
    "for result in n.results:\n",
    "    if (result[0] == result[1]):\n",
    "            correct += 1\n",
    "    pass\n",
    "pass\n",
    "\n",
    "# print the accuracy on test set\n",
    "print (\"Test set accuracy% = \", (100 * correct / len(n.results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "\n",
    "for row in range(0, len(n.hybrid_values)):\n",
    "    my_row = []\n",
    "    my_row.append(n.hybrid_class[row])\n",
    "    for col in range(0, len(n.hybrid_values[row])):\n",
    "        my_row.append(n.hybrid_values[row][col][0])\n",
    "    new_dataset.append(my_row)\n",
    "                   \n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_data = pd.DataFrame(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.494230</td>\n",
       "      <td>0.467823</td>\n",
       "      <td>0.595076</td>\n",
       "      <td>0.388914</td>\n",
       "      <td>0.467764</td>\n",
       "      <td>0.517085</td>\n",
       "      <td>0.533080</td>\n",
       "      <td>0.564102</td>\n",
       "      <td>0.429205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456384</td>\n",
       "      <td>0.617726</td>\n",
       "      <td>0.401893</td>\n",
       "      <td>0.437372</td>\n",
       "      <td>0.510095</td>\n",
       "      <td>0.512113</td>\n",
       "      <td>0.501598</td>\n",
       "      <td>0.475575</td>\n",
       "      <td>0.445245</td>\n",
       "      <td>0.380133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.624591</td>\n",
       "      <td>0.424304</td>\n",
       "      <td>0.650029</td>\n",
       "      <td>0.391525</td>\n",
       "      <td>0.421695</td>\n",
       "      <td>0.421476</td>\n",
       "      <td>0.312537</td>\n",
       "      <td>0.534023</td>\n",
       "      <td>0.486113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439536</td>\n",
       "      <td>0.652906</td>\n",
       "      <td>0.423631</td>\n",
       "      <td>0.478065</td>\n",
       "      <td>0.608949</td>\n",
       "      <td>0.648437</td>\n",
       "      <td>0.423185</td>\n",
       "      <td>0.504391</td>\n",
       "      <td>0.322251</td>\n",
       "      <td>0.527458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.617798</td>\n",
       "      <td>0.534772</td>\n",
       "      <td>0.571680</td>\n",
       "      <td>0.465620</td>\n",
       "      <td>0.483293</td>\n",
       "      <td>0.453296</td>\n",
       "      <td>0.568256</td>\n",
       "      <td>0.489467</td>\n",
       "      <td>0.533587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.490198</td>\n",
       "      <td>0.593565</td>\n",
       "      <td>0.375059</td>\n",
       "      <td>0.573670</td>\n",
       "      <td>0.497906</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>0.483968</td>\n",
       "      <td>0.440581</td>\n",
       "      <td>0.465039</td>\n",
       "      <td>0.483879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.423545</td>\n",
       "      <td>0.390823</td>\n",
       "      <td>0.529562</td>\n",
       "      <td>0.364918</td>\n",
       "      <td>0.543253</td>\n",
       "      <td>0.414250</td>\n",
       "      <td>0.444813</td>\n",
       "      <td>0.500588</td>\n",
       "      <td>0.424529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447035</td>\n",
       "      <td>0.466065</td>\n",
       "      <td>0.400395</td>\n",
       "      <td>0.578492</td>\n",
       "      <td>0.528461</td>\n",
       "      <td>0.351093</td>\n",
       "      <td>0.502654</td>\n",
       "      <td>0.497462</td>\n",
       "      <td>0.509921</td>\n",
       "      <td>0.440470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0.434693</td>\n",
       "      <td>0.400409</td>\n",
       "      <td>0.684090</td>\n",
       "      <td>0.443029</td>\n",
       "      <td>0.540683</td>\n",
       "      <td>0.388294</td>\n",
       "      <td>0.455705</td>\n",
       "      <td>0.571975</td>\n",
       "      <td>0.489039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443192</td>\n",
       "      <td>0.491207</td>\n",
       "      <td>0.287157</td>\n",
       "      <td>0.473730</td>\n",
       "      <td>0.447597</td>\n",
       "      <td>0.422574</td>\n",
       "      <td>0.480675</td>\n",
       "      <td>0.392080</td>\n",
       "      <td>0.567889</td>\n",
       "      <td>0.363806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.658381</td>\n",
       "      <td>0.430957</td>\n",
       "      <td>0.573086</td>\n",
       "      <td>0.499354</td>\n",
       "      <td>0.555867</td>\n",
       "      <td>0.326530</td>\n",
       "      <td>0.501080</td>\n",
       "      <td>0.600948</td>\n",
       "      <td>0.427330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443569</td>\n",
       "      <td>0.611450</td>\n",
       "      <td>0.280616</td>\n",
       "      <td>0.537071</td>\n",
       "      <td>0.458861</td>\n",
       "      <td>0.391992</td>\n",
       "      <td>0.474362</td>\n",
       "      <td>0.437483</td>\n",
       "      <td>0.489106</td>\n",
       "      <td>0.412709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.513190</td>\n",
       "      <td>0.363113</td>\n",
       "      <td>0.622211</td>\n",
       "      <td>0.478259</td>\n",
       "      <td>0.515056</td>\n",
       "      <td>0.447807</td>\n",
       "      <td>0.418159</td>\n",
       "      <td>0.544871</td>\n",
       "      <td>0.543357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419029</td>\n",
       "      <td>0.513147</td>\n",
       "      <td>0.550448</td>\n",
       "      <td>0.519971</td>\n",
       "      <td>0.434781</td>\n",
       "      <td>0.330372</td>\n",
       "      <td>0.565793</td>\n",
       "      <td>0.518561</td>\n",
       "      <td>0.402986</td>\n",
       "      <td>0.399268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0.660594</td>\n",
       "      <td>0.491482</td>\n",
       "      <td>0.610683</td>\n",
       "      <td>0.373437</td>\n",
       "      <td>0.583428</td>\n",
       "      <td>0.517546</td>\n",
       "      <td>0.478933</td>\n",
       "      <td>0.611986</td>\n",
       "      <td>0.499411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405028</td>\n",
       "      <td>0.577689</td>\n",
       "      <td>0.361144</td>\n",
       "      <td>0.554764</td>\n",
       "      <td>0.457009</td>\n",
       "      <td>0.529344</td>\n",
       "      <td>0.358498</td>\n",
       "      <td>0.503486</td>\n",
       "      <td>0.456975</td>\n",
       "      <td>0.319535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.543546</td>\n",
       "      <td>0.389894</td>\n",
       "      <td>0.537208</td>\n",
       "      <td>0.511608</td>\n",
       "      <td>0.501795</td>\n",
       "      <td>0.424094</td>\n",
       "      <td>0.501251</td>\n",
       "      <td>0.471739</td>\n",
       "      <td>0.548973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423703</td>\n",
       "      <td>0.448869</td>\n",
       "      <td>0.498349</td>\n",
       "      <td>0.504824</td>\n",
       "      <td>0.426270</td>\n",
       "      <td>0.399242</td>\n",
       "      <td>0.509458</td>\n",
       "      <td>0.468201</td>\n",
       "      <td>0.491694</td>\n",
       "      <td>0.425091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>0.674716</td>\n",
       "      <td>0.399831</td>\n",
       "      <td>0.569309</td>\n",
       "      <td>0.497585</td>\n",
       "      <td>0.489677</td>\n",
       "      <td>0.417305</td>\n",
       "      <td>0.558413</td>\n",
       "      <td>0.667773</td>\n",
       "      <td>0.458999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453993</td>\n",
       "      <td>0.631944</td>\n",
       "      <td>0.398568</td>\n",
       "      <td>0.525464</td>\n",
       "      <td>0.347463</td>\n",
       "      <td>0.552696</td>\n",
       "      <td>0.460307</td>\n",
       "      <td>0.558235</td>\n",
       "      <td>0.560515</td>\n",
       "      <td>0.306293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>0.888166</td>\n",
       "      <td>0.744576</td>\n",
       "      <td>0.287174</td>\n",
       "      <td>0.258629</td>\n",
       "      <td>0.681662</td>\n",
       "      <td>0.794190</td>\n",
       "      <td>0.044762</td>\n",
       "      <td>0.561346</td>\n",
       "      <td>0.959060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099449</td>\n",
       "      <td>0.890696</td>\n",
       "      <td>0.305130</td>\n",
       "      <td>0.105383</td>\n",
       "      <td>0.952261</td>\n",
       "      <td>0.860016</td>\n",
       "      <td>0.355413</td>\n",
       "      <td>0.035215</td>\n",
       "      <td>0.341627</td>\n",
       "      <td>0.025696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>0.730373</td>\n",
       "      <td>0.571258</td>\n",
       "      <td>0.365662</td>\n",
       "      <td>0.351842</td>\n",
       "      <td>0.640210</td>\n",
       "      <td>0.525977</td>\n",
       "      <td>0.207095</td>\n",
       "      <td>0.537340</td>\n",
       "      <td>0.775089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313269</td>\n",
       "      <td>0.742767</td>\n",
       "      <td>0.445088</td>\n",
       "      <td>0.355543</td>\n",
       "      <td>0.656129</td>\n",
       "      <td>0.634581</td>\n",
       "      <td>0.383048</td>\n",
       "      <td>0.282756</td>\n",
       "      <td>0.570108</td>\n",
       "      <td>0.266972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>0.911273</td>\n",
       "      <td>0.720311</td>\n",
       "      <td>0.221945</td>\n",
       "      <td>0.268593</td>\n",
       "      <td>0.614894</td>\n",
       "      <td>0.608913</td>\n",
       "      <td>0.067280</td>\n",
       "      <td>0.478254</td>\n",
       "      <td>0.942158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140734</td>\n",
       "      <td>0.838847</td>\n",
       "      <td>0.214658</td>\n",
       "      <td>0.134282</td>\n",
       "      <td>0.921574</td>\n",
       "      <td>0.813057</td>\n",
       "      <td>0.356563</td>\n",
       "      <td>0.046148</td>\n",
       "      <td>0.382962</td>\n",
       "      <td>0.043845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>0.932558</td>\n",
       "      <td>0.677074</td>\n",
       "      <td>0.352891</td>\n",
       "      <td>0.336862</td>\n",
       "      <td>0.529162</td>\n",
       "      <td>0.698485</td>\n",
       "      <td>0.049965</td>\n",
       "      <td>0.570361</td>\n",
       "      <td>0.958085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162953</td>\n",
       "      <td>0.872084</td>\n",
       "      <td>0.378509</td>\n",
       "      <td>0.111591</td>\n",
       "      <td>0.919121</td>\n",
       "      <td>0.905419</td>\n",
       "      <td>0.434574</td>\n",
       "      <td>0.036498</td>\n",
       "      <td>0.594042</td>\n",
       "      <td>0.043124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.691502</td>\n",
       "      <td>0.585320</td>\n",
       "      <td>0.340819</td>\n",
       "      <td>0.462442</td>\n",
       "      <td>0.625349</td>\n",
       "      <td>0.609365</td>\n",
       "      <td>0.198291</td>\n",
       "      <td>0.449403</td>\n",
       "      <td>0.856750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280659</td>\n",
       "      <td>0.629646</td>\n",
       "      <td>0.542666</td>\n",
       "      <td>0.262826</td>\n",
       "      <td>0.764815</td>\n",
       "      <td>0.683579</td>\n",
       "      <td>0.466251</td>\n",
       "      <td>0.144478</td>\n",
       "      <td>0.515523</td>\n",
       "      <td>0.145600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>0.836191</td>\n",
       "      <td>0.646481</td>\n",
       "      <td>0.355989</td>\n",
       "      <td>0.321231</td>\n",
       "      <td>0.583676</td>\n",
       "      <td>0.692851</td>\n",
       "      <td>0.062673</td>\n",
       "      <td>0.579816</td>\n",
       "      <td>0.924085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166864</td>\n",
       "      <td>0.890528</td>\n",
       "      <td>0.409879</td>\n",
       "      <td>0.157178</td>\n",
       "      <td>0.921679</td>\n",
       "      <td>0.802660</td>\n",
       "      <td>0.377529</td>\n",
       "      <td>0.045518</td>\n",
       "      <td>0.582008</td>\n",
       "      <td>0.059110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>0.808383</td>\n",
       "      <td>0.698526</td>\n",
       "      <td>0.303217</td>\n",
       "      <td>0.343251</td>\n",
       "      <td>0.525590</td>\n",
       "      <td>0.700091</td>\n",
       "      <td>0.101771</td>\n",
       "      <td>0.689810</td>\n",
       "      <td>0.893156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197315</td>\n",
       "      <td>0.866733</td>\n",
       "      <td>0.358386</td>\n",
       "      <td>0.160402</td>\n",
       "      <td>0.907846</td>\n",
       "      <td>0.747773</td>\n",
       "      <td>0.452527</td>\n",
       "      <td>0.086284</td>\n",
       "      <td>0.390670</td>\n",
       "      <td>0.069377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>0.873532</td>\n",
       "      <td>0.684099</td>\n",
       "      <td>0.256418</td>\n",
       "      <td>0.281667</td>\n",
       "      <td>0.766444</td>\n",
       "      <td>0.745901</td>\n",
       "      <td>0.056293</td>\n",
       "      <td>0.660158</td>\n",
       "      <td>0.934863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119674</td>\n",
       "      <td>0.856460</td>\n",
       "      <td>0.205770</td>\n",
       "      <td>0.134903</td>\n",
       "      <td>0.913404</td>\n",
       "      <td>0.854995</td>\n",
       "      <td>0.426901</td>\n",
       "      <td>0.044870</td>\n",
       "      <td>0.547123</td>\n",
       "      <td>0.036158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>0.835911</td>\n",
       "      <td>0.577467</td>\n",
       "      <td>0.415803</td>\n",
       "      <td>0.393485</td>\n",
       "      <td>0.425256</td>\n",
       "      <td>0.601619</td>\n",
       "      <td>0.182119</td>\n",
       "      <td>0.464801</td>\n",
       "      <td>0.791385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273740</td>\n",
       "      <td>0.748247</td>\n",
       "      <td>0.354975</td>\n",
       "      <td>0.285135</td>\n",
       "      <td>0.739539</td>\n",
       "      <td>0.806190</td>\n",
       "      <td>0.467419</td>\n",
       "      <td>0.169052</td>\n",
       "      <td>0.503222</td>\n",
       "      <td>0.130981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>0.811964</td>\n",
       "      <td>0.653062</td>\n",
       "      <td>0.304647</td>\n",
       "      <td>0.322584</td>\n",
       "      <td>0.605160</td>\n",
       "      <td>0.639365</td>\n",
       "      <td>0.088234</td>\n",
       "      <td>0.568141</td>\n",
       "      <td>0.890989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191538</td>\n",
       "      <td>0.794257</td>\n",
       "      <td>0.268252</td>\n",
       "      <td>0.185505</td>\n",
       "      <td>0.856023</td>\n",
       "      <td>0.740085</td>\n",
       "      <td>0.413501</td>\n",
       "      <td>0.075334</td>\n",
       "      <td>0.550353</td>\n",
       "      <td>0.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>0.871545</td>\n",
       "      <td>0.546188</td>\n",
       "      <td>0.409877</td>\n",
       "      <td>0.359726</td>\n",
       "      <td>0.370240</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.050762</td>\n",
       "      <td>0.365169</td>\n",
       "      <td>0.941842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161915</td>\n",
       "      <td>0.806814</td>\n",
       "      <td>0.303166</td>\n",
       "      <td>0.253495</td>\n",
       "      <td>0.921550</td>\n",
       "      <td>0.859913</td>\n",
       "      <td>0.397244</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>0.454205</td>\n",
       "      <td>0.046526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0.916786</td>\n",
       "      <td>0.729819</td>\n",
       "      <td>0.478744</td>\n",
       "      <td>0.301027</td>\n",
       "      <td>0.394804</td>\n",
       "      <td>0.632818</td>\n",
       "      <td>0.026532</td>\n",
       "      <td>0.262696</td>\n",
       "      <td>0.940198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170091</td>\n",
       "      <td>0.879354</td>\n",
       "      <td>0.405263</td>\n",
       "      <td>0.191217</td>\n",
       "      <td>0.971411</td>\n",
       "      <td>0.931820</td>\n",
       "      <td>0.325644</td>\n",
       "      <td>0.046513</td>\n",
       "      <td>0.286869</td>\n",
       "      <td>0.035229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9</td>\n",
       "      <td>0.724706</td>\n",
       "      <td>0.611188</td>\n",
       "      <td>0.469134</td>\n",
       "      <td>0.438071</td>\n",
       "      <td>0.470617</td>\n",
       "      <td>0.579553</td>\n",
       "      <td>0.120608</td>\n",
       "      <td>0.497411</td>\n",
       "      <td>0.870381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281535</td>\n",
       "      <td>0.688322</td>\n",
       "      <td>0.384792</td>\n",
       "      <td>0.328197</td>\n",
       "      <td>0.855929</td>\n",
       "      <td>0.778049</td>\n",
       "      <td>0.436984</td>\n",
       "      <td>0.113921</td>\n",
       "      <td>0.514604</td>\n",
       "      <td>0.079988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0.622919</td>\n",
       "      <td>0.562617</td>\n",
       "      <td>0.412245</td>\n",
       "      <td>0.327718</td>\n",
       "      <td>0.509545</td>\n",
       "      <td>0.582358</td>\n",
       "      <td>0.130504</td>\n",
       "      <td>0.417352</td>\n",
       "      <td>0.848702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248423</td>\n",
       "      <td>0.660609</td>\n",
       "      <td>0.439858</td>\n",
       "      <td>0.372068</td>\n",
       "      <td>0.854393</td>\n",
       "      <td>0.650994</td>\n",
       "      <td>0.432960</td>\n",
       "      <td>0.114071</td>\n",
       "      <td>0.488626</td>\n",
       "      <td>0.117873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0.743678</td>\n",
       "      <td>0.553682</td>\n",
       "      <td>0.482105</td>\n",
       "      <td>0.388093</td>\n",
       "      <td>0.484413</td>\n",
       "      <td>0.550554</td>\n",
       "      <td>0.097940</td>\n",
       "      <td>0.450786</td>\n",
       "      <td>0.906241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277929</td>\n",
       "      <td>0.779908</td>\n",
       "      <td>0.434099</td>\n",
       "      <td>0.282789</td>\n",
       "      <td>0.913636</td>\n",
       "      <td>0.737157</td>\n",
       "      <td>0.484573</td>\n",
       "      <td>0.118982</td>\n",
       "      <td>0.371722</td>\n",
       "      <td>0.086782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>0.943022</td>\n",
       "      <td>0.752091</td>\n",
       "      <td>0.363346</td>\n",
       "      <td>0.290115</td>\n",
       "      <td>0.607876</td>\n",
       "      <td>0.652248</td>\n",
       "      <td>0.030996</td>\n",
       "      <td>0.504306</td>\n",
       "      <td>0.970384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106968</td>\n",
       "      <td>0.867201</td>\n",
       "      <td>0.336852</td>\n",
       "      <td>0.166482</td>\n",
       "      <td>0.956981</td>\n",
       "      <td>0.920333</td>\n",
       "      <td>0.288736</td>\n",
       "      <td>0.017438</td>\n",
       "      <td>0.370443</td>\n",
       "      <td>0.011639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>0.711161</td>\n",
       "      <td>0.599640</td>\n",
       "      <td>0.522103</td>\n",
       "      <td>0.403097</td>\n",
       "      <td>0.429447</td>\n",
       "      <td>0.573243</td>\n",
       "      <td>0.183740</td>\n",
       "      <td>0.484845</td>\n",
       "      <td>0.850281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241864</td>\n",
       "      <td>0.717971</td>\n",
       "      <td>0.465129</td>\n",
       "      <td>0.257659</td>\n",
       "      <td>0.783623</td>\n",
       "      <td>0.709401</td>\n",
       "      <td>0.440979</td>\n",
       "      <td>0.119517</td>\n",
       "      <td>0.501293</td>\n",
       "      <td>0.151322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>0.951061</td>\n",
       "      <td>0.817977</td>\n",
       "      <td>0.298123</td>\n",
       "      <td>0.252601</td>\n",
       "      <td>0.598677</td>\n",
       "      <td>0.713036</td>\n",
       "      <td>0.013614</td>\n",
       "      <td>0.376497</td>\n",
       "      <td>0.990110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086321</td>\n",
       "      <td>0.929248</td>\n",
       "      <td>0.313767</td>\n",
       "      <td>0.115455</td>\n",
       "      <td>0.979463</td>\n",
       "      <td>0.949778</td>\n",
       "      <td>0.234263</td>\n",
       "      <td>0.012141</td>\n",
       "      <td>0.340643</td>\n",
       "      <td>0.008591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>0.892253</td>\n",
       "      <td>0.583868</td>\n",
       "      <td>0.452556</td>\n",
       "      <td>0.395637</td>\n",
       "      <td>0.443463</td>\n",
       "      <td>0.648370</td>\n",
       "      <td>0.063307</td>\n",
       "      <td>0.392715</td>\n",
       "      <td>0.973024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139701</td>\n",
       "      <td>0.902834</td>\n",
       "      <td>0.442264</td>\n",
       "      <td>0.139691</td>\n",
       "      <td>0.951943</td>\n",
       "      <td>0.872757</td>\n",
       "      <td>0.213484</td>\n",
       "      <td>0.022181</td>\n",
       "      <td>0.432048</td>\n",
       "      <td>0.020084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7</td>\n",
       "      <td>0.708107</td>\n",
       "      <td>0.521213</td>\n",
       "      <td>0.455892</td>\n",
       "      <td>0.400241</td>\n",
       "      <td>0.432592</td>\n",
       "      <td>0.543961</td>\n",
       "      <td>0.163184</td>\n",
       "      <td>0.472505</td>\n",
       "      <td>0.884560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315642</td>\n",
       "      <td>0.680063</td>\n",
       "      <td>0.430113</td>\n",
       "      <td>0.361468</td>\n",
       "      <td>0.830389</td>\n",
       "      <td>0.664809</td>\n",
       "      <td>0.399507</td>\n",
       "      <td>0.134994</td>\n",
       "      <td>0.488589</td>\n",
       "      <td>0.112811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44970</th>\n",
       "      <td>3</td>\n",
       "      <td>0.097989</td>\n",
       "      <td>0.790560</td>\n",
       "      <td>0.156103</td>\n",
       "      <td>0.121637</td>\n",
       "      <td>0.256388</td>\n",
       "      <td>0.835986</td>\n",
       "      <td>0.026902</td>\n",
       "      <td>0.598115</td>\n",
       "      <td>0.991415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.874232</td>\n",
       "      <td>0.756598</td>\n",
       "      <td>0.043511</td>\n",
       "      <td>0.100588</td>\n",
       "      <td>0.986283</td>\n",
       "      <td>0.995441</td>\n",
       "      <td>0.053994</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.002784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44971</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.994704</td>\n",
       "      <td>0.999267</td>\n",
       "      <td>0.013207</td>\n",
       "      <td>0.117074</td>\n",
       "      <td>0.091070</td>\n",
       "      <td>0.008191</td>\n",
       "      <td>0.016838</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991500</td>\n",
       "      <td>0.993681</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>0.985113</td>\n",
       "      <td>0.999766</td>\n",
       "      <td>0.999891</td>\n",
       "      <td>0.015148</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.998367</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44972</th>\n",
       "      <td>1</td>\n",
       "      <td>0.073935</td>\n",
       "      <td>0.896097</td>\n",
       "      <td>0.156712</td>\n",
       "      <td>0.445479</td>\n",
       "      <td>0.220772</td>\n",
       "      <td>0.765188</td>\n",
       "      <td>0.563065</td>\n",
       "      <td>0.125587</td>\n",
       "      <td>0.997690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831369</td>\n",
       "      <td>0.112692</td>\n",
       "      <td>0.924809</td>\n",
       "      <td>0.376936</td>\n",
       "      <td>0.993158</td>\n",
       "      <td>0.982778</td>\n",
       "      <td>0.672422</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.001596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44973</th>\n",
       "      <td>4</td>\n",
       "      <td>0.382876</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>0.479877</td>\n",
       "      <td>0.305806</td>\n",
       "      <td>0.365757</td>\n",
       "      <td>0.688519</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.962252</td>\n",
       "      <td>0.995841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.904691</td>\n",
       "      <td>0.077888</td>\n",
       "      <td>0.956331</td>\n",
       "      <td>0.277207</td>\n",
       "      <td>0.996049</td>\n",
       "      <td>0.998599</td>\n",
       "      <td>0.028825</td>\n",
       "      <td>0.005550</td>\n",
       "      <td>0.032314</td>\n",
       "      <td>0.006991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44974</th>\n",
       "      <td>3</td>\n",
       "      <td>0.063960</td>\n",
       "      <td>0.989253</td>\n",
       "      <td>0.123271</td>\n",
       "      <td>0.397002</td>\n",
       "      <td>0.558801</td>\n",
       "      <td>0.854081</td>\n",
       "      <td>0.011267</td>\n",
       "      <td>0.112918</td>\n",
       "      <td>0.979306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922974</td>\n",
       "      <td>0.780557</td>\n",
       "      <td>0.015007</td>\n",
       "      <td>0.328664</td>\n",
       "      <td>0.977143</td>\n",
       "      <td>0.998310</td>\n",
       "      <td>0.187958</td>\n",
       "      <td>0.008256</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>0.009174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44975</th>\n",
       "      <td>8</td>\n",
       "      <td>0.192237</td>\n",
       "      <td>0.076316</td>\n",
       "      <td>0.954055</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.131128</td>\n",
       "      <td>0.522267</td>\n",
       "      <td>0.122411</td>\n",
       "      <td>0.826195</td>\n",
       "      <td>0.996663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915537</td>\n",
       "      <td>0.120180</td>\n",
       "      <td>0.344185</td>\n",
       "      <td>0.723911</td>\n",
       "      <td>0.996972</td>\n",
       "      <td>0.985409</td>\n",
       "      <td>0.011703</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.963186</td>\n",
       "      <td>0.001447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44976</th>\n",
       "      <td>3</td>\n",
       "      <td>0.033162</td>\n",
       "      <td>0.479455</td>\n",
       "      <td>0.409225</td>\n",
       "      <td>0.237286</td>\n",
       "      <td>0.783323</td>\n",
       "      <td>0.548710</td>\n",
       "      <td>0.483859</td>\n",
       "      <td>0.375211</td>\n",
       "      <td>0.980239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858347</td>\n",
       "      <td>0.845671</td>\n",
       "      <td>0.102894</td>\n",
       "      <td>0.210884</td>\n",
       "      <td>0.993728</td>\n",
       "      <td>0.826089</td>\n",
       "      <td>0.072882</td>\n",
       "      <td>0.009098</td>\n",
       "      <td>0.076956</td>\n",
       "      <td>0.005730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44977</th>\n",
       "      <td>4</td>\n",
       "      <td>0.514909</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>0.363598</td>\n",
       "      <td>0.186163</td>\n",
       "      <td>0.372482</td>\n",
       "      <td>0.633269</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>0.936729</td>\n",
       "      <td>0.998570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971055</td>\n",
       "      <td>0.339339</td>\n",
       "      <td>0.924783</td>\n",
       "      <td>0.250874</td>\n",
       "      <td>0.998267</td>\n",
       "      <td>0.999491</td>\n",
       "      <td>0.016483</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.104279</td>\n",
       "      <td>0.002384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44978</th>\n",
       "      <td>2</td>\n",
       "      <td>0.011036</td>\n",
       "      <td>0.121130</td>\n",
       "      <td>0.997793</td>\n",
       "      <td>0.047162</td>\n",
       "      <td>0.084515</td>\n",
       "      <td>0.045675</td>\n",
       "      <td>0.032084</td>\n",
       "      <td>0.711802</td>\n",
       "      <td>0.983639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967933</td>\n",
       "      <td>0.370669</td>\n",
       "      <td>0.489646</td>\n",
       "      <td>0.977165</td>\n",
       "      <td>0.994197</td>\n",
       "      <td>0.988026</td>\n",
       "      <td>0.050413</td>\n",
       "      <td>0.007772</td>\n",
       "      <td>0.990430</td>\n",
       "      <td>0.003213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44979</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>0.997716</td>\n",
       "      <td>0.999795</td>\n",
       "      <td>0.016177</td>\n",
       "      <td>0.048298</td>\n",
       "      <td>0.156040</td>\n",
       "      <td>0.235444</td>\n",
       "      <td>0.006728</td>\n",
       "      <td>0.998358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989920</td>\n",
       "      <td>0.999898</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.990864</td>\n",
       "      <td>0.999652</td>\n",
       "      <td>0.990202</td>\n",
       "      <td>0.072935</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.996863</td>\n",
       "      <td>0.000166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44980</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.989819</td>\n",
       "      <td>0.943823</td>\n",
       "      <td>0.412973</td>\n",
       "      <td>0.029005</td>\n",
       "      <td>0.267024</td>\n",
       "      <td>0.878164</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.999516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998470</td>\n",
       "      <td>0.999772</td>\n",
       "      <td>0.103359</td>\n",
       "      <td>0.921100</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.957558</td>\n",
       "      <td>0.043646</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.018407</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44981</th>\n",
       "      <td>5</td>\n",
       "      <td>0.698437</td>\n",
       "      <td>0.062164</td>\n",
       "      <td>0.931174</td>\n",
       "      <td>0.455207</td>\n",
       "      <td>0.258440</td>\n",
       "      <td>0.379148</td>\n",
       "      <td>0.101657</td>\n",
       "      <td>0.572843</td>\n",
       "      <td>0.984802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917539</td>\n",
       "      <td>0.663742</td>\n",
       "      <td>0.498389</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>0.981478</td>\n",
       "      <td>0.983677</td>\n",
       "      <td>0.111224</td>\n",
       "      <td>0.018336</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.012052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44982</th>\n",
       "      <td>5</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.978944</td>\n",
       "      <td>0.718751</td>\n",
       "      <td>0.050628</td>\n",
       "      <td>0.133085</td>\n",
       "      <td>0.764493</td>\n",
       "      <td>0.037249</td>\n",
       "      <td>0.037595</td>\n",
       "      <td>0.995295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816137</td>\n",
       "      <td>0.680630</td>\n",
       "      <td>0.583450</td>\n",
       "      <td>0.553614</td>\n",
       "      <td>0.997551</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.165581</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.037240</td>\n",
       "      <td>0.000701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44983</th>\n",
       "      <td>1</td>\n",
       "      <td>0.196676</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.133421</td>\n",
       "      <td>0.324897</td>\n",
       "      <td>0.824048</td>\n",
       "      <td>0.336150</td>\n",
       "      <td>0.116276</td>\n",
       "      <td>0.991251</td>\n",
       "      <td>0.989752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.779868</td>\n",
       "      <td>0.015396</td>\n",
       "      <td>0.881038</td>\n",
       "      <td>0.086297</td>\n",
       "      <td>0.992464</td>\n",
       "      <td>0.926252</td>\n",
       "      <td>0.026698</td>\n",
       "      <td>0.010576</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>0.015649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44984</th>\n",
       "      <td>8</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.949594</td>\n",
       "      <td>0.005139</td>\n",
       "      <td>0.013833</td>\n",
       "      <td>0.303603</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>0.266143</td>\n",
       "      <td>0.998475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959877</td>\n",
       "      <td>0.308988</td>\n",
       "      <td>0.186621</td>\n",
       "      <td>0.921674</td>\n",
       "      <td>0.999109</td>\n",
       "      <td>0.998314</td>\n",
       "      <td>0.042080</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.748214</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44985</th>\n",
       "      <td>5</td>\n",
       "      <td>0.726037</td>\n",
       "      <td>0.199513</td>\n",
       "      <td>0.938182</td>\n",
       "      <td>0.048223</td>\n",
       "      <td>0.605256</td>\n",
       "      <td>0.494560</td>\n",
       "      <td>0.215233</td>\n",
       "      <td>0.156790</td>\n",
       "      <td>0.994253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912529</td>\n",
       "      <td>0.967654</td>\n",
       "      <td>0.577246</td>\n",
       "      <td>0.816030</td>\n",
       "      <td>0.994286</td>\n",
       "      <td>0.986834</td>\n",
       "      <td>0.169029</td>\n",
       "      <td>0.005417</td>\n",
       "      <td>0.060787</td>\n",
       "      <td>0.001777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44986</th>\n",
       "      <td>3</td>\n",
       "      <td>0.143014</td>\n",
       "      <td>0.724188</td>\n",
       "      <td>0.182727</td>\n",
       "      <td>0.180762</td>\n",
       "      <td>0.773200</td>\n",
       "      <td>0.424693</td>\n",
       "      <td>0.040673</td>\n",
       "      <td>0.528232</td>\n",
       "      <td>0.975852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899194</td>\n",
       "      <td>0.659557</td>\n",
       "      <td>0.206980</td>\n",
       "      <td>0.265959</td>\n",
       "      <td>0.985569</td>\n",
       "      <td>0.995650</td>\n",
       "      <td>0.159295</td>\n",
       "      <td>0.012589</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>0.011486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44987</th>\n",
       "      <td>4</td>\n",
       "      <td>0.378785</td>\n",
       "      <td>0.733943</td>\n",
       "      <td>0.940969</td>\n",
       "      <td>0.105713</td>\n",
       "      <td>0.088460</td>\n",
       "      <td>0.920614</td>\n",
       "      <td>0.008888</td>\n",
       "      <td>0.133422</td>\n",
       "      <td>0.996252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888400</td>\n",
       "      <td>0.965319</td>\n",
       "      <td>0.890197</td>\n",
       "      <td>0.412240</td>\n",
       "      <td>0.995780</td>\n",
       "      <td>0.994786</td>\n",
       "      <td>0.030225</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.919813</td>\n",
       "      <td>0.002107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44988</th>\n",
       "      <td>6</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>0.821653</td>\n",
       "      <td>0.943596</td>\n",
       "      <td>0.031252</td>\n",
       "      <td>0.025319</td>\n",
       "      <td>0.122395</td>\n",
       "      <td>0.031086</td>\n",
       "      <td>0.536850</td>\n",
       "      <td>0.990500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971542</td>\n",
       "      <td>0.721731</td>\n",
       "      <td>0.983547</td>\n",
       "      <td>0.476921</td>\n",
       "      <td>0.996807</td>\n",
       "      <td>0.996380</td>\n",
       "      <td>0.051705</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.004780</td>\n",
       "      <td>0.001711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44989</th>\n",
       "      <td>0</td>\n",
       "      <td>0.007116</td>\n",
       "      <td>0.989128</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.009291</td>\n",
       "      <td>0.023175</td>\n",
       "      <td>0.040724</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.998833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997910</td>\n",
       "      <td>0.999886</td>\n",
       "      <td>0.018739</td>\n",
       "      <td>0.995122</td>\n",
       "      <td>0.999888</td>\n",
       "      <td>0.997478</td>\n",
       "      <td>0.009962</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.996578</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44990</th>\n",
       "      <td>5</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>0.980853</td>\n",
       "      <td>0.657373</td>\n",
       "      <td>0.025742</td>\n",
       "      <td>0.049460</td>\n",
       "      <td>0.955235</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.048215</td>\n",
       "      <td>0.998068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.833433</td>\n",
       "      <td>0.059130</td>\n",
       "      <td>0.599000</td>\n",
       "      <td>0.998270</td>\n",
       "      <td>0.999942</td>\n",
       "      <td>0.047589</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.024373</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44991</th>\n",
       "      <td>7</td>\n",
       "      <td>0.442999</td>\n",
       "      <td>0.134806</td>\n",
       "      <td>0.596259</td>\n",
       "      <td>0.443389</td>\n",
       "      <td>0.373468</td>\n",
       "      <td>0.478404</td>\n",
       "      <td>0.836330</td>\n",
       "      <td>0.392747</td>\n",
       "      <td>0.978616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929884</td>\n",
       "      <td>0.754376</td>\n",
       "      <td>0.687109</td>\n",
       "      <td>0.458635</td>\n",
       "      <td>0.984706</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.092001</td>\n",
       "      <td>0.011957</td>\n",
       "      <td>0.078135</td>\n",
       "      <td>0.013297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44992</th>\n",
       "      <td>2</td>\n",
       "      <td>0.150706</td>\n",
       "      <td>0.328708</td>\n",
       "      <td>0.792058</td>\n",
       "      <td>0.628172</td>\n",
       "      <td>0.188206</td>\n",
       "      <td>0.129140</td>\n",
       "      <td>0.069941</td>\n",
       "      <td>0.789398</td>\n",
       "      <td>0.992327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922960</td>\n",
       "      <td>0.071333</td>\n",
       "      <td>0.939204</td>\n",
       "      <td>0.593096</td>\n",
       "      <td>0.993320</td>\n",
       "      <td>0.940947</td>\n",
       "      <td>0.079340</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.004476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44993</th>\n",
       "      <td>6</td>\n",
       "      <td>0.038404</td>\n",
       "      <td>0.943854</td>\n",
       "      <td>0.998904</td>\n",
       "      <td>0.173839</td>\n",
       "      <td>0.003979</td>\n",
       "      <td>0.154025</td>\n",
       "      <td>0.061688</td>\n",
       "      <td>0.043888</td>\n",
       "      <td>0.994183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980905</td>\n",
       "      <td>0.932087</td>\n",
       "      <td>0.937571</td>\n",
       "      <td>0.970809</td>\n",
       "      <td>0.994032</td>\n",
       "      <td>0.996922</td>\n",
       "      <td>0.047039</td>\n",
       "      <td>0.003806</td>\n",
       "      <td>0.416417</td>\n",
       "      <td>0.001418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44994</th>\n",
       "      <td>6</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>0.768312</td>\n",
       "      <td>0.974405</td>\n",
       "      <td>0.081786</td>\n",
       "      <td>0.009348</td>\n",
       "      <td>0.255429</td>\n",
       "      <td>0.048592</td>\n",
       "      <td>0.450067</td>\n",
       "      <td>0.989784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966438</td>\n",
       "      <td>0.800064</td>\n",
       "      <td>0.962523</td>\n",
       "      <td>0.740063</td>\n",
       "      <td>0.993433</td>\n",
       "      <td>0.997833</td>\n",
       "      <td>0.185830</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.011070</td>\n",
       "      <td>0.003189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44995</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.984795</td>\n",
       "      <td>0.997657</td>\n",
       "      <td>0.026665</td>\n",
       "      <td>0.055199</td>\n",
       "      <td>0.041547</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>0.012679</td>\n",
       "      <td>0.999832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997894</td>\n",
       "      <td>0.999832</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>0.959281</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999904</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.988162</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44996</th>\n",
       "      <td>1</td>\n",
       "      <td>0.162923</td>\n",
       "      <td>0.009284</td>\n",
       "      <td>0.114524</td>\n",
       "      <td>0.422997</td>\n",
       "      <td>0.824745</td>\n",
       "      <td>0.329085</td>\n",
       "      <td>0.139248</td>\n",
       "      <td>0.951360</td>\n",
       "      <td>0.993574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735619</td>\n",
       "      <td>0.029805</td>\n",
       "      <td>0.797868</td>\n",
       "      <td>0.108405</td>\n",
       "      <td>0.996227</td>\n",
       "      <td>0.950535</td>\n",
       "      <td>0.047932</td>\n",
       "      <td>0.005489</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>0.006812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44997</th>\n",
       "      <td>1</td>\n",
       "      <td>0.184629</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.134734</td>\n",
       "      <td>0.243669</td>\n",
       "      <td>0.837387</td>\n",
       "      <td>0.307312</td>\n",
       "      <td>0.085948</td>\n",
       "      <td>0.993235</td>\n",
       "      <td>0.999044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.948884</td>\n",
       "      <td>0.008878</td>\n",
       "      <td>0.953961</td>\n",
       "      <td>0.083689</td>\n",
       "      <td>0.999399</td>\n",
       "      <td>0.946191</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.010976</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44998</th>\n",
       "      <td>4</td>\n",
       "      <td>0.160808</td>\n",
       "      <td>0.405612</td>\n",
       "      <td>0.303304</td>\n",
       "      <td>0.298612</td>\n",
       "      <td>0.174680</td>\n",
       "      <td>0.884113</td>\n",
       "      <td>0.012060</td>\n",
       "      <td>0.276819</td>\n",
       "      <td>0.988087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849804</td>\n",
       "      <td>0.939973</td>\n",
       "      <td>0.976605</td>\n",
       "      <td>0.115506</td>\n",
       "      <td>0.981834</td>\n",
       "      <td>0.991308</td>\n",
       "      <td>0.204283</td>\n",
       "      <td>0.006938</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.007935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44999</th>\n",
       "      <td>7</td>\n",
       "      <td>0.315735</td>\n",
       "      <td>0.081858</td>\n",
       "      <td>0.920482</td>\n",
       "      <td>0.474865</td>\n",
       "      <td>0.155689</td>\n",
       "      <td>0.235793</td>\n",
       "      <td>0.938366</td>\n",
       "      <td>0.062199</td>\n",
       "      <td>0.996739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969605</td>\n",
       "      <td>0.995693</td>\n",
       "      <td>0.645942</td>\n",
       "      <td>0.638721</td>\n",
       "      <td>0.997451</td>\n",
       "      <td>0.062756</td>\n",
       "      <td>0.038346</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.425561</td>\n",
       "      <td>0.001089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45000 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6    \\\n",
       "0       5  0.494230  0.467823  0.595076  0.388914  0.467764  0.517085   \n",
       "1       0  0.624591  0.424304  0.650029  0.391525  0.421695  0.421476   \n",
       "2       4  0.617798  0.534772  0.571680  0.465620  0.483293  0.453296   \n",
       "3       1  0.423545  0.390823  0.529562  0.364918  0.543253  0.414250   \n",
       "4       9  0.434693  0.400409  0.684090  0.443029  0.540683  0.388294   \n",
       "5       2  0.658381  0.430957  0.573086  0.499354  0.555867  0.326530   \n",
       "6       1  0.513190  0.363113  0.622211  0.478259  0.515056  0.447807   \n",
       "7       3  0.660594  0.491482  0.610683  0.373437  0.583428  0.517546   \n",
       "8       1  0.543546  0.389894  0.537208  0.511608  0.501795  0.424094   \n",
       "9       4  0.674716  0.399831  0.569309  0.497585  0.489677  0.417305   \n",
       "10      3  0.888166  0.744576  0.287174  0.258629  0.681662  0.794190   \n",
       "11      5  0.730373  0.571258  0.365662  0.351842  0.640210  0.525977   \n",
       "12      3  0.911273  0.720311  0.221945  0.268593  0.614894  0.608913   \n",
       "13      6  0.932558  0.677074  0.352891  0.336862  0.529162  0.698485   \n",
       "14      1  0.691502  0.585320  0.340819  0.462442  0.625349  0.609365   \n",
       "15      7  0.836191  0.646481  0.355989  0.321231  0.583676  0.692851   \n",
       "16      2  0.808383  0.698526  0.303217  0.343251  0.525590  0.700091   \n",
       "17      8  0.873532  0.684099  0.256418  0.281667  0.766444  0.745901   \n",
       "18      6  0.835911  0.577467  0.415803  0.393485  0.425256  0.601619   \n",
       "19      9  0.811964  0.653062  0.304647  0.322584  0.605160  0.639365   \n",
       "20      4  0.871545  0.546188  0.409877  0.359726  0.370240  0.647059   \n",
       "21      0  0.916786  0.729819  0.478744  0.301027  0.394804  0.632818   \n",
       "22      9  0.724706  0.611188  0.469134  0.438071  0.470617  0.579553   \n",
       "23      1  0.622919  0.562617  0.412245  0.327718  0.509545  0.582358   \n",
       "24      1  0.743678  0.553682  0.482105  0.388093  0.484413  0.550554   \n",
       "25      2  0.943022  0.752091  0.363346  0.290115  0.607876  0.652248   \n",
       "26      4  0.711161  0.599640  0.522103  0.403097  0.429447  0.573243   \n",
       "27      3  0.951061  0.817977  0.298123  0.252601  0.598677  0.713036   \n",
       "28      2  0.892253  0.583868  0.452556  0.395637  0.443463  0.648370   \n",
       "29      7  0.708107  0.521213  0.455892  0.400241  0.432592  0.543961   \n",
       "...    ..       ...       ...       ...       ...       ...       ...   \n",
       "44970   3  0.097989  0.790560  0.156103  0.121637  0.256388  0.835986   \n",
       "44971   0  0.002386  0.994704  0.999267  0.013207  0.117074  0.091070   \n",
       "44972   1  0.073935  0.896097  0.156712  0.445479  0.220772  0.765188   \n",
       "44973   4  0.382876  0.002394  0.479877  0.305806  0.365757  0.688519   \n",
       "44974   3  0.063960  0.989253  0.123271  0.397002  0.558801  0.854081   \n",
       "44975   8  0.192237  0.076316  0.954055  0.027640  0.131128  0.522267   \n",
       "44976   3  0.033162  0.479455  0.409225  0.237286  0.783323  0.548710   \n",
       "44977   4  0.514909  0.002693  0.363598  0.186163  0.372482  0.633269   \n",
       "44978   2  0.011036  0.121130  0.997793  0.047162  0.084515  0.045675   \n",
       "44979   0  0.001641  0.997716  0.999795  0.016177  0.048298  0.156040   \n",
       "44980   7  0.000588  0.989819  0.943823  0.412973  0.029005  0.267024   \n",
       "44981   5  0.698437  0.062164  0.931174  0.455207  0.258440  0.379148   \n",
       "44982   5  0.009103  0.978944  0.718751  0.050628  0.133085  0.764493   \n",
       "44983   1  0.196676  0.000756  0.133421  0.324897  0.824048  0.336150   \n",
       "44984   8  0.001885  0.105300  0.949594  0.005139  0.013833  0.303603   \n",
       "44985   5  0.726037  0.199513  0.938182  0.048223  0.605256  0.494560   \n",
       "44986   3  0.143014  0.724188  0.182727  0.180762  0.773200  0.424693   \n",
       "44987   4  0.378785  0.733943  0.940969  0.105713  0.088460  0.920614   \n",
       "44988   6  0.003170  0.821653  0.943596  0.031252  0.025319  0.122395   \n",
       "44989   0  0.007116  0.989128  0.999983  0.032000  0.009291  0.023175   \n",
       "44990   5  0.004855  0.980853  0.657373  0.025742  0.049460  0.955235   \n",
       "44991   7  0.442999  0.134806  0.596259  0.443389  0.373468  0.478404   \n",
       "44992   2  0.150706  0.328708  0.792058  0.628172  0.188206  0.129140   \n",
       "44993   6  0.038404  0.943854  0.998904  0.173839  0.003979  0.154025   \n",
       "44994   6  0.003507  0.768312  0.974405  0.081786  0.009348  0.255429   \n",
       "44995   0  0.000396  0.984795  0.997657  0.026665  0.055199  0.041547   \n",
       "44996   1  0.162923  0.009284  0.114524  0.422997  0.824745  0.329085   \n",
       "44997   1  0.184629  0.000055  0.134734  0.243669  0.837387  0.307312   \n",
       "44998   4  0.160808  0.405612  0.303304  0.298612  0.174680  0.884113   \n",
       "44999   7  0.315735  0.081858  0.920482  0.474865  0.155689  0.235793   \n",
       "\n",
       "            7         8         9      ...          191       192       193  \\\n",
       "0      0.533080  0.564102  0.429205    ...     0.456384  0.617726  0.401893   \n",
       "1      0.312537  0.534023  0.486113    ...     0.439536  0.652906  0.423631   \n",
       "2      0.568256  0.489467  0.533587    ...     0.490198  0.593565  0.375059   \n",
       "3      0.444813  0.500588  0.424529    ...     0.447035  0.466065  0.400395   \n",
       "4      0.455705  0.571975  0.489039    ...     0.443192  0.491207  0.287157   \n",
       "5      0.501080  0.600948  0.427330    ...     0.443569  0.611450  0.280616   \n",
       "6      0.418159  0.544871  0.543357    ...     0.419029  0.513147  0.550448   \n",
       "7      0.478933  0.611986  0.499411    ...     0.405028  0.577689  0.361144   \n",
       "8      0.501251  0.471739  0.548973    ...     0.423703  0.448869  0.498349   \n",
       "9      0.558413  0.667773  0.458999    ...     0.453993  0.631944  0.398568   \n",
       "10     0.044762  0.561346  0.959060    ...     0.099449  0.890696  0.305130   \n",
       "11     0.207095  0.537340  0.775089    ...     0.313269  0.742767  0.445088   \n",
       "12     0.067280  0.478254  0.942158    ...     0.140734  0.838847  0.214658   \n",
       "13     0.049965  0.570361  0.958085    ...     0.162953  0.872084  0.378509   \n",
       "14     0.198291  0.449403  0.856750    ...     0.280659  0.629646  0.542666   \n",
       "15     0.062673  0.579816  0.924085    ...     0.166864  0.890528  0.409879   \n",
       "16     0.101771  0.689810  0.893156    ...     0.197315  0.866733  0.358386   \n",
       "17     0.056293  0.660158  0.934863    ...     0.119674  0.856460  0.205770   \n",
       "18     0.182119  0.464801  0.791385    ...     0.273740  0.748247  0.354975   \n",
       "19     0.088234  0.568141  0.890989    ...     0.191538  0.794257  0.268252   \n",
       "20     0.050762  0.365169  0.941842    ...     0.161915  0.806814  0.303166   \n",
       "21     0.026532  0.262696  0.940198    ...     0.170091  0.879354  0.405263   \n",
       "22     0.120608  0.497411  0.870381    ...     0.281535  0.688322  0.384792   \n",
       "23     0.130504  0.417352  0.848702    ...     0.248423  0.660609  0.439858   \n",
       "24     0.097940  0.450786  0.906241    ...     0.277929  0.779908  0.434099   \n",
       "25     0.030996  0.504306  0.970384    ...     0.106968  0.867201  0.336852   \n",
       "26     0.183740  0.484845  0.850281    ...     0.241864  0.717971  0.465129   \n",
       "27     0.013614  0.376497  0.990110    ...     0.086321  0.929248  0.313767   \n",
       "28     0.063307  0.392715  0.973024    ...     0.139701  0.902834  0.442264   \n",
       "29     0.163184  0.472505  0.884560    ...     0.315642  0.680063  0.430113   \n",
       "...         ...       ...       ...    ...          ...       ...       ...   \n",
       "44970  0.026902  0.598115  0.991415    ...     0.874232  0.756598  0.043511   \n",
       "44971  0.008191  0.016838  0.998514    ...     0.991500  0.993681  0.002947   \n",
       "44972  0.563065  0.125587  0.997690    ...     0.831369  0.112692  0.924809   \n",
       "44973  0.003878  0.962252  0.995841    ...     0.904691  0.077888  0.956331   \n",
       "44974  0.011267  0.112918  0.979306    ...     0.922974  0.780557  0.015007   \n",
       "44975  0.122411  0.826195  0.996663    ...     0.915537  0.120180  0.344185   \n",
       "44976  0.483859  0.375211  0.980239    ...     0.858347  0.845671  0.102894   \n",
       "44977  0.008469  0.936729  0.998570    ...     0.971055  0.339339  0.924783   \n",
       "44978  0.032084  0.711802  0.983639    ...     0.967933  0.370669  0.489646   \n",
       "44979  0.235444  0.006728  0.998358    ...     0.989920  0.999898  0.000638   \n",
       "44980  0.878164  0.001502  0.999516    ...     0.998470  0.999772  0.103359   \n",
       "44981  0.101657  0.572843  0.984802    ...     0.917539  0.663742  0.498389   \n",
       "44982  0.037249  0.037595  0.995295    ...     0.816137  0.680630  0.583450   \n",
       "44983  0.116276  0.991251  0.989752    ...     0.779868  0.015396  0.881038   \n",
       "44984  0.006668  0.266143  0.998475    ...     0.959877  0.308988  0.186621   \n",
       "44985  0.215233  0.156790  0.994253    ...     0.912529  0.967654  0.577246   \n",
       "44986  0.040673  0.528232  0.975852    ...     0.899194  0.659557  0.206980   \n",
       "44987  0.008888  0.133422  0.996252    ...     0.888400  0.965319  0.890197   \n",
       "44988  0.031086  0.536850  0.990500    ...     0.971542  0.721731  0.983547   \n",
       "44989  0.040724  0.014800  0.998833    ...     0.997910  0.999886  0.018739   \n",
       "44990  0.000876  0.048215  0.998068    ...     0.921569  0.833433  0.059130   \n",
       "44991  0.836330  0.392747  0.978616    ...     0.929884  0.754376  0.687109   \n",
       "44992  0.069941  0.789398  0.992327    ...     0.922960  0.071333  0.939204   \n",
       "44993  0.061688  0.043888  0.994183    ...     0.980905  0.932087  0.937571   \n",
       "44994  0.048592  0.450067  0.989784    ...     0.966438  0.800064  0.962523   \n",
       "44995  0.003092  0.012679  0.999832    ...     0.997894  0.999832  0.002116   \n",
       "44996  0.139248  0.951360  0.993574    ...     0.735619  0.029805  0.797868   \n",
       "44997  0.085948  0.993235  0.999044    ...     0.948884  0.008878  0.953961   \n",
       "44998  0.012060  0.276819  0.988087    ...     0.849804  0.939973  0.976605   \n",
       "44999  0.938366  0.062199  0.996739    ...     0.969605  0.995693  0.645942   \n",
       "\n",
       "            194       195       196       197       198       199       200  \n",
       "0      0.437372  0.510095  0.512113  0.501598  0.475575  0.445245  0.380133  \n",
       "1      0.478065  0.608949  0.648437  0.423185  0.504391  0.322251  0.527458  \n",
       "2      0.573670  0.497906  0.473200  0.483968  0.440581  0.465039  0.483879  \n",
       "3      0.578492  0.528461  0.351093  0.502654  0.497462  0.509921  0.440470  \n",
       "4      0.473730  0.447597  0.422574  0.480675  0.392080  0.567889  0.363806  \n",
       "5      0.537071  0.458861  0.391992  0.474362  0.437483  0.489106  0.412709  \n",
       "6      0.519971  0.434781  0.330372  0.565793  0.518561  0.402986  0.399268  \n",
       "7      0.554764  0.457009  0.529344  0.358498  0.503486  0.456975  0.319535  \n",
       "8      0.504824  0.426270  0.399242  0.509458  0.468201  0.491694  0.425091  \n",
       "9      0.525464  0.347463  0.552696  0.460307  0.558235  0.560515  0.306293  \n",
       "10     0.105383  0.952261  0.860016  0.355413  0.035215  0.341627  0.025696  \n",
       "11     0.355543  0.656129  0.634581  0.383048  0.282756  0.570108  0.266972  \n",
       "12     0.134282  0.921574  0.813057  0.356563  0.046148  0.382962  0.043845  \n",
       "13     0.111591  0.919121  0.905419  0.434574  0.036498  0.594042  0.043124  \n",
       "14     0.262826  0.764815  0.683579  0.466251  0.144478  0.515523  0.145600  \n",
       "15     0.157178  0.921679  0.802660  0.377529  0.045518  0.582008  0.059110  \n",
       "16     0.160402  0.907846  0.747773  0.452527  0.086284  0.390670  0.069377  \n",
       "17     0.134903  0.913404  0.854995  0.426901  0.044870  0.547123  0.036158  \n",
       "18     0.285135  0.739539  0.806190  0.467419  0.169052  0.503222  0.130981  \n",
       "19     0.185505  0.856023  0.740085  0.413501  0.075334  0.550353  0.082000  \n",
       "20     0.253495  0.921550  0.859913  0.397244  0.046401  0.454205  0.046526  \n",
       "21     0.191217  0.971411  0.931820  0.325644  0.046513  0.286869  0.035229  \n",
       "22     0.328197  0.855929  0.778049  0.436984  0.113921  0.514604  0.079988  \n",
       "23     0.372068  0.854393  0.650994  0.432960  0.114071  0.488626  0.117873  \n",
       "24     0.282789  0.913636  0.737157  0.484573  0.118982  0.371722  0.086782  \n",
       "25     0.166482  0.956981  0.920333  0.288736  0.017438  0.370443  0.011639  \n",
       "26     0.257659  0.783623  0.709401  0.440979  0.119517  0.501293  0.151322  \n",
       "27     0.115455  0.979463  0.949778  0.234263  0.012141  0.340643  0.008591  \n",
       "28     0.139691  0.951943  0.872757  0.213484  0.022181  0.432048  0.020084  \n",
       "29     0.361468  0.830389  0.664809  0.399507  0.134994  0.488589  0.112811  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "44970  0.100588  0.986283  0.995441  0.053994  0.004025  0.000613  0.002784  \n",
       "44971  0.985113  0.999766  0.999891  0.015148  0.000399  0.998367  0.000100  \n",
       "44972  0.376936  0.993158  0.982778  0.672422  0.001563  0.000081  0.001596  \n",
       "44973  0.277207  0.996049  0.998599  0.028825  0.005550  0.032314  0.006991  \n",
       "44974  0.328664  0.977143  0.998310  0.187958  0.008256  0.002821  0.009174  \n",
       "44975  0.723911  0.996972  0.985409  0.011703  0.002524  0.963186  0.001447  \n",
       "44976  0.210884  0.993728  0.826089  0.072882  0.009098  0.076956  0.005730  \n",
       "44977  0.250874  0.998267  0.999491  0.016483  0.002126  0.104279  0.002384  \n",
       "44978  0.977165  0.994197  0.988026  0.050413  0.007772  0.990430  0.003213  \n",
       "44979  0.990864  0.999652  0.990202  0.072935  0.000342  0.996863  0.000166  \n",
       "44980  0.921100  0.999417  0.957558  0.043646  0.000050  0.018407  0.000031  \n",
       "44981  0.732800  0.981478  0.983677  0.111224  0.018336  0.166667  0.012052  \n",
       "44982  0.553614  0.997551  0.998500  0.165581  0.000920  0.037240  0.000701  \n",
       "44983  0.086297  0.992464  0.926252  0.026698  0.010576  0.006410  0.015649  \n",
       "44984  0.921674  0.999109  0.998314  0.042080  0.000349  0.748214  0.000114  \n",
       "44985  0.816030  0.994286  0.986834  0.169029  0.005417  0.060787  0.001777  \n",
       "44986  0.265959  0.985569  0.995650  0.159295  0.012589  0.002435  0.011486  \n",
       "44987  0.412240  0.995780  0.994786  0.030225  0.002925  0.919813  0.002107  \n",
       "44988  0.476921  0.996807  0.996380  0.051705  0.000884  0.004780  0.001711  \n",
       "44989  0.995122  0.999888  0.997478  0.009962  0.000143  0.996578  0.000058  \n",
       "44990  0.599000  0.998270  0.999942  0.047589  0.000450  0.024373  0.000266  \n",
       "44991  0.458635  0.984706  0.113636  0.092001  0.011957  0.078135  0.013297  \n",
       "44992  0.593096  0.993320  0.940947  0.079340  0.004051  0.001551  0.004476  \n",
       "44993  0.970809  0.994032  0.996922  0.047039  0.003806  0.416417  0.001418  \n",
       "44994  0.740063  0.993433  0.997833  0.185830  0.001721  0.011070  0.003189  \n",
       "44995  0.959281  0.999987  0.999904  0.006693  0.000015  0.988162  0.000006  \n",
       "44996  0.108405  0.996227  0.950535  0.047932  0.005489  0.002075  0.006812  \n",
       "44997  0.083689  0.999399  0.946191  0.003382  0.000865  0.010976  0.001149  \n",
       "44998  0.115506  0.981834  0.991308  0.204283  0.006938  0.000616  0.007935  \n",
       "44999  0.638721  0.997451  0.062756  0.038346  0.001603  0.425561  0.001089  \n",
       "\n",
       "[45000 rows x 201 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataframe, split=0.8, fraction = 1.0):\n",
    "    \n",
    "    dataframe = dataframe.sample(frac=fraction, replace=True)\n",
    "    \n",
    "    \n",
    "    #Rearange to make first column the last\n",
    "    \n",
    "#     print(\"Before\")\n",
    "#     print(dataframe.head())\n",
    "    \n",
    "    cols = dataframe.columns.tolist()\n",
    "    cols = cols[-(len(cols)-1):] + cols[:-(len(cols)-1)]\n",
    "    \n",
    "    dataframe = dataframe[cols]\n",
    "    \n",
    "    \n",
    "    print(len(dataframe.columns.tolist()))\n",
    "    \n",
    "#     print(\"After\")\n",
    "#     print(dataframe.head())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Overide to Class_index to always be the Last Column\n",
    "    class_idx = len(dataframe.columns)-1\n",
    "    \n",
    "    \n",
    "    #dataframe = dataframe.sample(len(dataframe))\n",
    "    instances = dataframe.values\n",
    "\n",
    "\n",
    "    print (\"Class Index: \"+str(class_idx))\n",
    "    # dpockemivide data into label and feature sets.\n",
    "    X = instances[:,0:class_idx] # you may need to change these depending on which dataset you are use\n",
    "    Y = instances[:,class_idx] \n",
    "    \n",
    "#     print(\"X\",X,\":\",\"Y\",Y)\n",
    "    \n",
    "   \n",
    "    X_train = [] # features for the train set\n",
    "    Y_train = [] # class labels for the train set\n",
    "    X_test = [] # features for the test set\n",
    "    Y_test = [] # class labels for the test set\n",
    "    \n",
    "    # the zip iterator is a neat construct in Python\n",
    "    # it lets you iterate over 2 arrays / lists structures \n",
    "    # importantly it iterates upto the length of the smallest structure of the two \n",
    "    # in our case X and Y will be of same length\n",
    "    for  x, y in zip(X, Y): \n",
    "        if random.random() < split: # Return the next random floating point number in the range [0.0, 1.0) and compare\n",
    "            X_train.append(x)\n",
    "            Y_train.append(y)\n",
    "        else:\n",
    "            X_test.append(x)\n",
    "            Y_test.append(y)       \n",
    "    print(\"train set size: \", len(X_train))       \n",
    "    print(\"test set size: \", len(X_test))\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Within our class we now need code for each of the components of k-NN.\n",
    "#First, lets create a method that will measure the distance between two vectors.\n",
    "def euclidean(instance1, instance2):\n",
    "        '''\n",
    "        Calculates euclidean distance between two instances of data\n",
    "        instance1 will be a List of Float values\n",
    "        instance2 will be a List of Float values\n",
    "        length will be an Integer denoting the length of the Lists\n",
    "        '''\n",
    "        distance = 0\n",
    "        for val1, val2 in zip(instance1, instance2):            \n",
    "            distance += pow((val1 - val2), 2)\n",
    "        \n",
    "        distance = pow(distance, 1/2)\n",
    "             \n",
    "              \n",
    "        return 1 / (1+ distance)\n",
    "    \n",
    "\n",
    "def manhattan(instance1, instance2):\n",
    "        '''\n",
    "        Calculates manhattan distance between two instances of data\n",
    "        instance1 will be a List of Float values\n",
    "        instance2 will be a List of Float values\n",
    "        length will be an Integer denoting the length of the Lists\n",
    "        '''\n",
    "        distance = 0\n",
    "        for val1, val2 in zip(instance1, instance2):\n",
    "            distance += abs(val1 - val2)      \n",
    "              \n",
    "        return 1 / (1+ distance)\n",
    "    \n",
    "def dot_product(instance1, instance2):\n",
    "        '''\n",
    "        Calculates dot product between two instances \n",
    "        instance1 will be a List of Float values\n",
    "        instance2 will be a List of Float values\n",
    "        length will be an Integer denoting the length of the Lists\n",
    "        '''\n",
    "        return np.dot(instance1, instance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we can test to see how many of the test instances we got correct\n",
    "def accuracy(results):\n",
    "    correct = 0\n",
    "    for predict, target in results:\n",
    "\n",
    "        if predict == target:\n",
    "            correct += 1\n",
    "    return (correct/float(len(results))) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNN:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    X_train, Y_train : list\n",
    "    these consists of the training set feature values and associated class labels\n",
    "    k : int\n",
    "    specify the number of neighbours\n",
    "    sim : literal\n",
    "    specify the name of the similarity metric (e.g. manhattan, eucliedean)\n",
    "    weighted : Boolean\n",
    "    specify the voting strategy as weighted or not weighted by similarity values\n",
    "  \n",
    "    Attributes\n",
    "    -----------  \n",
    "    Results : list\n",
    "      Target and predicted class labels for the test data.    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, X_train, Y_train, k=3, sim=euclidean, weighted=False):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        \n",
    "        if k <= len(self.X_train):\n",
    "            self.k = k # set the k value for neighbourhood size\n",
    "        else:\n",
    "            self.k = len(self.X_train) # to ensure the get_neighbours dont crash\n",
    "    \n",
    "        self.similarity = sim # specify a sim metric that has been pre-defined e.g. manhattan or euclidean\n",
    "        \n",
    "        self.weighted = weighted # boolean to choose between weighted / unweighted majority voting\n",
    "        \n",
    "        #store results from testing \n",
    "        self.results= []\n",
    "        \n",
    "    #With k-NN, we are interested in finding the k number of points with the greatest similarity \n",
    "    # to the the query or test instance.\n",
    "    def get_neighbours(self, test_instance):\n",
    "        '''\n",
    "        Locate most similar neighbours \n",
    "        X_train will be a containing features (Float) values (i.e. your training data)\n",
    "        Y_train will be the corresponding class labels for each instance in X_train\n",
    "        test_instance will be a List of Float values (i.e. a query instance)\n",
    "        '''\n",
    "        similarities = [] # collection to store the similarities to be computed\n",
    "\n",
    "        for train_instance, y in zip(self.X_train, self.Y_train): #for each member of the training set\n",
    "            sim = self.similarity(test_instance, train_instance) #calculate the similarity to the test instance\n",
    "            \n",
    "            similarities.append((y, sim)) #add the actual label of the example and the computed similarity to a collection \n",
    "        #print(distances)\n",
    "        similarities.sort(key = operator.itemgetter(1), reverse = True) #sort the collection by decreasing similarity\n",
    "        neighbours = [] # holds the k most similar neighbours\n",
    "        for x in range(self.k): #extract the k top indices of the collection for return\n",
    "            neighbours.append(similarities[x])\n",
    "\n",
    "        return neighbours\n",
    "\n",
    "    # given the neighbours make a prediction\n",
    "    # the boolean parameter when set to False will use unweighted majority voting; otherwise weighted majority voting\n",
    "    # weighting can be helpful to break any ties in voting\n",
    "    def predict(self, neighbours):\n",
    "        '''\n",
    "        Summarise a prediction based upon weighted neighbours calculation\n",
    "        '''\n",
    "        class_votes = {}\n",
    "        for x in range(len(neighbours)):\n",
    "            response = neighbours[x][0]\n",
    "            if response in class_votes:\n",
    "                class_votes[response] += (1-self.weighted) + (self.weighted * neighbours[x][1]) #if not weighted simply add 1\n",
    "                #class_votes[response] += [1, neighbours[x][1]][weighted == True] \n",
    "              \n",
    "            else:\n",
    "                class_votes[response] = (1-self.weighted) + (self.weighted * neighbours[x][1])\n",
    "                #class_votes[response] = [1, neighbours[x][1]][weighted == True] \n",
    "                \n",
    "        #print(class_votes)\n",
    "        sorted_votes = sorted(class_votes, key = lambda k: (class_votes[k], k), reverse = True)\n",
    "        #print(sorted_votes)\n",
    "        return sorted_votes[0]\n",
    "    \n",
    "    #iterate through all the test data to calculate accuracy\n",
    "    def test(self, X_test, Y_test):\n",
    "        self.results = [] # store the predictions returned by kNN\n",
    "\n",
    "        for test_instance, target_label in zip(X_test, Y_test):\n",
    "            neighbours = self.get_neighbours(test_instance)\n",
    "#             print(neighbours)\n",
    "            predict_label = self.predict(neighbours)\n",
    "            self.results.append([predict_label, target_label])\n",
    "            #print('> predicted = ', result,', actual = ', test_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "Class Index: 200\n",
      "train set size:  534\n",
      "test set size:  141\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset and maintain the features (X) and class labels (Y) separately  \n",
    "# make sure you understand what the 4 and 0.8 default values are in the call\n",
    "# you may have to modify these depending on the dataset you work with.\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "X_train, Y_train, X_test, Y_test = load_dataset(hybrid_data, split=0.8, fraction=0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN Accuracy on test set is:  84.39716312056737\n"
     ]
    }
   ],
   "source": [
    "knn = kNN(X_train, Y_train, k=int(math.sqrt(len(Y_test))), weighted=True)\n",
    "knn.test(X_test, Y_test) # now get the predictions on the test set\n",
    "\n",
    "print(\"kNN Accuracy on test set is: \", accuracy(knn.results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
