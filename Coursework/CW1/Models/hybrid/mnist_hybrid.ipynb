{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "#for the sigmoid function we need expit() from scipy\n",
    "import scipy.special\n",
    "#library for plotting arrays\n",
    "import matplotlib.pyplot as plt\n",
    "# A particularly interesting backend, provided by IPython, is the inline backend. \n",
    "# This is available only for the Jupyter Notebook and the Jupyter QtConsole. \n",
    "# It can be invoked as follows: %matplotlib inline\n",
    "# With this backend, the output of plotting commands is displayed inline \n",
    "# within frontends like the Jupyter notebook, directly below the code cell that produced it. \n",
    "# The resulting plots are inside this notebook, not an external window.\n",
    "\n",
    "import pandas as pd # to manage data frames and reading csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set our Global Variables\n",
    "later you will need to modify these to present your solution to the Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of input, hidden and output nodes\n",
    "input_nodes = 784 #we have 28 * 28 matrix to describe each digit\n",
    "hidden_nodes = 200\n",
    "output_nodes = 10\n",
    "\n",
    "learning_rate = 0.3\n",
    "batch_size = 10\n",
    "\n",
    "# epochs is the number of training iterations \n",
    "epochs = 30\n",
    "\n",
    "# datasets to read\n",
    "# you can change these when trying out other datasets\n",
    "train_file = \"data/mnist_train.csv\"\n",
    "test_file = \"data/mnist_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the mnist training data CSV file into a list\n",
    "train_data_file = open(train_file, 'r')\n",
    "train_data_list = train_data_file.readlines() # read all lines into memory \n",
    "train_data_file.close() \n",
    "print(\"train set size: \", len(train_data_list))\n",
    "\n",
    "#testing the network\n",
    "#load the mnist test data CSV file into a list\n",
    "#test_data_file = open(\"mnist/mnist_test_10.csv\", 'r') # read the file with 10 instances first\n",
    "test_data_file = open(test_file, 'r') # read the file with 10 instances first\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()\n",
    "print(\"test set size: \", len(test_data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    \"\"\"Artificial Neural Network classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    lr : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    ep : int\n",
    "      Number of epochs for training the network towards achieving convergence\n",
    "    batch_size : int\n",
    "      Size of the training batch to be used when calculating the gradient descent. \n",
    "      batch_size = 0 standard gradient descent\n",
    "      batch_size > 0 stochastic gradient descent \n",
    "\n",
    "    inodes : int\n",
    "      Number of input nodes which is normally the number of features in an instance.\n",
    "    hnodes : int\n",
    "      Number of hidden nodes in the net.\n",
    "    onodes : int\n",
    "      Number of output nodes in the net.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    wih : 2d-array\n",
    "      Input2Hidden node weights after fitting \n",
    "    who : 2d-array\n",
    "      Hidden2Output node weights after fitting \n",
    "    E : list\n",
    "      Sum-of-squares error value in each epoch.\n",
    "      \n",
    "    Results : list\n",
    "      Target and predicted class labels for the test data.\n",
    "      \n",
    "    Functions\n",
    "    ---------\n",
    "    activation_function : float (between 1 and -1)\n",
    "        implments the sigmoid function which squashes the node input\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputnodes=784, hiddennodes=200, outputnodes=10, learningrate=0.3, batch_size=10, epochs=30):\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        #link weight matrices, wih (input to hidden) and who (hidden to output)\n",
    "        #a weight on link from node i to node j is w_ij\n",
    "        \n",
    "        \n",
    "        #Draw random samples from a normal (Gaussian) distribution centered around 0.\n",
    "        #numpy.random.normal(loc to centre gaussian=0.0, scale=1, size=dimensions of the array we want) \n",
    "        #scale is usually set to the standard deviation which is related to the number of incoming links i.e. \n",
    "        #1/sqrt(num of incoming inputs). we use pow to raise it to the power of -0.5.\n",
    "        #We have set 0 as the centre of the guassian dist.\n",
    "        # size is set to the dimensions of the number of hnodes, inodes and onodes\n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        #set the learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        #set the batch size\n",
    "        self.bs = batch_size\n",
    "        \n",
    "        #set the number of epochs\n",
    "        self.ep = epochs\n",
    "        \n",
    "        #store errors at each epoch\n",
    "        self.E= []\n",
    "        \n",
    "        #store results from testing the model\n",
    "        #keep track of the network performance on each test instance\n",
    "        self.results= []\n",
    "        \n",
    "        #define the activation function here\n",
    "        #specify the sigmoid squashing function. Here expit() provides the sigmoid function.\n",
    "        #lambda is a short cut function which is executed there and then with no def (i.e. like an anonymous function)\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "   \n",
    "    def batch_input(self, input_list):\n",
    "        \"\"\"Yield consecutive batches of the specified size from the input list.\"\"\"\n",
    "        for i in range(0, len(input_list), self.bs):\n",
    "            yield input_list[i:i + self.bs]\n",
    "    \n",
    "    #train the neural net\n",
    "    #note the first part is very similar to the query function because they both require the forward pass\n",
    "    def train(self, train_inputs):\n",
    "        \"\"\"Training the neural net. \n",
    "           This includes the forward pass ; error computation; \n",
    "           backprop of the error ; calculation of gradients and updating the weights.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            train_inputs : {array-like}, shape = [n_instances, n_features]\n",
    "            Training vectors, where n_instances is the number of training instances and\n",
    "            n_features is the number of features.\n",
    "            Note this contains all features including the class feature which is in first position\n",
    "        \n",
    "            Returns\n",
    "            -------\n",
    "            self : object\n",
    "        \"\"\"\n",
    "        self.hybrid_values = []\n",
    "        self.hybrid_class = []\n",
    "        for e in range(self.ep):\n",
    "            print(\"Training epoch#: \", e)\n",
    "            sum_error = 0.0   \n",
    "            for batch in self.batch_input(train_inputs):\n",
    "                #creating variables to store the gradients   \n",
    "                delta_who = 0\n",
    "                delta_wih = 0\n",
    "                \n",
    "                # iterate through the inputs sent in\n",
    "                for instance in batch:\n",
    "                    # split it by the commas\n",
    "                    all_values = instance.split(',')\n",
    "                    self.hybrid_class.append(all_values[0])\n",
    "                    # scale and shift the inputs to address the problem of diminishing weights due to multiplying by zero\n",
    "                    # divide the raw inputs which are in the range 0-255 by 255 will bring them into the range 0-1\n",
    "                    # multiply by 0.99 to bring them into the range 0.0 - 0.99.\n",
    "                    # add 0.01 to shift them up to the desired range 0.01 - 1. \n",
    "                    inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "                    #create the target output values for each instance so that we can use it with the neural net\n",
    "                    #note we need 10 nodes where each represents one of the digits\n",
    "                    targets = np.zeros(output_nodes) + 0.01 #all initialised to 0.01\n",
    "                    #all_value[0] has the target class label for this instance\n",
    "                    targets[int(all_values[0])] = 0.99\n",
    "        \n",
    "                    #convert  inputs list to 2d array\n",
    "                    inputs = np.array(inputs,  ndmin=2).T\n",
    "                    targets = np.array(targets, ndmin=2).T\n",
    "\n",
    "                    #calculate signals into hidden layer\n",
    "                    hidden_inputs = np.dot(self.wih, inputs)\n",
    "                    #calculate the signals emerging from the hidden layer\n",
    "                    hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "                    #calculate signals into final output layer\n",
    "                    final_inputs=np.dot(self.who, hidden_outputs)\n",
    "                    \n",
    "                    self.hybrid_values.append(hidden_outputs)\n",
    "                    #calculate the signals emerging from final output layer\n",
    "                    final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "                    #to calculate the error we need to compute the element wise diff between target and actual\n",
    "                    output_errors = targets - final_outputs\n",
    "                    #Next distribute the error to the hidden layer such that hidden layer error\n",
    "                    #is the output_errors, split by weights, recombined at hidden nodes\n",
    "                    hidden_errors = np.dot(self.who.T, output_errors)\n",
    "            \n",
    "                       \n",
    "                    ## for each instance accumilate the gradients from each instance\n",
    "                    ## delta_who are the gradients between hidden and output weights\n",
    "                    ## delta_wih are the gradients between input and hidden weights\n",
    "                    delta_who += np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "                    delta_wih += np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "                    \n",
    "                    sum_error += np.dot(output_errors.T, output_errors)#this is the sum of squared error accumilated over each batced instance\n",
    "                   \n",
    "                pass #instance\n",
    "            \n",
    "                # update the weights by multiplying the gradient with the learning rate\n",
    "                # note that the deltas are divided by batch size to obtain the average gradient according to the given batch\n",
    "                # obviously if batch size = 1 then we dont need to bother with an average\n",
    "                self.who += self.lr * (delta_who / self.bs)\n",
    "                self.wih += self.lr * (delta_wih / self.bs)\n",
    "            pass # batch\n",
    "            self.E.append(np.asfarray(sum_error).flatten())\n",
    "            print(\"errors (SSE): \", self.E[-1])\n",
    "        pass # epoch\n",
    "    \n",
    "    #query the neural net\n",
    "    def query(self, inputs_list):\n",
    "        #convert inputs_list to a 2d array\n",
    "        #print(numpy.matrix(inputs_list))\n",
    "        #inputs_list [[ 1.   0.5 -1.5]]\n",
    "        inputs = np.array(inputs_list, ndmin=2).T \n",
    "        #once converted it appears as follows\n",
    "        #[[ 1. ]\n",
    "        # [ 0.5]\n",
    "        # [-1.5]]\n",
    "        #print(numpy.matrix(inputs))\n",
    "        \n",
    "        #propogate input into hidden layer. This is the start of the forward pass\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        \n",
    "        \n",
    "        #squash the content in the hidden node using the sigmoid function (value between 1, -1)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "                \n",
    "        #propagate into output layer and the apply the squashing sigmoid function\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        \n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        return final_outputs\n",
    "    \n",
    "     \n",
    "    #iterate through all the test data to calculate model accuracy\n",
    "    def test(self, test_inputs):\n",
    "        self.results = []\n",
    "        \n",
    "        #go through each test instances\n",
    "        for instance in test_inputs:\n",
    "            all_values = instance.split(',') # extract the input feature values for the instance\n",
    "    \n",
    "            target_label = int(all_values[0]) # get the target class for the instance\n",
    "    \n",
    "            #scale and shift the inputs this is to make sure values dont lead to zero when multiplied with weights\n",
    "            inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "    \n",
    "            #query the network with test inputs\n",
    "            #note this returns 10 output values ; of which the index of the highest value\n",
    "            # is the networks predicted class label\n",
    "            outputs = self.query(inputs)\n",
    "    \n",
    "            #get the index of the highest output node as this corresponds to the predicted class\n",
    "            predict_label = np.argmax(outputs) #this is the class predicted by the ANN\n",
    "    \n",
    "            self.results.append([predict_label, target_label])\n",
    "            #compute network error\n",
    "            #if (predict_label == target_label):\n",
    "            #    self.results.append(1)\n",
    "            #else: \n",
    "            #    self.results.append(0)\n",
    "            pass\n",
    "        pass\n",
    "        self.results = np.asfarray(self.results) # flatten results to avoid nested arrays\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Artificial Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instance of neuralnet\n",
    "n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate, batch_size, epochs)\n",
    "\n",
    "# numpy.random.choice generates a random sample from a given 1-D array\n",
    "# we can use this to select a sample from our training data in case we want to work with a small sample\n",
    "# for instance we use a small sample here such as 1500\n",
    "# mini_training_data = np.random.choice(train_data_list, 1500, replace = False)\n",
    "mini_training_data = train_data_list[:1500]\n",
    "print(\"Percentage of training data used:\", (len(mini_training_data)/len(train_data_list)) * 100)\n",
    "n.train(mini_training_data)\n",
    "\n",
    "# n.train(train_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the model error with increasing epochs\n",
    "\n",
    "Error at the end of each epoch has been stored in self.E\n",
    "We can now use mathplotlib to plot the error at the end of each epoch. \n",
    "Our expectation is that as we continue to descend (hill-walking) we should move closer to the minima\n",
    "as such error should decrease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the error over epochs\n",
    "\n",
    "plt.figure(figsize=(20,5)) #width, height settings for figures\n",
    "plt.plot(range(0, n.ep), np.asfarray(n.E), marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of errors')\n",
    "\n",
    "\n",
    "\n",
    "# plt.savefig('images/02_07.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the ANN and compute the Accuracy\n",
    "We will keep track of the predicted and actual outputs in order to \n",
    "calculate the accuracy of the model on the unseen test data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.test(test_data_list)\n",
    "#print network performance as an accuracy metric\n",
    "correct = 0 # number of predictions that were correct\n",
    "\n",
    "#iteratre through each tested instance and accumilate number of correct predictions\n",
    "for result in n.results:\n",
    "    if (result[0] == result[1]):\n",
    "            correct += 1\n",
    "    pass\n",
    "pass\n",
    "\n",
    "# print the accuracy on test set\n",
    "print (\"Test set accuracy% = \", (100 * correct / len(n.results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "\n",
    "for row in range(0, len(n.hybrid_values)):\n",
    "    my_row = []\n",
    "    my_row.append(n.hybrid_class[row])\n",
    "    for col in range(0, len(n.hybrid_values[row])):\n",
    "        my_row.append(n.hybrid_values[row][col][0])\n",
    "    new_dataset.append(my_row)\n",
    "                   \n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_data = pd.DataFrame(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataframe, split=0.8, fraction = 1.0):\n",
    "    \n",
    "    dataframe = dataframe.sample(frac=fraction, replace=True)\n",
    "    \n",
    "    \n",
    "    #Rearange to make first column the last\n",
    "    \n",
    "#     print(\"Before\")\n",
    "#     print(dataframe.head())\n",
    "    \n",
    "    cols = dataframe.columns.tolist()\n",
    "    cols = cols[-(len(cols)-1):] + cols[:-(len(cols)-1)]\n",
    "    \n",
    "    dataframe = dataframe[cols]\n",
    "    \n",
    "    \n",
    "    print(len(dataframe.columns.tolist()))\n",
    "    \n",
    "#     print(\"After\")\n",
    "#     print(dataframe.head())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Overide to Class_index to always be the Last Column\n",
    "    class_idx = len(dataframe.columns)-1\n",
    "    \n",
    "    \n",
    "    #dataframe = dataframe.sample(len(dataframe))\n",
    "    instances = dataframe.values\n",
    "\n",
    "\n",
    "    print (\"Class Index: \"+str(class_idx))\n",
    "    # dpockemivide data into label and feature sets.\n",
    "    X = instances[:,0:class_idx] # you may need to change these depending on which dataset you are use\n",
    "    Y = instances[:,class_idx] \n",
    "    \n",
    "#     print(\"X\",X,\":\",\"Y\",Y)\n",
    "    \n",
    "   \n",
    "    X_train = [] # features for the train set\n",
    "    Y_train = [] # class labels for the train set\n",
    "    X_test = [] # features for the test set\n",
    "    Y_test = [] # class labels for the test set\n",
    "    \n",
    "    # the zip iterator is a neat construct in Python\n",
    "    # it lets you iterate over 2 arrays / lists structures \n",
    "    # importantly it iterates upto the length of the smallest structure of the two \n",
    "    # in our case X and Y will be of same length\n",
    "    for  x, y in zip(X, Y): \n",
    "        if random.random() < split: # Return the next random floating point number in the range [0.0, 1.0) and compare\n",
    "            X_train.append(x)\n",
    "            Y_train.append(y)\n",
    "        else:\n",
    "            X_test.append(x)\n",
    "            Y_test.append(y)       \n",
    "    print(\"train set size: \", len(X_train))       \n",
    "    print(\"test set size: \", len(X_test))\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Within our class we now need code for each of the components of k-NN.\n",
    "#First, lets create a method that will measure the distance between two vectors.\n",
    "def euclidean(instance1, instance2):\n",
    "        '''\n",
    "        Calculates euclidean distance between two instances of data\n",
    "        instance1 will be a List of Float values\n",
    "        instance2 will be a List of Float values\n",
    "        length will be an Integer denoting the length of the Lists\n",
    "        '''\n",
    "        distance = 0\n",
    "        for val1, val2 in zip(instance1, instance2):            \n",
    "            distance += pow((val1 - val2), 2)\n",
    "        \n",
    "        distance = pow(distance, 1/2)\n",
    "             \n",
    "              \n",
    "        return 1 / (1+ distance)\n",
    "    \n",
    "\n",
    "def manhattan(instance1, instance2):\n",
    "        '''\n",
    "        Calculates manhattan distance between two instances of data\n",
    "        instance1 will be a List of Float values\n",
    "        instance2 will be a List of Float values\n",
    "        length will be an Integer denoting the length of the Lists\n",
    "        '''\n",
    "        distance = 0\n",
    "        for val1, val2 in zip(instance1, instance2):\n",
    "            distance += abs(val1 - val2)      \n",
    "              \n",
    "        return 1 / (1+ distance)\n",
    "    \n",
    "def dot_product(instance1, instance2):\n",
    "        '''\n",
    "        Calculates dot product between two instances \n",
    "        instance1 will be a List of Float values\n",
    "        instance2 will be a List of Float values\n",
    "        length will be an Integer denoting the length of the Lists\n",
    "        '''\n",
    "        return np.dot(instance1, instance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we can test to see how many of the test instances we got correct\n",
    "def accuracy(results):\n",
    "    correct = 0\n",
    "    for predict, target in results:\n",
    "\n",
    "        if predict == target:\n",
    "            correct += 1\n",
    "    return (correct/float(len(results))) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNN:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    X_train, Y_train : list\n",
    "    these consists of the training set feature values and associated class labels\n",
    "    k : int\n",
    "    specify the number of neighbours\n",
    "    sim : literal\n",
    "    specify the name of the similarity metric (e.g. manhattan, eucliedean)\n",
    "    weighted : Boolean\n",
    "    specify the voting strategy as weighted or not weighted by similarity values\n",
    "  \n",
    "    Attributes\n",
    "    -----------  \n",
    "    Results : list\n",
    "      Target and predicted class labels for the test data.    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, X_train, Y_train, k=3, sim=euclidean, weighted=False):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        \n",
    "        if k <= len(self.X_train):\n",
    "            self.k = k # set the k value for neighbourhood size\n",
    "        else:\n",
    "            self.k = len(self.X_train) # to ensure the get_neighbours dont crash\n",
    "    \n",
    "        self.similarity = sim # specify a sim metric that has been pre-defined e.g. manhattan or euclidean\n",
    "        \n",
    "        self.weighted = weighted # boolean to choose between weighted / unweighted majority voting\n",
    "        \n",
    "        #store results from testing \n",
    "        self.results= []\n",
    "        \n",
    "    #With k-NN, we are interested in finding the k number of points with the greatest similarity \n",
    "    # to the the query or test instance.\n",
    "    def get_neighbours(self, test_instance):\n",
    "        '''\n",
    "        Locate most similar neighbours \n",
    "        X_train will be a containing features (Float) values (i.e. your training data)\n",
    "        Y_train will be the corresponding class labels for each instance in X_train\n",
    "        test_instance will be a List of Float values (i.e. a query instance)\n",
    "        '''\n",
    "        similarities = [] # collection to store the similarities to be computed\n",
    "\n",
    "        for train_instance, y in zip(self.X_train, self.Y_train): #for each member of the training set\n",
    "            sim = self.similarity(test_instance, train_instance) #calculate the similarity to the test instance\n",
    "            \n",
    "            similarities.append((y, sim)) #add the actual label of the example and the computed similarity to a collection \n",
    "        #print(distances)\n",
    "        similarities.sort(key = operator.itemgetter(1), reverse = True) #sort the collection by decreasing similarity\n",
    "        neighbours = [] # holds the k most similar neighbours\n",
    "        for x in range(self.k): #extract the k top indices of the collection for return\n",
    "            neighbours.append(similarities[x])\n",
    "\n",
    "        return neighbours\n",
    "\n",
    "    # given the neighbours make a prediction\n",
    "    # the boolean parameter when set to False will use unweighted majority voting; otherwise weighted majority voting\n",
    "    # weighting can be helpful to break any ties in voting\n",
    "    def predict(self, neighbours):\n",
    "        '''\n",
    "        Summarise a prediction based upon weighted neighbours calculation\n",
    "        '''\n",
    "        class_votes = {}\n",
    "        for x in range(len(neighbours)):\n",
    "            response = neighbours[x][0]\n",
    "            if response in class_votes:\n",
    "                class_votes[response] += (1-self.weighted) + (self.weighted * neighbours[x][1]) #if not weighted simply add 1\n",
    "                #class_votes[response] += [1, neighbours[x][1]][weighted == True] \n",
    "              \n",
    "            else:\n",
    "                class_votes[response] = (1-self.weighted) + (self.weighted * neighbours[x][1])\n",
    "                #class_votes[response] = [1, neighbours[x][1]][weighted == True] \n",
    "                \n",
    "        #print(class_votes)\n",
    "        sorted_votes = sorted(class_votes, key = lambda k: (class_votes[k], k), reverse = True)\n",
    "        #print(sorted_votes)\n",
    "        return sorted_votes[0]\n",
    "    \n",
    "    #iterate through all the test data to calculate accuracy\n",
    "    def test(self, X_test, Y_test):\n",
    "        self.results = [] # store the predictions returned by kNN\n",
    "\n",
    "        for test_instance, target_label in zip(X_test, Y_test):\n",
    "            neighbours = self.get_neighbours(test_instance)\n",
    "#             print(neighbours)\n",
    "            predict_label = self.predict(neighbours)\n",
    "            self.results.append([predict_label, target_label])\n",
    "            #print('> predicted = ', result,', actual = ', test_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset and maintain the features (X) and class labels (Y) separately  \n",
    "# make sure you understand what the 4 and 0.8 default values are in the call\n",
    "# you may have to modify these depending on the dataset you work with.\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "X_train, Y_train, X_test, Y_test = load_dataset(hybrid_data, split=0.8, fraction=0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = kNN(X_train, Y_train, k=int(math.sqrt(len(Y_test))), weighted=True)\n",
    "knn.test(X_test, Y_test) # now get the predictions on the test set\n",
    "\n",
    "print(\"kNN Accuracy on test set is: \", accuracy(knn.results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
