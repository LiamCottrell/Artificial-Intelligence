{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification - IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#for nlp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#text vectorisation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc\n",
    "\n",
    "#import method releated to evaluation\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, StratifiedShuffleSplit, ShuffleSplit, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "#classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#for graphs\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vairiables for script\n",
    "\n",
    "cross_validation_iterations = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'SMSSpamData.csv'\n",
    "SPAM_dataframe = pd.read_csv(filename, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>sms_msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                            sms_msg\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPAM_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Class Index to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ham': 0, 'spam': 1}\n"
     ]
    }
   ],
   "source": [
    "class_mapping = {label:idx for idx,label in enumerate(np.unique(SPAM_dataframe['class']))}\n",
    "\n",
    "print(class_mapping)\n",
    "class_labels = [x for x in class_mapping] # store the class labels for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the mapping dictionary to transform the class labels into integers\n",
    "\n",
    "SPAM_dataframe[\"class\"] = SPAM_dataframe[\"class\"].map(class_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>sms_msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                            sms_msg\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPAM_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPAM_dataframe.loc[42, 'sms_msg']#[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import regular expressions to clean up the text\n",
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) # remove all html markup\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) # findall the emoticons\n",
    "    \n",
    "    # remove the non-word chars '[\\W]+'\n",
    "    # append the emoticons to end \n",
    "    #convert all to lowercase\n",
    "    # remove nose char for consistency\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', '')) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'07732584351 rodger burns msg we tried to call you re your reply to our sms for a free nokia mobile free camcorder please call now 08000930705 for delivery tomorrow'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(SPAM_dataframe.loc[42, 'sms_msg'])#[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the preprocessor to the entire dataframe (i.e. column review)\n",
    "SPAM_dataframe['sms_msg'] = SPAM_dataframe['sms_msg'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenise, Stemmer & Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = str(text)\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_stemmer(text):\n",
    "    return [stemmer.stem(word) for word in tokenizer(text)]#text.split()]\n",
    "\n",
    "\n",
    "def stop_removal(text):\n",
    "       return [w for w in text if not w in stop]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'07732584351 rodger burns msg we tried to call you re your reply to our sms for a free nokia mobile free camcorder please call now 08000930705 for delivery tomorrow'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPAM_dataframe.loc[42, 'sms_msg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPAM_dataframe_subset = SPAM_dataframe.sample(n=1000)\n",
    "SPAM_dataframe_subset = SPAM_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_train, X_test, y_val_train, y_test = train_test_split(\n",
    "     SPAM_dataframe_subset.iloc[:,1], SPAM_dataframe_subset.iloc[:,0], test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "     X_val_train, y_val_train, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries for Training of best models: 3567\n",
      "Number of entries for Validating best models: 892\n",
      "Number of entries for Testing best models: 1115\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of entries for Training of best models: \" + str(len(X_train)))\n",
    "print(\"Number of entries for Validating best models: \" + str(len(X_val)))\n",
    "print(\"Number of entries for Testing best models: \" + str(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid0 = [{'vect__ngram_range': [(1, 1), (1, 2), (1, 3)], #can also extract 2-grams of words in addition to the 1-grams (individual words)\n",
    "               'vect__stop_words': [stop, None], # use the stop dictionary of stopwords or not\n",
    "               'vect__tokenizer': [tokenizer_stemmer]}, # use a tokeniser and the stemmer \n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: LogisticRegression\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6862f8b68d7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                                n_jobs=-1)\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mgs_mnb_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# Regenerate parameter iterable for each fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mcandidate_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_param_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m         \u001b[0mn_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# in this case we want to sample without replacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         all_lists = np.all([not hasattr(v, \"rvs\")\n\u001b[0;32m--> 238\u001b[0;31m                             for v in self.param_distributions.values()])\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mrnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "\n",
    "my_models = []\n",
    "\n",
    "\n",
    "for func in [LogisticRegression(),\n",
    "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
    "                   SVC(kernel='rbf', gamma=0.7, C=1.0)]:\n",
    "    results = {}\n",
    "    \n",
    "    stop_results = []\n",
    "    none_results = []\n",
    "    \n",
    "    print(\"Testing: \" + func.__class__.__name__)\n",
    "    \n",
    "    mnb_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf',  func)])\n",
    "                   \n",
    "    gs_mnb_tfidf = RandomizedSearchCV(mnb_tfidf, param_grid0,\n",
    "                               scoring='accuracy',\n",
    "                               cv=cross_validation_iterations,\n",
    "                               verbose=1,\n",
    "                               n_jobs=-1)\n",
    "    \n",
    "    gs_mnb_tfidf.fit(X_val, y_val)\n",
    "    \n",
    "         \n",
    "    clf = gs_mnb_tfidf.best_estimator_\n",
    "    \n",
    "    if clf.get_params()['vect__stop_words']:\n",
    "        stopwords_used = 'yes'\n",
    "    else:\n",
    "        stopwords_used = 'no'\n",
    "    \n",
    "    my_models.append((str(func.__class__.__name__) + ': w/stopwords removed = ' + stopwords_used, clf))\n",
    "    \n",
    "    \n",
    "    print(\"Model cv score: \" + str(gs_mnb_tfidf.best_score_* 100))\n",
    "    \n",
    "print('Best models created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "roc_prediction = []\n",
    "for name, model in my_models:\n",
    "    print('---------------------------------')\n",
    "    print('Cross validation of model: ' + name + ' with settings: \\n')\n",
    "    print(str(model.get_params()['clf']) + '\\n')\n",
    "    \n",
    "    cv = ShuffleSplit(n_splits=cross_validation_iterations, test_size=0.2, random_state=42)\n",
    "    \n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, \n",
    "                                                 cv=cv, scoring='accuracy',\n",
    "                                                 verbose=1,\n",
    "                                                 n_jobs=-1)\n",
    "    roc_prediction.append(model.predict(X_test))\n",
    "    print(cv_results)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('---------------------------------')\n",
    "    print('Complete')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 10))\n",
    "\n",
    "for result in range(0, len(results)):\n",
    "    fpr, tpr, threshold = roc_curve(y_test, roc_prediction[result]) \n",
    "\n",
    "# This is the AUC\n",
    "    the_auc = auc(fpr, tpr)\n",
    "# This is the ROC curve\n",
    "\n",
    "    plot_label = \"`\" + names[result] + \"` (area = \"+str(round(the_auc, 4))+\").\"\n",
    "    \n",
    "    axes[0].plot(fpr,tpr, label=plot_label)\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--') # diagonal    \n",
    "\n",
    "axes[0].set_title('ROC Curve \\n'+\n",
    "                  'Best accuracy StopWord paramiter from each model. \\n' +\n",
    "                 'Quantity (Train: ' + str(X_train.count()) + '; Test:' + str(X_test.count())+') ')\n",
    "axes[0].set_xlabel('False positive rate')\n",
    "axes[0].set_ylabel('True positive rate')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].xaxis.grid(True)\n",
    "\n",
    "spacing = 0.1 # This can be your user specified spacing. \n",
    "minorLocator = MultipleLocator(spacing)\n",
    "\n",
    "axes[0].yaxis.set_minor_locator(minorLocator)\n",
    "axes[0].xaxis.set_minor_locator(minorLocator)\n",
    "axes[0].grid(which = 'minor')\n",
    "\n",
    "\n",
    "# box plot\n",
    "\n",
    "bplot2 = axes[1].boxplot(results,\n",
    "                         vert=True,  # vertical box alignment\n",
    "                         patch_artist=True,  # fill with color\n",
    "                         labels=names\n",
    "                         )  # will be used to label x-ticks\n",
    "axes[1].set_title('Model Accuracy \\n'+ \n",
    "                  'Performed with ' + str(cross_validation_iterations) +' cross validation iterations. \\n' + \n",
    "                  'Quantity (Train: ' + str(X_train.count()) + '; Test:' + str(X_test.count())+') ')\n",
    "\n",
    "axes[1].set_xlabel('Models used')\n",
    "axes[1].set_ylabel('Accuracy Recorded (range(0 - 1) = 0 - 100%)')\n",
    "\n",
    "# fill with colors\n",
    "colors = ['#CBD9D6', '#8DA593', '#D9CBA3', '#F2A88C', '#D98282']\n",
    "for patch, color in zip(bplot2['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# adding horizontal grid lines\n",
    "for ax in axes:\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "    \n",
    "    \n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
