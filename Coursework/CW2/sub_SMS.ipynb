{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification - IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#for nlp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#text vectorisation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc\n",
    "\n",
    "#import method releated to evaluation\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, StratifiedShuffleSplit, ShuffleSplit, cross_val_score, GridSearchCV\n",
    "\n",
    "#classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#for graphs\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vairiables for script\n",
    "\n",
    "cross_validation_iterations = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'SMSSpamData.csv'\n",
    "SPAM_dataframe = pd.read_csv(filename, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SPAM_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Class Index to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {label:idx for idx,label in enumerate(np.unique(SPAM_dataframe['class']))}\n",
    "\n",
    "print(class_mapping)\n",
    "class_labels = [x for x in class_mapping] # store the class labels for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the mapping dictionary to transform the class labels into integers\n",
    "\n",
    "SPAM_dataframe[\"class\"] = SPAM_dataframe[\"class\"].map(class_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPAM_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SPAM_dataframe.loc[42, 'sms_msg']#[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import regular expressions to clean up the text\n",
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) # remove all html markup\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) # findall the emoticons\n",
    "    \n",
    "    # remove the non-word chars '[\\W]+'\n",
    "    # append the emoticons to end \n",
    "    #convert all to lowercase\n",
    "    # remove nose char for consistency\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', '')) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor(SPAM_dataframe.loc[42, 'sms_msg'])#[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the preprocessor to the entire dataframe (i.e. column review)\n",
    "SPAM_dataframe['sms_msg'] = SPAM_dataframe['sms_msg'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenise, Stemmer & Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = str(text)\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_stemmer(text):\n",
    "    return [stemmer.stem(word) for word in tokenizer(text)]#text.split()]\n",
    "\n",
    "\n",
    "def stop_removal(text):\n",
    "       return [w for w in text if not w in stop]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SPAM_dataframe.loc[42, 'sms_msg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPAM_dataframe_subset = SPAM_dataframe.sample(n=1000)\n",
    "SPAM_dataframe_subset = SPAM_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     SPAM_dataframe_subset.iloc[:,1], SPAM_dataframe_subset.iloc[:,0], test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid0 = [{'vect__ngram_range': [(1, 1)], #can also extract 2-grams of words in addition to the 1-grams (individual words)\n",
    "               'vect__stop_words': [stop, None], # use the stop dictionary of stopwords or not\n",
    "               'vect__tokenizer': [tokenizer_stemmer]}, # use a tokeniser and the stemmer \n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "results_list = []\n",
    "\n",
    "names = []\n",
    "\n",
    "raw_names = []\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "predictions = []\n",
    "\n",
    "best_settings = []\n",
    "\n",
    "\n",
    "for func in [LogisticRegression(solver='lbfgs', multi_class='multinomial'),\n",
    "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
    "                   SVC(kernel='rbf', gamma=0.7, C=1.0)]:\n",
    "    results = {}\n",
    "    \n",
    "    stop_results = []\n",
    "    none_results = []\n",
    "    \n",
    "    print(\"Testing: \" + func.__class__.__name__)\n",
    "    \n",
    "    mnb_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf',  func)])\n",
    "                   \n",
    "    gs_mnb_tfidf = GridSearchCV(mnb_tfidf, param_grid0,\n",
    "                               scoring='accuracy',\n",
    "                               cv=cross_validation_iterations,\n",
    "                               verbose=1,\n",
    "                               n_jobs=-1)\n",
    "    \n",
    "    gs_mnb_tfidf.fit(X_train, y_train)\n",
    "    \n",
    "         \n",
    "    clf = gs_mnb_tfidf.best_estimator_\n",
    "    \n",
    "    print('Test Accuracy: %.3f' % (clf.score(X_test, y_test) * 100))\n",
    "\n",
    "    print('CV Accuracy: %.3f' % (gs_mnb_tfidf.best_score_ * 100))\n",
    "    \n",
    "    results['cv_acc'] = gs_mnb_tfidf.best_score_* 100\n",
    "    \n",
    "    results['accuracy'] = clf.score(X_test, y_test) * 100\n",
    "    \n",
    "    results['name'] = func.__class__.__name__ \n",
    "    \n",
    "    results_list.append(results)\n",
    "    \n",
    "    temp_split = []\n",
    "    \n",
    "    for split in range (0, gs_mnb_tfidf.n_splits_):\n",
    "        \n",
    "        print(\"CV split \" + str(split + 1) + \" results:\")\n",
    "        \n",
    "        print(gs_mnb_tfidf.cv_results_['split'+ str(split) +'_test_score'])\n",
    "        \n",
    "        stop_results.append(gs_mnb_tfidf.cv_results_['split'+ str(split) +'_test_score'][0])\n",
    "        none_results.append(gs_mnb_tfidf.cv_results_['split'+ str(split) +'_test_score'][1])\n",
    "        \n",
    "        \n",
    "    predictions.append(clf.predict(X_test))\n",
    "    best_settings.append(clf.get_params(GridSearchCV)['vect__stop_words']  )\n",
    "        \n",
    "    raw_names.append(func.__class__.__name__)\n",
    "    \n",
    "    names.append(func.__class__.__name__ + \" StopWords\")\n",
    "    cv_results.append(stop_results)\n",
    "\n",
    "    names.append(func.__class__.__name__ + \" None\")\n",
    "    cv_results.append(none_results)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results_list) \n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_mnb_tfidf.n_splits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in range(0, len(cv_results)):\n",
    "    print(names[method])\n",
    "    print(cv_results[method])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs_mnb_tfidf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# fig.suptitle('Algorithm Comparison')\n",
    "# ax = fig.add_subplot(111)\n",
    "# plt.boxplot(cv_results)\n",
    "# ax.set_xticklabels(names)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 10))\n",
    "\n",
    "for predict in range(0, len(predictions)):\n",
    "    fpr, tpr, threshold = roc_curve(y_test, predictions[predict]) \n",
    "\n",
    "# This is the AUC\n",
    "    the_auc = auc(fpr, tpr)\n",
    "# This is the ROC curve\n",
    "    if best_settings[predict]:\n",
    "        plot_label = \"`\" + raw_names[predict] + \"` with StopWords (area = \"+str(round(the_auc, 4))+\").\"\n",
    "    else:\n",
    "        plot_label = \"`\" + raw_names[predict] + \"` with no StopWords (area = \"+str(round(the_auc, 4))+\").\"\n",
    "    axes[0].plot(fpr,tpr, label=plot_label % (the_auc))\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--') # diagonal    \n",
    "\n",
    "axes[0].set_title('ROC Curve \\n'+\n",
    "                  'Best accuracy StopWord paramiter from each model. \\n' +\n",
    "                 'Quantity (Train: ' + str(X_train.count()) + '; Test:' + str(X_test.count())+') ')\n",
    "axes[0].set_xlabel('False positive rate')\n",
    "axes[0].set_ylabel('True positive rate')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].xaxis.grid(True)\n",
    "\n",
    "spacing = 0.1 # This can be your user specified spacing. \n",
    "minorLocator = MultipleLocator(spacing)\n",
    "\n",
    "axes[0].yaxis.set_minor_locator(minorLocator)\n",
    "axes[0].xaxis.set_minor_locator(minorLocator)\n",
    "axes[0].grid(which = 'minor')\n",
    "\n",
    "\n",
    "# box plot\n",
    "\n",
    "bplot2 = axes[1].boxplot(cv_results,\n",
    "                         vert=True,  # vertical box alignment\n",
    "                         patch_artist=True,  # fill with color\n",
    "                         labels=names\n",
    "                         )  # will be used to label x-ticks\n",
    "axes[1].set_title('Model Accuracy \\n'+ \n",
    "                  'Performed with ' + str(cross_validation_iterations) +' cross validation iterations. \\n' + \n",
    "                  'Quantity (Train: ' + str(X_train.count()) + '; Test:' + str(X_test.count())+') ')\n",
    "\n",
    "axes[1].set_xlabel('Models used')\n",
    "axes[1].set_ylabel('Accuracy Recorded (range(0 - 1) = 0 - 100%)')\n",
    "\n",
    "# fill with colors\n",
    "colors = ['#CBD9D6', '#8DA593', '#D9CBA3', '#F2A88C', '#D98282']\n",
    "for patch, color in zip(bplot2['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# adding horizontal grid lines\n",
    "for ax in axes:\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "    \n",
    "    \n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
