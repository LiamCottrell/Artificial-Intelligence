{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install --yes seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import method releated to evaluation\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, StratifiedShuffleSplit, ShuffleSplit, cross_val_score, GridSearchCV\n",
    "\n",
    "#transformers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "#export the models from the sklearn library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue with Diabetes Dataset - Prepare Dataset\n",
    "We will use the Pima Indians diabetes dataset to explore the different evaluation methods provided in sklearn. \n",
    "It contains 768 rows and 9 features and is used to build models that can predict the onset of diabetes based on diagnostic measures.\n",
    "Description of the features follow:\n",
    "\n",
    "0 preg = Number of times pregnant\n",
    "\n",
    "1 plas = Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "\n",
    "2 pres = Diastolic blood pressure (mm Hg)\n",
    "\n",
    "3 skin = Triceps skin fold thickness (mm)\n",
    "\n",
    "4 test = 2-Hour serum insulin (mu U/ml)\n",
    "\n",
    "5 mass = Body mass index (weight in kg/(height in m)^2)\n",
    "\n",
    "6 pedi = Diabetes pedigree function\n",
    "\n",
    "7 age = Age (years)\n",
    "\n",
    "8 class = Class variable (1:tested positive for diabetes, 0: tested negative for diabetes)\"\n",
    "\n",
    "All of the values in the file are numeric, specifically floating point values. \n",
    "We will learn how to load the file first, then use it with sklearn by dividing the data into class labels (Y) and feature sets (X). We dont need to split it into test/train as we are going to use scklearn methods to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "filename = \"data/pima-indians-diabetes.data.csv\"\n",
    "#lets add some column names as the CSV file does not have these\n",
    "names = ['pregnancies', 'glucose', 'bp', 'skin', 'insuline', 'bmi', 'pedi', 'age', 'class']\n",
    "#lets use the convinient pandas library to read and form a data frame of our dataset\n",
    "dataframe = pandas.read_csv(filename, names=names)\n",
    "\n",
    "seed =100 #useful for random generators\n",
    "\n",
    "array = dataframe.values\n",
    "# divide data into label and feature sets.\n",
    "# this dataset has its class column in location 8\n",
    "X = array[:,0:8] # upto column index 7 i.e. 0 to 7\n",
    "y = array[:,8] # 8th column index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare models\n",
    "We will use the scikitLearn library to access a bunch of ML models already avaiable\n",
    "and run each of them on the diabetes classification dataset. \n",
    "\n",
    "For instance with a statement such as: \n",
    "models.append(('ANN', MLPClassifier()))\n",
    "\n",
    "we are using sciktlearn's neural net classifier which is a Multi-layer Perceptron classifier. \n",
    "\n",
    "We will be using the default settings for each of the classifiers. \n",
    "\n",
    "We encourage you to look up the scikitlearn documentation on classifiers to understand the different hyper-parameter settings and how you might change them (http://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(kernel='rbf', gamma=0.7, C=1.0)))\n",
    "models.append(('ANN', MLPClassifier()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By evaluating our classifier performance on data that has been seen during training, we could get false confidence in the predictive power of our model. In the worst case, it may simply memorize the training samples but completely fails classifying new, similar samples -- we really don't want to put such a system into production!\n",
    "Instead of using the same dataset for training and testing (this is called \"resubstitution evaluation\"), it is much much better to use a train/test split in order to estimate how well your trained model is doing on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Fold Cross validation\n",
    " \n",
    "Often (labeled) data is precious, but with the previous hold-out (test_train_split) approach we can use only \n",
    "~ 3/4 of our data for training. On the other hand, we will only ever try to apply our model 1/4 of our data for testing. A common way to use more of the data to build a model, but also get a more robust estimate of the generalization performance, is to use cross-validation. \n",
    "\n",
    "In cross-validation, the data is split repeatedly into a training and non-overlapping test-sets, with a separate model built for every pair. Each split of the data is called a fold. \n",
    "Essentially the algorithm is trained on k-1 folds with one held back and tested on the held back fold. This is repeated so that each fold of the dataset is given a chance to be the held back test set.\n",
    "The most common way to do cross-validation is k-fold cross-validation, in which the data is first split into k (often 5 or 10) equal-sized folds, and then for each iteration, one of the k folds is used as test data, and the rest as training data.\n",
    "\n",
    "The test-set scores are then aggregated for a more robust estimate.\n",
    "\n",
    "This way, each data point will be in the test-set exactly once, and we can use all but a k'th of the data for training. Let us apply this technique to evaluate the KNeighborsClassifier algorithm on the Iris dataset:\n",
    "\n",
    "Cross validation is an approach that you can use to estimate the performance of a machine learning algorithm with less variance than a single train-test set split.\n",
    "\n",
    "After running cross validation you end up with k different performance scores that you can summarize using a mean and a standard deviation.\n",
    "The result is a more reliable estimate of the performance of the algorithm on new data given your test data. It is more accurate because the algorithm is trained and evaluated multiple times on different data.\n",
    "\n",
    "The choice of k must allow the size of each test partition to be large enough to be a reasonable sample of the problem, whilst allowing enough repetitions of the train-test evaluation of the algorithm to provide a fair estimate of the algorithms performance on unseen data. For modest sized datasets in the thousands or tens of thousands of records, k values of 3, 5 and 10 are common.\n",
    "\n",
    "In the example below we use 10-fold cross validation.\n",
    "The model_selection.KFold forms the kfold and model_selection.cross_val_score aggregates the scores (specified here as scoring = 'accuracy') providing mean and standard deviation over the specified number of runs (specified here as n_splits=10).\n",
    "\n",
    "For more details you can also read up on the function at http://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "##iterate through each model\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When do we Need Stratification?\n",
    "By default, cross_val_score will use StratifiedKFold for classification, which ensures that the class proportions in the dataset are reflected in each fold. If you have a binary classification dataset with 90% of data point belonging to class 0, that would mean that in each fold, 90% of datapoints would belong to class 0. If you would just use KFold cross-validation, it is likely that you would generate a split that only contains class 0. It is generally a good idea to use StratifiedKFold whenever you do classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5)\n",
    "idx = 0\n",
    "for train, test in kfold.split(X, y):\n",
    "    print(\"Fold: \", idx)\n",
    "    print(test)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, cross_val_score will use StratifiedKFold for classification, which ensures that the class proportions in the dataset are reflected in each fold. If you have a binary classification dataset with 90% of data point belonging to class 0, that would mean that in each fold, 90% of datapoints would belong to class 0. If you would just use KFold cross-validation, it is likely that you would generate a split that only contains class 0. It is generally a good idea to use StratifiedKFold whenever you do classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset - Understanding Stratified Cross-Fold Validation\n",
    "\n",
    "\n",
    "The labels in iris are sorted, which means that if we split the data without stratification then the first fold will only have the label 0 in it, while the last one will only have the label 2.\n",
    "To avoid this we want stratification to select instances for folds from different areas of the dataset to ensure a class distribution that is representative of the original dataset. \n",
    "\n",
    "Next we will attempt to understand this different by plotting the locations from where instances are selected for folds over the full dataset.\n",
    "\n",
    "We will do this by plotting the results for both stratified and non-stratified versions.\n",
    "We do this by creating a binary vectored mask which marks the location  /index of each \n",
    "test instances selected from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets load the Iris dataset which is part of the sklearn library\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "class_names = iris.target_names\n",
    "\n",
    "print(\"Number of instances: \", len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset - Visualising Membership of Instances in Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to help with visualisation\n",
    "# lets define a generic plot method\n",
    "# which takes cross-validation folds together with the dataset (X) and class labels (y)\n",
    "def plot_cv(cv, X, y):\n",
    "    masks = []\n",
    "    for train, test in cv.split(X, y):\n",
    "        mask = np.zeros(len(y), dtype=bool) # initialise a mask of length X\n",
    "       \n",
    "        mask[test] = 1 # set the mask location True if the corresponding instance was selected as a test instance\n",
    "        #print(test)\n",
    "        #print(mask)\n",
    "        masks.append(mask) # append the mask from this fold\n",
    "    \n",
    "    #matshow displays an array as an impage\n",
    "    # cmap is a colour map using the mask to show area where test instance indexes were extracted\n",
    "    # each mask corresponding to a fold is displayed in a row\n",
    "    plt.matshow(masks, cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stratified\")\n",
    "plot_cv(StratifiedKFold(n_splits=5), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Not Stratified\")\n",
    "plot_cv(KFold(n_splits=5), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "- Can you understand the difference due to stratification from the graphoc above?\n",
    "- There is also a version of stratification for the basic train_test_split that we worked with earlier. It is called StratifiedShuffleSplit. We return to it at the end of this lab but you can find out more about it here http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset - StratifiedShuffleSplit to generate several train-test splits\n",
    "This generator randomly splits the data into train and test repeatedly. With stratification it ensures that the split maintains class distribution in both train and test. \n",
    "This allows the user to specify the number of repetitions and the training/test set size independently.\n",
    "\n",
    "Note this is very similar to test-train-splits but is stratified by default and by specifying n_splits it repeatedly creates several train-test splits to run models and aggregation an average accuracy over all the train-test splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cv(ShuffleSplit(n_splits=10, test_size=.03, random_state=seed), iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely you will notice that the first row is not well distributed in terms of where the test instances are being picked. Remember that in the Iris dataset the instances are originally ordered by class.\n",
    "Ideally what we want is to select test instances from across the avaiable classes. \n",
    "You can now visualise this using the StratifiedShuffleSplit version.\n",
    "Note that it is unlikly that you will need to select 0.03 percent only as test set; but we have used this very low value so that you can dive into and understand the plot better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cv(StratifiedShuffleSplit(n_splits=10, test_size=.03, random_state=seed), iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use all of these cross-validation generators with the cross_val_score method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "# use 10 iterations of cross validation\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "##iterate through each model\n",
    "for name, model in models:\n",
    "    kfold = StratifiedShuffleSplit(n_splits=10, random_state=seed, test_size=.2)\n",
    "    cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "In all our evaluations so far we have been working with accuracy as the main metric of comparison. \n",
    "\n",
    "However there are several other scoring metrics (e.g. precision, recall, f1-score, support) other than \"accuracy\" that can be used to compare results. Modify the code above to explore these metrics. Note that in Scikit, default \"precision\", \"recall\" and \"f1-score\" are for binary class only. For multi-class, you will need to use a variant that sepcifies the averaging method e.g. \"precision_micro\". Other averaging options are \"macro\" and \"weighted\". You can read about the different metrics here: http://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules\n",
    "\n",
    "LeaveOneOut (or LOO) is a simple cross-validation approach. \n",
    "You can import as \"from sklearn.model_selection import LeaveOneOut\". Modify the code above to explore LeaveOneOut. \n",
    "\n",
    "Each learning set is created by taking all the samples except one, the test set being the sample left out. Thus, for  samples, we have  different training sets and  different tests set. This cross-validation procedure does not waste much data as only one sample is removed from the training set. However it is time consuming ans so mainly used with small datasets. \n",
    "You can read more about it here http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html#sklearn.model_selection.LeaveOneOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST - Advance Evaluation Setup - How to use Grid Search CV with MNIST?\n",
    "GridSearchCV carries out an exhaustive search over specified parameter values for a classifier.\n",
    "\n",
    "Like other cross validation methods, Gridsearch splits up your test set into eqally sized folds, \n",
    "uses one fold as test and the rest for training. \n",
    "\n",
    "In this way it optimizes as many classifiers as parts you split your data into.\n",
    "\n",
    "You need to specify the number of folds, a classifier (such as MLP), and a Grid of parameters you may want to optimize as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "filename = \"data/mnist_60K.csv\"\n",
    "print('sampling with replacement ... this will take a moment ...')\n",
    "\n",
    "#lets use the convinient pandas library to read and form a data frame of our dataset\n",
    "mnist = pandas.read_csv(filename, header=None)\n",
    "#df.head()\n",
    "\n",
    "#store the class names for plotting later on\n",
    "class_names = []\n",
    "for i in range(0,10):\n",
    "    class_names.append(str(i))\n",
    "\n",
    "\n",
    "#use a very small fraction of the 60K dataset\n",
    "array = mnist.sample(frac=0.01, replace=True).values\n",
    "print('shape of data sampled:', array.shape)\n",
    "\n",
    "\n",
    "X = array[:,1:] \n",
    "y = array[:,0] # mnist has its class column in first position\n",
    "print('shape of X:', X.shape)\n",
    "print('shape of y:', y.shape)\n",
    "labels = np.asfarray(y)\n",
    "\n",
    "\n",
    "#basic plot to visualise the histogram of the class values\n",
    "plt.hist(labels, bins=[0,1,2,3,4,5,6,7,8,9]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST - Setup a grid search to explore a variety of parameters relevant to a classifier\n",
    "In this example we will setup grid search for a MLPClassifier model (i.e. a neural network) from the scikitLearn library. \n",
    "Therefore the grid search will explore a variety of parameters that are relevant to this specific classifier. \n",
    "\n",
    "Obviously if you are working with a different classifier then you must use and explore parameters relevant to that specific classifier. \n",
    "\n",
    "For the MLPClassifier we explore different parameter settings for activation functions, weight update methods and hidden layer sizes.\n",
    "Details on parameters appear http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "        {\n",
    "            'activation' : ['identity', 'logistic', 'relu'], \n",
    "            'solver' : ['lbfgs', 'adam'],\n",
    "            'hidden_layer_sizes': [(10,),(30,)] # a single hidden layer\n",
    "        }\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X[0])\n",
    "#print(X[1])\n",
    "#print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
    "\n",
    "print(\"Searching for best parameters - this will take a moment ...\")\n",
    "clf = GridSearchCV(MLPClassifier(), param_grid, cv=5, scoring='accuracy')\n",
    "clf.fit(X_train,y_train)\n",
    "print(\"Best parameters set found on validation:\")\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST - Use the best found parameter combination to create a model\n",
    "\n",
    "We use the parameters that were returned by GridSearchCV; (in our example this will be clf.best_params_) to create the new model.\n",
    "We also use two new method to provide some useful statistics about the model performance: classification_report and confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_params_)\n",
    "\n",
    "print(\"Predicting on the test set\")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Target labels: \", y_test)\n",
    "print(\"Predicted labels: \", y_pred)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "\n",
    "y_test2 = pandas.Series(y_test)\n",
    "y_pred2 = pandas.Series(y_pred)\n",
    "\n",
    "pandas.crosstab(y_test2, y_pred2, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST - How to plot and visualise a confusion matrix?\n",
    "For this we will use the heatmap function from the seaborn library. You can compare the quality of the graph provided with and without seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=confusion_matrix(y_test, y_pred)\n",
    "plt.imshow(conf, cmap=\"YlGnBu\", interpolation='None')\n",
    "plt.show()\n",
    "\n",
    "# or we can use a heatmap from the seaborn library\n",
    "import seaborn as sn\n",
    "df_cm = pandas.DataFrame(conf, range(10), range(10))\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(df_cm, cmap=\"YlGnBu\", annot=True, annot_kws={\"size\": 16})# font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "How might you conduct a grid search with a different classifier from sklearn? For instance the SVM classifier?\n",
    "You can read about this classifier and its paramemeters that you could apply grid search to here http://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
