{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import method releated to evaluation\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, StratifiedShuffleSplit, ShuffleSplit, cross_val_score, GridSearchCV\n",
    "\n",
    "#transformers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "#export the models from the sklearn library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "df=pandas.DataFrame(X, y)\n",
    "df.tail()\n",
    "df.shape\n",
    "\n",
    "seed =100\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a pipeline\n",
    "Lets start by standadizing the breast cancer dataset. Lets also assume we want to compress the features into 2 principal components. For this we will use principal component analysis as a feature extractor method to dimensionality reduction.\n",
    "We can include each of these steps in a pipeline which will see next. \n",
    "<img src=\"pipeline.png\", width=500>\n",
    "The pipeline object takes a list of tuples as input, where the first vaule in each tuple is an arbitrary identifier string that we can use to access the individual elements in the pipeline. The second element in every tuple is a scikit-learn transformer or estimator. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), # normilising values\n",
    "                        PCA(n_components=2), # extracting features\n",
    "                        LogisticRegression(random_state=1)) # specify a basic classifier\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print('Test Accuracy: %.3f' % pipeline.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Repeat the code above for a different classifier. For instance you could use SVC or MLPClassifier. Note a list of classifiers were provided in evaluation part1 e.g. \n",
    "LogisticRegression(), LinearDiscriminantAnalysis(), KNeighborsClassifier(), DecisionTreeClassifier(), GaussianNB(), SVC(kernel='rbf', gamma=0.7, C=1.0), MLPClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding underfitting and overfitting problems\n",
    "we will see how to address these model issues using validation curves. For this we will use scikit-learn's learning curve function to evaulate the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(),\n",
    "                       LogisticRegression(penalty='l2', random_state=1))\n",
    "\n",
    "train_sizes, train_scores, test_scores =\\\n",
    "                learning_curve(estimator=pipe,\n",
    "                               X=X_train,\n",
    "                               y=y_train,\n",
    "                               train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                               cv=10,\n",
    "                               n_jobs=1)\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='training accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='validation accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.8, 1.03])\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/06_05.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_sizes parameter in the learning_curve function, we can control the absolute or relative number of training samples that are used to generate the learning curve. Here we set training_sizes = np.linspace(0.1, 1.0, 10) to use 10 evenly spaced relative intervals for the training set sizes. \n",
    "By default the learning_curve function uses stratified k-fold cross validationto calculate the cross-validation accuracy. We set k =10 via the cv parameter. \n",
    "the mathplotlib simply plots on the x axis for increasing values of the training set size the accuracy on the y-axis. \n",
    "Note the use of the standard deviation to colour the area above and below the mean accuracy values of the validation accuracy. This is done using the fill_between function to indicate the variance estimation. \n",
    "\n",
    "From the plot we can conclude that our model is doing good; however there might be some evidence to suggest a slight overfitting to training data; due to the visible (though small ) gap with the validation accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Once again you can try the above code for different classifiers. However make sure relevant parameters are set with each. For example a basic ANN can be called with MLPClassifier(). You can read up on the different hyper parameters and how to set them here http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "To study the different metrics provided in sklearn; lets first create grid search cross validation with a basic MLPClassifier.\n",
    "You can study the param_grid to understand the different hyper parameter values that we plan to explore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "#pipe.fit(X_train, y_train)\n",
    "#y_pred = pipe.predict(X_test)\n",
    "\n",
    "param_grid = [\n",
    "        {\n",
    "            'activation' : ['identity', 'logistic', 'relu'], \n",
    "            'solver' : ['lbfgs', 'adam'],\n",
    "            'hidden_layer_sizes': [(10,),(30,)] # a single hidden layer\n",
    "        }\n",
    "       ]\n",
    "\n",
    "gs = GridSearchCV(MLPClassifier(), param_grid, cv=10, scoring='accuracy')\n",
    "gs.fit(X_train,y_train)\n",
    "y_pred = gs.predict(X_test)\n",
    "\n",
    "clf = gs.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "print('Test accuracy: %.3f' % clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall Scores\n",
    "Scoring metrics are all implmented in scikit-learn and can be imported from the sklearn.metrics module. \n",
    "For multiclass metrics these need to be used with macro or micro options. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(y_pred, y_test)\n",
    "print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer - Generating ROC Curves on a Binary Classification Task\n",
    "We will use the load function (load_breast_cancer) that is already provided with scikitLearn for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=seed)\n",
    "\n",
    "# k-NN model creates confidences based on the mean of the k nearest neighbors\n",
    "model1 = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred1 = model1.predict(X_test)\n",
    "print(\"Training set score: %f\" % model1.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % model1.score(X_test, y_test))\n",
    "\n",
    "model2 = KNeighborsClassifier(n_neighbors=21, weights='distance')\n",
    "model2.fit(X_train, y_train)\n",
    "y_pred2 = model2.predict(X_test)\n",
    "print(\"Training set score: %f\" % model2.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % model2.score(X_test, y_test))\n",
    "\n",
    "#Neural nets creates confidences based on the output activation\n",
    "model3=MLPClassifier(hidden_layer_sizes=(300,), max_iter=20,)\n",
    "model3.fit(X_train, y_train)\n",
    "y_pred3 = model3.predict(X_test)\n",
    "print(\"Training set score: %f\" % model3.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % model3.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What information do we need to generate the ROC curve?\n",
    "\n",
    "## Receiver Operating Characteristic (ROC)\n",
    "\n",
    "\n",
    "\n",
    "ROC curves typically feature true positive rate on the Y axis, and false\n",
    "positive rate on the X axis. This means that the top left corner of the plot is\n",
    "the \"ideal\" point - a false positive rate of zero, and a true positive rate of\n",
    "one. This is not very realistic, but it does mean that a larger area under the\n",
    "curve (AUC) is usually better.\n",
    "\n",
    "The \"steepness\" of ROC curves is also important, since it is ideal to maximize\n",
    "the true positive rate while minimizing the false positive rate.\n",
    "\n",
    "\n",
    "Example of Receiver Operating Characteristic (ROC) metric to evaluate\n",
    "classifier output quality appears next.\n",
    "Primarily we need to have the false positive rate and true positive rate at hand. \n",
    "Additionally the plot requires different levels of thresholds at which it will generate the tpr and fpr; that then get plotted. \n",
    "You can see from the accuracy results and the ROC curve that the neural net (MLP) struggles to acheive good performance compared to kNN on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr1, tpr1, threshold1 = roc_curve(y_test, y_pred1) \n",
    "fpr2, tpr2, threshold2 = roc_curve(y_test, y_pred2) \n",
    "fpr3, tpr3, threshold3 = roc_curve(y_test, y_pred3) \n",
    "# This is the AUC\n",
    "auc1 = auc(fpr1, tpr1)\n",
    "auc2 = auc(fpr2, tpr2)\n",
    "auc3 = auc(fpr3, tpr3)\n",
    "#auc = np.trapz(tpr1,fpr1)\n",
    "\n",
    "# This is the ROC curve\n",
    "plt.plot(fpr1,tpr1, label='k=5 (area = %0.2f)' % (auc1))\n",
    "plt.plot(fpr2,tpr2, label='k=21 (area = %0.2f)' % (auc2))\n",
    "plt.plot(fpr3,tpr3, label='MLP (area = %0.2f)' % (auc2))\n",
    "plt.plot([0, 1], [0, 1], 'k--') # diagonal\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional ROC curves using all the models on the breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(kernel='rbf', gamma=0.7, C=1.0)))\n",
    "models.append(('ANN', MLPClassifier()))\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=10, test_size=0.25, random_state=seed)\n",
    "split.get_n_splits(X, y)\n",
    "\n",
    "for train_index, test_index in split.split(X, y):\n",
    "    #print('TRAIN:', train_index)\n",
    "    #print('TEST:', test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "##iterate through each model\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    fpr, tpr, threshold = roc_curve(y_test, y_pred) \n",
    "    #auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr,tpr, label=name)# '%s (area = %0.2f)' % (name, auc))\n",
    "    print(fpr, tpr, threshold)\n",
    "    results.append([fpr, tpr, threshold])\n",
    "    names.append(name)\n",
    "\n",
    "results = np.asarray(results)\n",
    "plt.plot([0, 1], [0, 1], 'k--') # diagonal\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show() \n",
    "#print('std', results.std())\n",
    "#msg = \"%s: %f (%f)\" % (name, results.mean(), results.std())\n",
    "#print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "Which category of classifiers do you think are performing best on this dataset?\n",
    "\n",
    "Try the above comparison with a stratified 10 fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Dataset for scoring for multiclass metrics\n",
    "## Recall, Precision and F1 \n",
    "\n",
    "Options for multiclass situations other than with 'accuracy' commonly uses the macro and micro versions of recall, precision and F1 measures. Macro aggregations treat all classes as equally impportant. However this can lead to misleading resulst for instance if you have a unbalanced dataset where one class few instances that are all correctly classifier yet another class has many instances but are incorrectly classified. Here the Micro aggregation version is better suited at it will capture a less optimistic view of the algorithm performance. \n",
    "Note that when we use recall and precision related metrics on multi-class problems the metric calculation will adopt a one-versus-all view of the classifier to count the TPs and FPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "scoring = 'recall_macro'\n",
    "scoring_list = ['f1_macro', 'f1_micro', 'f1_weighted', 'precision_macro', 'precision_micro', 'precision_weighted', \n",
    "                'recall_macro', 'recall_micro', 'recall_weighted']\n",
    "model = KNeighborsClassifier()\n",
    "#lets load the Iris dataset which is part of the sklearn library\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "class_names = iris.target_names\n",
    "\n",
    "kfold = StratifiedShuffleSplit(n_splits=10, random_state=seed, test_size=.4)\n",
    "for scoring in scoring_list:\n",
    "    print(scoring)\n",
    "    cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "    print(cv_results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Do you understand why we need macro and micro versions of recall and precision scoring with multi-class datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
