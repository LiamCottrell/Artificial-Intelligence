{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percptron Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all our libraries here\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt # this library will be used for the scatter plot\n",
    "import numpy as np \n",
    "import pandas as pd # to manage data frames and reading csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Creating a Perceptron Classifier Class\n",
    "The first step is to develop a function that can make predictions.\n",
    "This will be needed both in the evaluation of candidate weights values in stochastic gradient descent, and after the model is finalized and we wish to start making predictions on test data or new data.\n",
    "Below is a function named predict() that predicts an output value for an given instance using a set of weights.\n",
    "The first weight is always the bias as it is standalone and not responsible for a specific input value.\n",
    "Note: the best weights will be learnt iterative with gradient descent through the train method\n",
    "\n",
    "\n",
    "## Training Network Weights\n",
    "We can estimate the weight values for our training data using stochastic gradient descent.\n",
    "\n",
    "Stochastic gradient descent requires two parameters:\n",
    "\n",
    "Learning Rate: Used to limit the amount each weight is corrected each time it is updated.\n",
    "Epochs: The number of times to run through the training data while updating the weight.\n",
    "These, along with the training data will be the arguments to the function.\n",
    "\n",
    "There are 3 loops we need to perform in the function:\n",
    "\n",
    "Loop over each epoch.\n",
    "Loop over each data instance in the training data for an epoch.\n",
    "Loop over each weight and update it for the training instance in an epoch.\n",
    "As you can see, we update each weight for each instance in the training data, each epoch.\n",
    "\n",
    "Weights are updated based on the error the model made. The error is calculated as the difference between the expected output value and the prediction made with the candidate weights.\n",
    "\n",
    "There is one weight for each input attribute, and these are updated in a consistent way. Remember the bias usually needs to be multiplied with 1. \n",
    "\n",
    "w(t+1)= w(t) + learning_rate * (expected(t) - predicted(t)) * x(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron(object):\n",
    "    \"\"\"Perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    lr : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    input_size : int\n",
    "      number of features in an instance.\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "    epochs : int\n",
    "        Number of epochs for training the network towards achieving convergence\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    W : 1d-array\n",
    "      Weights after fitting.\n",
    "    E : list\n",
    "      Sum-of-squares cost function value in each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, lr=0.01, epochs=50, random_state=1):\n",
    "               \n",
    "        self.input_size = input_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.E= []\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        rgen = np.random.RandomState(self.random_state) # use a random seed and draw from a normal dist centered on zero\n",
    "        self.W = rgen.normal(loc=0.0, scale=0.01, size=self.input_size+1) #initialise weights and add one for bias\n",
    "        #self.W = np.random.normal(0.0, pow(input_size, -0.5), (1, input_size))\n",
    "        #self.W = np.zeros(input_size+1) #initialise weights to zero and add one for bias\n",
    "               \n",
    "        self.E = []\n",
    "        \n",
    "    \n",
    "    def net_input(self, x):\n",
    "        z = self.W.T.dot(x) # dot product between input and the weights matrix \n",
    "        return z\n",
    "    \n",
    "    def activation(self, x):\n",
    "        return 1 if x >= 0.0 else -1 #we have a binary classification\n",
    "    \n",
    "    def predict (self, x):\n",
    "        #z = self.W.T.dot(x) # dot product between input and the weights matrix \n",
    "        z = self.net_input(x)\n",
    "        a = self.activation(z)\n",
    "        return a\n",
    "    \n",
    "    def train(self, X, t): # X is inputs d are the targets\n",
    "        \"\"\"Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "          Training vectors, where n_samples is the number of samples and\n",
    "          n_features is the number of features.\n",
    "        t : array-like, shape = [n_samples]\n",
    "          Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "               \n",
    "        for _ in range(self.epochs):\n",
    "            sum_error = 0.0\n",
    "            for i in range(t.shape[0]): # go through each instance\n",
    "                x = np.insert(X[i], 0, 1) #remember to insert 1 for the bias input\n",
    "                y = self.predict(x)\n",
    "                error = t[i] - y\n",
    "                self.W = self.W + self.lr * error * x\n",
    "                sum_error += error**2 #this is the sum of squared error accumilated over all of the train_set\n",
    "            pass\n",
    "            self.E.append(sum_error)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy dataset to test AND gates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[0,0], [0,1], [1,0], [1,1]]) # this is an input example for an AND gate\n",
    "print(\"Binary inputs to test an AND gate:\", inputs)\n",
    "targets = np.array([-1,-1,-1,1])\n",
    "print(\"Outputs from the AND gate should be:\", targets)\n",
    "p = perceptron(input_size=len(inputs[0]), epochs=10)\n",
    "p.train(inputs, targets)\n",
    "print(\"The weights learnt \",  p.W) # these are the weights that would model an AND gate with a perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise  - logic gates\n",
    "- How would you test if the above weights learnt by the perceptron correctly model an AND gate? Hint: You can create an instance and use the perceptron to make a prediction p.predict(x). But remember to augment the instance with the extra bias feature.\n",
    "- Can the perceptron be used for OR gates? If, yes, you should now try the above for the OR gate. Simply add a cell in the jupyter notebook (+ button) and type in your code and execute that cell. \n",
    "- Perceptron cannot model an XOR gate; can you think why this might be the case?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets try the perceptron on the SONAR Data Classification \n",
    "\n",
    "In this use case, you have been provided with a SONAR data set which contains the data about 208 patterns obtained by bouncing sonar signals off a metal cylinder (naval mine) and a rock at various angles and under various conditions. Now, as you know, a naval mine is a self-contained explosive device placed in water to damage or destroy surface ships or submarines. So, our goal is to build a model that can predict whether the object is a naval mine or rock based on our data set. \n",
    "<img src=\"comic.png\">\n",
    "\n",
    "Now, let us have a look at our SONAR data set:\n",
    "\n",
    "<img src=\"sonar.png\">\n",
    "\n",
    "\n",
    "Here, the overall fundamental procedure will be same as that of AND gate with few difference which will be discussed to avoid any confusion. \n",
    "\n",
    "Lets first read in the sonar training data which is stored as a csv file. Once we create the perceptron model we can test its accuracy on a the disjoint test set. \n",
    "For convinieice I have converted the class label \"R\" into integer 1 and \"M\" into integer 0. Yuo can explore the csv file in an excel spreadsheet. Note that class label is the last column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the Sonar data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sonar-train.csv')\n",
    "df.head() # show the first 5 rows\n",
    "# note you can use tail to view the last 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sonar Data Exercise - Data View:\n",
    "- You should try the tail() function on the sonar data\n",
    "- another useful function is info(); again try this out and exploure the output (e.g. df.info()   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a more detailed view can be had with describe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Count, min and max rows are self-explanatory. std is standard deviation, which measures how dispersed the values are. The 25%, 50% and 75% rows show the corresponding percentiles:  apercentile indicates the value below which a given percentage of observations in a group of observations falls. For example , 25% of the instances have a V1 value lower than 0.013350 , while 50% are lower than 0.022650 and 75% are lower than 0.137100. These are often called 25th percentile (or 1st quartile), the median , and the 75th percentile (or 3rd quartile).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms of  Sonar data\n",
    "We will try to visualise the data using a scatter plot\n",
    "Note that we have 60 features in this data set so we need to choose 2 features for the x and y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring features\n",
    "df. hist(bins=50, figsize=(20,15))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scatter plot of Sonar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the class labels for each instance\n",
    "y = df.iloc[0:, [-1]].values\n",
    "y = np.asfarray(y).flatten()\n",
    "#print(y)\n",
    "#X = df.iloc[0:, [0:59]]\n",
    "#print(X)\n",
    "\n",
    "count = np.where(y == 1, 1, 0).sum() # count of the number of instances that beong to class 1\n",
    "#we can use coount as an index as the instances are sorted by the class label\n",
    "#so if we want to access class=1, then they appear first.\n",
    "#print(count)\n",
    "\n",
    "# extract 2 features using their index location\n",
    "f1 = 0\n",
    "f2 = 10\n",
    "\n",
    "X = df.iloc[0:, [f1, f2]].values\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:count, 0], X[:count, 1], color='red', marker='o', label='1')\n",
    "plt.scatter(X[count:, 0], X[count:, 1], color='blue', marker='x', label='-1')\n",
    "\n",
    "plt.xlabel('feature1 ')\n",
    "plt.ylabel('feature2 ')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "## plt.savefig('images/02_06.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sona Data  Exercise - Scatter Plots\n",
    "You can try out the code above for different feature combinations. \n",
    "Through the scatter plots you will notice that this is a hard data set to linearly separate with just two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets the read the train and test csv files to create and test our model\n",
    "train = pd.read_csv('sonar-train.csv')\n",
    "test = pd.read_csv('sonar-test.csv')\n",
    "train_data_list = train.iloc[0:].values\n",
    "print(train_data_list.shape)\n",
    "test_data_list = test.iloc[0:].values\n",
    "print(test_data_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the perceptron on the SONAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [] #empty list to hold the class labels\n",
    "inputs = [] # empty list to hold the instances\n",
    "\n",
    "#targets = np.zeros(rows) # initialise the 1-dimensional array which will hold the class values\n",
    "#inputs = np.zeros(shape=(rows,cols)) # initialise the 2-dimensional matrix which has the set of train data\n",
    "\n",
    "\n",
    "for instance in train_data_list:\n",
    "    # split it by the commas\n",
    "    #all_values = instance#.split(',') \n",
    "    input = (np.asfarray(instance[:-1]) )# return all except the last element which is the target class label\n",
    "    target = (np.asfarray(instance[-1:]) )\n",
    "       \n",
    "    inputs.append(input) # append to the list of instances\n",
    "    targets.append(int(target)) # append to the list of targets and make sure classes are integers\n",
    "    pass\n",
    "\n",
    "# we will convert our lists into numpy array so its compatible with our perceptron class\n",
    "# for this we use asarray function for this\n",
    "inputs = (np.asarray(inputs)) \n",
    "targets = (np.asarray(targets).flatten())\n",
    "print(len(inputs), len(inputs[0]), len(targets))\n",
    "    \n",
    "# create a new instance of the perceptron class and train it to generate the set of weights\n",
    "p = perceptron(input_size=len(inputs[0]), epochs=100, lr=0.0001)\n",
    "print(\"weights learnt are:\")\n",
    "p.train(inputs, targets) # works well with lr =0.01 and higher epochs like 1000\n",
    "\n",
    "#print(p.W)\n",
    "print(p.E)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the model error with increasing epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5)) #width, height settings for figures\n",
    "plt.plot(range(1, len(p.E) + 1), p.E, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of updates')\n",
    "\n",
    "# plt.savefig('images/02_07.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets Test the Perceptron on the SONAR test data\n",
    "We will keep track of the predicted and actual outputs in order to calculate the accuracy of the perceptron on the unseen test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty list called results to keep track of the network performance on each test instance\n",
    "results = []\n",
    "#print (\"shape \", test_data_list.shape)\n",
    "#print(all_values)\n",
    "\n",
    "#go through all the test instances\n",
    "for instance in test_data_list:\n",
    "    #all_values = instance#.split(',')\n",
    "    input = (np.asfarray(instance[:-1]) )# return all except the last element which is the target class label\n",
    "    #print(len(input))\n",
    "        \n",
    "    target_label = np.asfarray(instance[-1:])\n",
    "    #print(\"Correct label\", int(target_label))\n",
    "    \n",
    "    #query the perceptron with the test input\n",
    "    x = np.insert(input, 0, 1) #remember to add the bias value of 1 to the instance\n",
    "    predict_label = p.predict(x)\n",
    "    #print(\"Predicted class:\", predict_label )\n",
    "    \n",
    "    #compute network error\n",
    "    if (predict_label == target_label):\n",
    "        results.append(1)\n",
    "    else: \n",
    "        results.append(0)\n",
    "        pass\n",
    "    pass\n",
    "        \n",
    "#print network performance as an accuracy metric\n",
    "results_array = np.asfarray(results)\n",
    "print (\"accuracy = \", results_array.sum() / results_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "p1 = perceptron(input_size=len(inputs[0]), epochs=20, lr=0.1)\n",
    "p1.train(inputs, targets)\n",
    "ax[0].plot(range(1, len(p1.E) + 1), np.log10(p1.E), marker='o')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('log(Sum-squared-error)')\n",
    "ax[0].set_title('Learning rate 0.01')\n",
    "\n",
    "p2 = perceptron(input_size=len(inputs[0]), epochs=20, lr=0.0001)\n",
    "p2.train(inputs, targets)\n",
    "ax[1].plot(range(1, len(p2.E) + 1), p2.E, marker='o')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Sum-squared-error')\n",
    "ax[1].set_title('Learning rate 0.0001')\n",
    "\n",
    "# plt.savefig('images/02_11.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises - Perceptron hyperparameters\n",
    "- Explore different learning rate values, and epochs. Do they make any improvement towards the final test accuracy?\n",
    "- Try to identify another dataset from the Uni California Irvine (UCI) ML repository (ideally identify a binary dataset from http://mlr.cs.umass.edu/ml/datasets.html); which is where we obtained the Sonar dataset from. Explore how you might use the above code to apply the perceptron on that dataset.\n",
    "How would you change the weight update steps to implement the Adaline weight update algorithm? Hint: Adaline uses the real values returned by the net_input method instead of the quantisation output returned from the activation method. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
