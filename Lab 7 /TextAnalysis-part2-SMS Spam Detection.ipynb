{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install --yes anaconda beautifulsoup4\n",
    "# !conda install --yes anaconda lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#for nlp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#text vectorisation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "#import method releated to evaluation\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "#classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#for graphs\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Case Study - Mobile Text Spam Detection\n",
    "We first load the text data  \"SMSSpamCollection\"\n",
    "\n",
    "Furthermore, we perform some simple preprocessing and split the data array into two parts:\n",
    "\n",
    "1. `text`: A list of lists, where each sublists contains the contents of our emails\n",
    "2. `y`: our SPAM vs HAM labels stored in binary; a 1 represents a spam message, and a 0 represnts a ham (non-spam) message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/SMSSpamData.csv'\n",
    "df = pd.read_csv(filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "- Examine the df dataframe using functions such as df.head(10), df.columns and df.shape, df.describe\n",
    "- How many text messages are contained in this dataset?\n",
    "\n",
    "You will notice that the class labels column is categorical. This means that we must convert them to integers. \n",
    "The code in the next cell does this by creating a mapping between 'spam' to 1 and 'ham' to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {label:idx for idx,label in enumerate(np.unique(df['class']))}\n",
    "\n",
    "print(class_mapping)\n",
    "class_labels = [x for x in class_mapping] # store the class labels for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the mapping dictionary to transform the class labels into integers\n",
    "\n",
    "df[\"class\"] = df[\"class\"].map(class_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need to preprocess raw text?\n",
    "User generated content particularly such as social media and text messaging normally contain non-word content.\n",
    "\n",
    "Whilst some of this content will be useful (e.g. emoji for sentiment analysis) to a given task; others won't. \n",
    "\n",
    "Accordingly we use regular expressions to clean our text before we can continue with converting text messages into vectors which can then form input into a sklearn classification algorithm. \n",
    "\n",
    "Use the next cell to explore different messages and the types of html tage, emojis and special chars that are contained in this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[15, 'sms_msg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions to clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import regular expressions to clean up the text\n",
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) # remove all html markup\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) # findall the emoticons\n",
    "    \n",
    "    # remove the non-word chars '[\\W]+'\n",
    "    # append the emoticons to end \n",
    "    #convert all to lowercase\n",
    "    # remove nose char for consistency\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', '')) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor(df.loc[15, 'sms_msg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the preprocessor to the entire dataframe (i.e. column review)\n",
    "df['sms_msg'] = df['sms_msg'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the stopwords if not done before (need an Internet connection)\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A basic text pre-processing pipeline\n",
    "The basic pipeline includes stopword removal, tokenising and stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenizer(text):\n",
    "       return text.split()\n",
    "\n",
    "def tokenizer_stemmer(text):\n",
    "    return [stemmer.stem(word) for word in tokenizer(text)]#text.split()]\n",
    "\n",
    "\n",
    "def stop_removal(text):\n",
    "       return [w for w in text if not w in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[180, 'sms_msg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Exercise\n",
    " - Explore how each of the preprocessors can be applied to the an example message. \n",
    " - Would you consider different rules for this dataset compared to the IMDB dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Vectorisation of text data\n",
    "Next lets prepare the data using the CountVectorizer to parse the text data into a bag-of-words model.\n",
    "Thereafter fit a sklearn calssifier.\n",
    "First we start by creating a basic train_test_split to check that the data is trasnformed correctly before setting up a comparative study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.loc[:, 'sms_msg'].values\n",
    "y = df.loc[:, 'class'].values\n",
    "\n",
    "text_train, text_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                          random_state=42,\n",
    "                                                          test_size=0.25,\n",
    "                                                          stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    "This method converts the text into count vector or a binary vector. For details of the avaiable parameters please refer to \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "In particular read up on:\n",
    "- how to vectorise with n_gram_range using different n-grams i.e. unigram, bigram etc- how to vectorise with \n",
    "- how to ignore terms that appear in less than a set number of documents with min_df; and conversely with max_df\n",
    "- how to restrict the vocabulary with max_features\n",
    "- how to set callable preprocessing steps with preprocessor\n",
    "\n",
    "\n",
    "## Exercise \n",
    "Modify the code below to explore parameters such as min_df; max_df; max_features etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X) # Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "\n",
    "X_train = vectorizer.transform(text_train)\n",
    "X_test = vectorizer.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sice of the vocabulary or the number of features in the vector ', len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names()[2000:2020]) #Array mapping from feature integer indices to feature name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Classifier on Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have transformed our text into a vector form, we can train a classifier, for instance a logistic regression classifier, which is a fast baseline for text classification tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the classifier on the testing set. Let's first use the built-in score function, which is the rate of correct classification in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the score on the training set to see how well we do there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a pipeline \n",
    "In the previous example we carried out several steps involving: count vectorising; tfidf transforming and then applying this to both the train and test before fitting a classifier for preditcion. \n",
    "This pipeline of trnasformation steps and the final prediction can be carried out by setting up a pipeline.\n",
    "Instead of using the transformed vectors of X_train and Y_train ; we will use the original train and test which contained the text data i.e. text_train and text_test. These can then be sent through the transformation piepline steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    CountVectorizer(),\n",
    "    TfidfTransformer(),\n",
    "    LogisticRegression(random_state=1))\n",
    "\n",
    "pipeline.fit(text_train, y_train)\n",
    "\n",
    "print('accuracy %s' % pipeline.score(text_test, y_test))\n",
    "\n",
    "y_pred = pipeline.predict(text_test)\n",
    "print(classification_report(y_test, y_pred,target_names=class_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Modify the code above to:\n",
    "- use a Multinomial Naive Bayes instead of a Logistic Regression classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a gridsearch with Cross Validation\n",
    "In the previous cells we calculated results based in a single test-train split. \n",
    "Ideally we want to do this using cross-validation. For this purpose we can use GridSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will setup a parameter grid search for the TfidfVectorizer (which combines CountVectorizer() \n",
    "with TfidfTransformer().\n",
    "Details of this function provides many possible parameters (see http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). \n",
    "\n",
    "Some of the common parameters you may want to explore are listed below with some examples settings:\n",
    "\n",
    "min_df=1,  # min count for relevant vocabulary\n",
    "    \n",
    "max_features=4000,  # maximum number of features\n",
    "\n",
    "strip_accents='unicode',  # replace all accented unicode char by their corresponding  ASCII char\n",
    "\n",
    "analyzer='word',  # features made of words\n",
    "\n",
    "token_pattern=r'\\w{1,}',  # tokenize only words of 4+ chars\n",
    "\n",
    "ngram_range=(1, 1),  # features made of a single tokens\n",
    "\n",
    "use_idf=True,  # enable inverse-document-frequency reweighting\n",
    "\n",
    "smooth_idf=True,  # prevents zero division for unseen words\n",
    "\n",
    "In order to specify a specific parameter in a gridsearch we must use the estimator\\__parameter syntax. \n",
    "\n",
    "For instance if we want to specify a specific value for min_df; we state it as; 'tfidfvectorizer\\__min_df'\n",
    "\n",
    "In the follow-on code we set up alternative values for a few parameters and use a pipeline to setup the cross validation study to search the best combination of parameter values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'tfidfvectorizer__ngram_range': [(1, 1)], #can also extract 2-grams of words in addition to the 1-grams (individual words)\n",
    "               'tfidfvectorizer__stop_words': [stop, None], # use the stop dictionary of stopwords or not\n",
    "               'tfidfvectorizer__max_features': [1000, 4000], # use the stop dictionary of stopwords or not\n",
    "               'tfidfvectorizer__tokenizer': [tokenizer_stemmer]}, # use a tokeniser and the stemmer \n",
    "               ]\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        #max_features=4000,\n",
    "                        min_df=7,\n",
    "                        preprocessor=None)\n",
    "\n",
    "pipeline = make_pipeline(TfidfVectorizer(strip_accents=None, lowercase=False, min_df=7, preprocessor=None), \n",
    "                         LogisticRegression(random_state=1))\n",
    "\n",
    "gs_tfidf = GridSearchCV(pipeline, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split dataset into 2 parts, to form the test and training. This will ensure that the cross validation takes place on the training data and final accuracy on the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                          random_state=42,\n",
    "                                                          test_size=0.25,\n",
    "                                                          stratify=y)\n",
    "gs_tfidf.fit(text_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "- Once the gridsearchCV is complete; access the best_params_ and print it\n",
    "- use best_score_ to access the accuracy for the best parameter combination on the train set\n",
    "- use the best_estimator_ to make predictions on the unseen text data i.e. text_test. You can use gs_tfidf.best_estimator_.score(text_test, y_test) to calculate the accuracy on test.\n",
    "- Use classification_report() to provides a summary of performance.\n",
    "- Try setting the different tfidfvectoriser params to find the best combination for this dataset. For instance change the parameters min_df and ngram_range of the TfidfVectorizer. How does that change the important features?\n",
    "- Finally you may want to do a similar study on the IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning \n",
    "\n",
    "This is a family of machine learning algorithms used to draw inferences from datasets consisting of input data. Since the focus is away from class labels we refer to these as unsupervised.\n",
    "The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data.\n",
    "Clustering of documents is a common task and often used as a means to form indexes to search large collections of documents (e.g. indexing of web collections). \n",
    "\n",
    "## Document Clustering - Organizing clusters as a hierarchical tree\n",
    "In agglomerative clustering method, at each stage, the pair of clusters with minimum between-cluster distance are merged. \n",
    "Once the distance matrix (dist) is computed the dendogram can be generated by stating a linkage method.\n",
    "To cluster our documents it is best to work with a small sample of emails messages in order to visualise the dendogram. \n",
    "This can be controlled with setting the frac parameter. \n",
    "\n",
    "### Clustering the Spam dataset\n",
    "We will study clustering using the same dataset we used before for text classification. However we will not be using the class labels as we did before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(frac=0.05, random_state=1) # use a frac of the data\n",
    "print(df_sample.shape)\n",
    "labels = list(range(len(df_sample.index.values))) # labels are text message ids extracted from the data frame index\n",
    "index = labels # need these for the dendogram\n",
    "\n",
    "#a = list(range(len(df_sample.index.values)))\n",
    "#labels = (list(map(str, a)))\n",
    "#print(labels)\n",
    "#variables = list(df_sample.columns.values)\n",
    "#features = list(df_sample.columns.values)\n",
    "#print(variables)\n",
    "#np.array(labels).T.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up the data further for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['sms_msg'] = df_sample['sms_msg'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example text messages\n",
    "df_sample.iloc[0:10].sms_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataframe to hold the vectorised versions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=20, min_df=4, smooth_idf=True, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenizer_stemmer, ngram_range=(1,1))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_sample['sms_msg']) #fit the vectorizer to sms messages\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "#print(tfidf_matrix.shape)\n",
    "\n",
    "#print(terms)\n",
    "#print(tfidf_matrix)\n",
    "#df_sample['smsVect'] = list(tfidf_matrix.toarray()) # create the corresponding vectorised representation\n",
    "#z= list(tfidf_matrix.toarray())\n",
    "\n",
    "#list(tfidf_matrix.toarray())\n",
    "df3 = pd.DataFrame(list(tfidf_matrix.toarray()), columns = terms, index = index)\n",
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can visualise this document x term matrix using a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "uniform_data = np.random.rand(10, 12)\n",
    "ax = sns.heatmap(df3)\n",
    "ax.figure.savefig(\"heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing hierarchical clustering on a distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "row_dist = pd.DataFrame(squareform(pdist(df3.values, metric='euclidean')),\n",
    "                        columns=index,\n",
    "                        index=index)\n",
    "print(row_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "row_clusters = linkage(df3.values, method='single', metric='euclidean')\n",
    "pd.DataFrame(row_clusters,\n",
    "             columns=['row label 1', 'row label 2',\n",
    "                      'distance', 'no. of items in clust.'],\n",
    "             index=['cluster %d' % (i + 1)\n",
    "                    for i in range(row_clusters.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Dendogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "#make dendrogram black (part 1/2)\n",
    "#from scipy.cluster.hierarchy import set_link_color_palette\n",
    "#set_link_color_palette(['black'])\n",
    "\n",
    "fig, row_dendr = plt.subplots(figsize=(15, 20)) # set size\n",
    "\n",
    "row_dendr = dendrogram(row_clusters, \n",
    "                       labels=labels,\n",
    "                       orientation=\"right\",\n",
    "                       # make dendrogram black (part 2/2)\n",
    "                       # color_threshold=np.inf\n",
    "                       )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Euclidean distance')\n",
    "#plt.savefig('images/11_11.png', dpi=300, \n",
    "#            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "Change the linkage method from 'complete' to 'single' and create the dendogram. Do you notice any obvious differences? Single link tends to create straggly clusters compared to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further visualisations with the seaborn library\n",
    "seaborn provides some useful methods to display both the dendogram aswell as visualise the distances that led to the merging of instances (or text messages). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import seaborn as sns\n",
    "\n",
    " \n",
    "# Data set\n",
    "df3\n",
    " \n",
    "# draw the dendogram.\n",
    "# there are several linkage methods that can be used here\n",
    "sns.clustermap(df3, metric=\"euclidean\", standard_scale=1, method=\"single\")\n",
    "sns.clustermap(df3, metric=\"euclidean\", standard_scale=1, method=\"complete\", cmap=\"mako\")\n",
    "#sns.clustermap(df3, metric=\"euclidean\", standard_scale=1, method=\"average\", cmap=\"viridis\")\n",
    "#sns.clustermap(df3, metric=\"euclidean\", standard_scale=1, method=\"complete\", cmap=\"Blues\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorations\n",
    "\n",
    "The code above can be used to cluster any text collection you might want to manage. \n",
    "For instance we could use it to cluster similar movies from synopsis; or we could cluster similar books from their abstracts; or cluster friends in a network based on similar postings etc. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
